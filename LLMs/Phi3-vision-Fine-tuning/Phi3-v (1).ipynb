{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun  1 09:15:01 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 NVL                Off | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              60W / 400W |      7MiB / 95830MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-sm|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (10.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchvision in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.3.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torchvision) (2.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wandb in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (0.17.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from wandb) (2.3.1)\n",
      "Requirement already satisfied: setproctitle in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/miniconda/envs/phi3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install Pillow  \n",
    "!pip install torchvision\n",
    "!pip install wandb  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca3e5fdcb394f458eea0add49aaf0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e55e1a9db147f8a41a0053c9a40204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/287k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b76a6f40ba457f9c2e9f1f07ecd2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3038 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download N.A.: Invalid URL 'N.A.': No scheme supplied. Perhaps you meant https://N.A.?\n",
      "Failed to download N.A.: Invalid URL 'N.A.': No scheme supplied. Perhaps you meant https://N.A.?\n",
      "Dataset and images saved to ./data/burberry_dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# Function to download an image from a URL and save it locally\n",
    "def download_image(image_url, save_path):\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        image.save(save_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {image_url}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Download the dataset from Hugging Face\n",
    "dataset = load_dataset('DBQ/Burberry.Product.prices.United.States')\n",
    "\n",
    "\n",
    "# Convert the Hugging Face dataset to a Pandas DataFrame\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "\n",
    "# Create directories to save the dataset and images\n",
    "dataset_dir = './data/burberry_dataset'\n",
    "images_dir = os.path.join(dataset_dir, 'images')\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Filter out rows where image download fails\n",
    "filtered_rows = []\n",
    "for idx, row in df.iterrows():\n",
    "    image_url = row['imageurl']\n",
    "    image_name = f\"{row['product_code']}.jpg\"\n",
    "    image_path = os.path.join(images_dir, image_name)\n",
    "    if download_image(image_url, image_path):\n",
    "        row['local_image_path'] = image_path\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "\n",
    "# Create a new DataFrame with the filtered rows\n",
    "filtered_df = pd.DataFrame(filtered_rows)\n",
    "\n",
    "\n",
    "# Save the updated dataset to disk\n",
    "dataset_path = os.path.join(dataset_dir, 'burberry_dataset.csv')\n",
    "filtered_df.to_csv(dataset_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"Dataset and images saved to {dataset_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pyamegdv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e19595170b4351b7ac35b5e0521b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-energy-13</strong> at: <a href='https://wandb.ai/davidsajare/davidwei-phi3-v-test/runs/pyamegdv' target=\"_blank\">https://wandb.ai/davidsajare/davidwei-phi3-v-test/runs/pyamegdv</a><br/> View project at: <a href='https://wandb.ai/davidsajare/davidwei-phi3-v-test' target=\"_blank\">https://wandb.ai/davidsajare/davidwei-phi3-v-test</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240601_095937-pyamegdv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pyamegdv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b491430567cf4ceea3e3da91ee817416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112390966657889, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wandb/run-20240601_103239-c1x5amoy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/davidsajare/davidwei-phi3-v/runs/c1x5amoy' target=\"_blank\">celestial-sea-5</a></strong> to <a href='https://wandb.ai/davidsajare/davidwei-phi3-v' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/davidsajare/davidwei-phi3-v' target=\"_blank\">https://wandb.ai/davidsajare/davidwei-phi3-v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/davidsajare/davidwei-phi3-v/runs/c1x5amoy' target=\"_blank\">https://wandb.ai/davidsajare/davidwei-phi3-v/runs/c1x5amoy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206b3ab4850740f0b9781b4650ec6b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 1, Batch Loss: 11.79928970336914\n",
      "Epoch: 0, Step: 2, Batch Loss: 11.847307205200195\n",
      "Epoch: 0, Step: 3, Batch Loss: 11.846869468688965\n",
      "Epoch: 0, Step: 4, Batch Loss: 11.832585334777832\n",
      "Epoch: 0, Step: 5, Batch Loss: 11.901601791381836\n",
      "Epoch: 0, Step: 6, Batch Loss: 11.851520538330078\n",
      "Epoch: 0, Step: 7, Batch Loss: 11.837945938110352\n",
      "Epoch: 0, Step: 8, Batch Loss: 11.759607315063477\n",
      "Epoch: 0, Step: 9, Batch Loss: 11.84061050415039\n",
      "Epoch: 0, Step: 10, Batch Loss: 11.6901273727417\n",
      "Epoch: 0, Step: 11, Batch Loss: 11.843684196472168\n",
      "Epoch: 0, Step: 12, Batch Loss: 11.66565227508545\n",
      "Epoch: 0, Step: 13, Batch Loss: 11.819778442382812\n",
      "Epoch: 0, Step: 14, Batch Loss: 11.870205879211426\n",
      "Epoch: 0, Step: 15, Batch Loss: 11.808638572692871\n",
      "Epoch: 0, Step: 16, Batch Loss: 11.656807899475098\n",
      "Epoch: 0, Step: 17, Batch Loss: 11.825624465942383\n",
      "Epoch: 0, Step: 18, Batch Loss: 11.754814147949219\n",
      "Epoch: 0, Step: 19, Batch Loss: 11.570878028869629\n",
      "Epoch: 0, Step: 20, Batch Loss: 11.812657356262207\n",
      "Epoch: 0, Step: 21, Batch Loss: 11.778510093688965\n",
      "Epoch: 0, Step: 22, Batch Loss: 11.817607879638672\n",
      "Epoch: 0, Step: 23, Batch Loss: 11.749275207519531\n",
      "Epoch: 0, Step: 24, Batch Loss: 11.676424026489258\n",
      "Epoch: 0, Step: 25, Batch Loss: 11.853963851928711\n",
      "Epoch: 0, Step: 26, Batch Loss: 11.818915367126465\n",
      "Epoch: 0, Step: 27, Batch Loss: 11.749344825744629\n",
      "Epoch: 0, Step: 28, Batch Loss: 11.759849548339844\n",
      "Epoch: 0, Step: 29, Batch Loss: 11.878864288330078\n",
      "Epoch: 0, Step: 30, Batch Loss: 11.720442771911621\n",
      "Epoch: 0, Step: 31, Batch Loss: 11.777013778686523\n",
      "Epoch: 0, Step: 32, Batch Loss: 11.87210750579834\n",
      "Epoch: 0, Step: 33, Batch Loss: 11.86601734161377\n",
      "Epoch: 0, Step: 34, Batch Loss: 11.72638988494873\n",
      "Epoch: 0, Step: 35, Batch Loss: 11.676530838012695\n",
      "Epoch: 0, Step: 36, Batch Loss: 11.746923446655273\n",
      "Epoch: 0, Step: 37, Batch Loss: 11.852855682373047\n",
      "Epoch: 0, Step: 38, Batch Loss: 11.881075859069824\n",
      "Epoch: 0, Step: 39, Batch Loss: 11.84420108795166\n",
      "Epoch: 0, Step: 40, Batch Loss: 11.62468433380127\n",
      "Epoch: 0, Step: 41, Batch Loss: 11.594399452209473\n",
      "Epoch: 0, Step: 42, Batch Loss: 11.778833389282227\n",
      "Epoch: 0, Step: 43, Batch Loss: 11.666238784790039\n",
      "Epoch: 0, Step: 44, Batch Loss: 11.876359939575195\n",
      "Epoch: 0, Step: 45, Batch Loss: 11.872221946716309\n",
      "Epoch: 0, Step: 46, Batch Loss: 11.694598197937012\n",
      "Epoch: 0, Step: 47, Batch Loss: 11.834132194519043\n",
      "Epoch: 0, Step: 48, Batch Loss: 11.849971771240234\n",
      "Epoch: 0, Step: 49, Batch Loss: 11.695515632629395\n",
      "Epoch: 0, Step: 50, Batch Loss: 11.62839412689209\n",
      "Epoch: 0, Step: 51, Batch Loss: 11.845246315002441\n",
      "Epoch: 0, Step: 52, Batch Loss: 11.778923034667969\n",
      "Epoch: 0, Step: 53, Batch Loss: 11.490656852722168\n",
      "Epoch: 0, Step: 54, Batch Loss: 11.881674766540527\n",
      "Epoch: 0, Step: 55, Batch Loss: 11.783812522888184\n",
      "Epoch: 0, Step: 56, Batch Loss: 11.765254974365234\n",
      "Epoch: 0, Step: 57, Batch Loss: 11.770750999450684\n",
      "Epoch: 0, Step: 58, Batch Loss: 11.683554649353027\n",
      "Epoch: 0, Step: 59, Batch Loss: 11.879448890686035\n",
      "Epoch: 0, Step: 60, Batch Loss: 11.698189735412598\n",
      "Epoch: 0, Step: 61, Batch Loss: 11.798291206359863\n",
      "Epoch: 0, Step: 62, Batch Loss: 11.80339527130127\n",
      "Epoch: 0, Step: 63, Batch Loss: 11.854876518249512\n",
      "Epoch: 0, Step: 64, Batch Loss: 11.736981391906738\n",
      "Epoch: 0, Step: 65, Batch Loss: 6.581153392791748\n",
      "Epoch: 0, Step: 66, Batch Loss: 6.6241021156311035\n",
      "Epoch: 0, Step: 67, Batch Loss: 6.537566661834717\n",
      "Epoch: 0, Step: 68, Batch Loss: 6.625484943389893\n",
      "Epoch: 0, Step: 69, Batch Loss: 6.515063762664795\n",
      "Epoch: 0, Step: 70, Batch Loss: 6.606765270233154\n",
      "Epoch: 0, Step: 71, Batch Loss: 6.557328224182129\n",
      "Epoch: 0, Step: 72, Batch Loss: 6.559120178222656\n",
      "Epoch: 0, Step: 73, Batch Loss: 6.556735992431641\n",
      "Epoch: 0, Step: 74, Batch Loss: 6.548151969909668\n",
      "Epoch: 0, Step: 75, Batch Loss: 6.606973171234131\n",
      "Epoch: 0, Step: 76, Batch Loss: 6.590042591094971\n",
      "Epoch: 0, Step: 77, Batch Loss: 6.549166679382324\n",
      "Epoch: 0, Step: 78, Batch Loss: 6.618786811828613\n",
      "Epoch: 0, Step: 79, Batch Loss: 6.607950687408447\n",
      "Epoch: 0, Step: 80, Batch Loss: 6.539787292480469\n",
      "Epoch: 0, Step: 81, Batch Loss: 6.501924991607666\n",
      "Epoch: 0, Step: 82, Batch Loss: 6.553048133850098\n",
      "Epoch: 0, Step: 83, Batch Loss: 6.610913276672363\n",
      "Epoch: 0, Step: 84, Batch Loss: 6.483138561248779\n",
      "Epoch: 0, Step: 85, Batch Loss: 6.488030910491943\n",
      "Epoch: 0, Step: 86, Batch Loss: 6.618666648864746\n",
      "Epoch: 0, Step: 87, Batch Loss: 6.619875907897949\n",
      "Epoch: 0, Step: 88, Batch Loss: 6.606849670410156\n",
      "Epoch: 0, Step: 89, Batch Loss: 6.6143317222595215\n",
      "Epoch: 0, Step: 90, Batch Loss: 6.5347514152526855\n",
      "Epoch: 0, Step: 91, Batch Loss: 6.6028947830200195\n",
      "Epoch: 0, Step: 92, Batch Loss: 6.51725435256958\n",
      "Epoch: 0, Step: 93, Batch Loss: 6.58258581161499\n",
      "Epoch: 0, Step: 94, Batch Loss: 6.591337203979492\n",
      "Epoch: 0, Step: 95, Batch Loss: 6.575711250305176\n",
      "Epoch: 0, Step: 96, Batch Loss: 6.527541160583496\n",
      "Epoch: 0, Step: 97, Batch Loss: 6.609715461730957\n",
      "Epoch: 0, Step: 98, Batch Loss: 6.600139141082764\n",
      "Epoch: 0, Step: 99, Batch Loss: 6.586878776550293\n",
      "Epoch: 0, Step: 100, Batch Loss: 6.493865489959717\n",
      "Epoch: 0, Step: 101, Batch Loss: 6.629748344421387\n",
      "Epoch: 0, Step: 102, Batch Loss: 6.530716896057129\n",
      "Epoch: 0, Step: 103, Batch Loss: 6.496994495391846\n",
      "Epoch: 0, Step: 104, Batch Loss: 6.6374831199646\n",
      "Epoch: 0, Step: 105, Batch Loss: 6.613664627075195\n",
      "Epoch: 0, Step: 106, Batch Loss: 6.597713947296143\n",
      "Epoch: 0, Step: 107, Batch Loss: 6.5598554611206055\n",
      "Epoch: 0, Step: 108, Batch Loss: 6.529253005981445\n",
      "Epoch: 0, Step: 109, Batch Loss: 6.646240711212158\n",
      "Epoch: 0, Step: 110, Batch Loss: 6.584869384765625\n",
      "Epoch: 0, Step: 111, Batch Loss: 6.618061065673828\n",
      "Epoch: 0, Step: 112, Batch Loss: 6.502835273742676\n",
      "Epoch: 0, Step: 113, Batch Loss: 6.602906227111816\n",
      "Epoch: 0, Step: 114, Batch Loss: 6.60877799987793\n",
      "Epoch: 0, Step: 115, Batch Loss: 6.599211692810059\n",
      "Epoch: 0, Step: 116, Batch Loss: 6.52932071685791\n",
      "Epoch: 0, Step: 117, Batch Loss: 6.548778057098389\n",
      "Epoch: 0, Step: 118, Batch Loss: 6.5048017501831055\n",
      "Epoch: 0, Step: 119, Batch Loss: 6.607408046722412\n",
      "Epoch: 0, Step: 120, Batch Loss: 6.605045318603516\n",
      "Epoch: 0, Step: 121, Batch Loss: 6.5436506271362305\n",
      "Epoch: 0, Step: 122, Batch Loss: 6.5500569343566895\n",
      "Epoch: 0, Step: 123, Batch Loss: 6.515637397766113\n",
      "Epoch: 0, Step: 124, Batch Loss: 6.619070053100586\n",
      "Epoch: 0, Step: 125, Batch Loss: 6.4798479080200195\n",
      "Epoch: 0, Step: 126, Batch Loss: 6.581770420074463\n",
      "Epoch: 0, Step: 127, Batch Loss: 6.537093639373779\n",
      "Epoch: 0, Step: 128, Batch Loss: 6.502429485321045\n",
      "Epoch: 0, Step: 129, Batch Loss: 3.226149082183838\n",
      "Epoch: 0, Step: 130, Batch Loss: 3.2499637603759766\n",
      "Epoch: 0, Step: 131, Batch Loss: 3.248182535171509\n",
      "Epoch: 0, Step: 132, Batch Loss: 3.209596872329712\n",
      "Epoch: 0, Step: 133, Batch Loss: 3.249359607696533\n",
      "Epoch: 0, Step: 134, Batch Loss: 3.226552724838257\n",
      "Epoch: 0, Step: 135, Batch Loss: 3.2512946128845215\n",
      "Epoch: 0, Step: 136, Batch Loss: 3.2536845207214355\n",
      "Epoch: 0, Step: 137, Batch Loss: 3.2640345096588135\n",
      "Epoch: 0, Step: 138, Batch Loss: 3.2360877990722656\n",
      "Epoch: 0, Step: 139, Batch Loss: 3.2675232887268066\n",
      "Epoch: 0, Step: 140, Batch Loss: 3.239379405975342\n",
      "Epoch: 0, Step: 141, Batch Loss: 3.258749485015869\n",
      "Epoch: 0, Step: 142, Batch Loss: 3.263237476348877\n",
      "Epoch: 0, Step: 143, Batch Loss: 3.2642030715942383\n",
      "Epoch: 0, Step: 144, Batch Loss: 3.1859939098358154\n",
      "Epoch: 0, Step: 145, Batch Loss: 3.225569486618042\n",
      "Epoch: 0, Step: 146, Batch Loss: 3.2464046478271484\n",
      "Epoch: 0, Step: 147, Batch Loss: 3.2491230964660645\n",
      "Epoch: 0, Step: 148, Batch Loss: 3.2911458015441895\n",
      "Epoch: 0, Step: 149, Batch Loss: 3.203043222427368\n",
      "Epoch: 0, Step: 150, Batch Loss: 3.265394687652588\n",
      "Step: 150, Validation Loss: 3.244710679117002, Validation Price Error (Normalized): 884.875\n",
      "Epoch: 0, Step: 151, Batch Loss: 3.2509684562683105\n",
      "Epoch: 0, Step: 152, Batch Loss: 3.2912583351135254\n",
      "Epoch: 0, Step: 153, Batch Loss: 3.227564811706543\n",
      "Epoch: 0, Step: 154, Batch Loss: 3.234786033630371\n",
      "Epoch: 0, Step: 155, Batch Loss: 3.2238457202911377\n",
      "Epoch: 0, Step: 156, Batch Loss: 3.271169900894165\n",
      "Epoch: 0, Step: 157, Batch Loss: 3.2500860691070557\n",
      "Epoch: 0, Step: 158, Batch Loss: 3.2687909603118896\n",
      "Epoch: 0, Step: 159, Batch Loss: 3.2715988159179688\n",
      "Epoch: 0, Step: 160, Batch Loss: 3.2357048988342285\n",
      "Epoch: 0, Step: 161, Batch Loss: 3.246311664581299\n",
      "Epoch: 0, Step: 162, Batch Loss: 3.2509355545043945\n",
      "Epoch: 0, Step: 163, Batch Loss: 3.2721023559570312\n",
      "Epoch: 0, Step: 164, Batch Loss: 3.2526636123657227\n",
      "Epoch: 0, Step: 165, Batch Loss: 3.2217702865600586\n",
      "Epoch: 0, Step: 166, Batch Loss: 3.255958318710327\n",
      "Epoch: 0, Step: 167, Batch Loss: 3.2638132572174072\n",
      "Epoch: 0, Step: 168, Batch Loss: 3.1942687034606934\n",
      "Epoch: 0, Step: 169, Batch Loss: 3.232394218444824\n",
      "Epoch: 0, Step: 170, Batch Loss: 3.3144545555114746\n",
      "Epoch: 0, Step: 171, Batch Loss: 3.2586214542388916\n",
      "Epoch: 0, Step: 172, Batch Loss: 3.224330425262451\n",
      "Epoch: 0, Step: 173, Batch Loss: 3.2394497394561768\n",
      "Epoch: 0, Step: 174, Batch Loss: 3.266275405883789\n",
      "Epoch: 0, Step: 175, Batch Loss: 3.2633984088897705\n",
      "Epoch: 0, Step: 176, Batch Loss: 3.201050043106079\n",
      "Epoch: 0, Step: 177, Batch Loss: 3.2770044803619385\n",
      "Epoch: 0, Step: 178, Batch Loss: 3.299539089202881\n",
      "Epoch: 0, Step: 179, Batch Loss: 3.2463788986206055\n",
      "Epoch: 0, Step: 180, Batch Loss: 3.274153470993042\n",
      "Epoch: 0, Step: 181, Batch Loss: 3.239363193511963\n",
      "Epoch: 0, Step: 182, Batch Loss: 3.2482850551605225\n",
      "Epoch: 0, Step: 183, Batch Loss: 3.2363851070404053\n",
      "Epoch: 0, Step: 184, Batch Loss: 3.2076611518859863\n",
      "Epoch: 0, Step: 185, Batch Loss: 3.261183023452759\n",
      "Epoch: 0, Step: 186, Batch Loss: 3.2267730236053467\n",
      "Epoch: 0, Step: 187, Batch Loss: 3.2629551887512207\n",
      "Epoch: 0, Step: 188, Batch Loss: 3.228070020675659\n",
      "Epoch: 0, Step: 189, Batch Loss: 3.2946090698242188\n",
      "Epoch: 0, Step: 190, Batch Loss: 3.254364252090454\n",
      "Epoch: 0, Step: 191, Batch Loss: 3.2286670207977295\n",
      "Epoch: 0, Step: 192, Batch Loss: 3.2534730434417725\n",
      "Epoch: 0, Step: 193, Batch Loss: 0.6311724781990051\n",
      "Epoch: 0, Step: 194, Batch Loss: 0.6634857058525085\n",
      "Epoch: 0, Step: 195, Batch Loss: 0.6140602231025696\n",
      "Epoch: 0, Step: 196, Batch Loss: 0.6105101704597473\n",
      "Epoch: 0, Step: 197, Batch Loss: 0.6191427111625671\n",
      "Epoch: 0, Step: 198, Batch Loss: 0.632084310054779\n",
      "Epoch: 0, Step: 199, Batch Loss: 0.6110999584197998\n",
      "Epoch: 0, Step: 200, Batch Loss: 0.6465357542037964\n",
      "Epoch: 0, Step: 201, Batch Loss: 0.6206196546554565\n",
      "Epoch: 0, Step: 202, Batch Loss: 0.6174724102020264\n",
      "Epoch: 0, Step: 203, Batch Loss: 0.6423224806785583\n",
      "Epoch: 0, Step: 204, Batch Loss: 0.620768666267395\n",
      "Epoch: 0, Step: 205, Batch Loss: 0.5978780388832092\n",
      "Epoch: 0, Step: 206, Batch Loss: 0.6208987236022949\n",
      "Epoch: 0, Step: 207, Batch Loss: 0.5829942226409912\n",
      "Epoch: 0, Step: 208, Batch Loss: 0.6228272914886475\n",
      "Epoch: 0, Step: 209, Batch Loss: 0.5967060923576355\n",
      "Epoch: 0, Step: 210, Batch Loss: 0.6237964630126953\n",
      "Epoch: 0, Step: 211, Batch Loss: 0.7427056431770325\n",
      "Epoch: 0, Step: 212, Batch Loss: 0.7251061201095581\n",
      "Epoch: 0, Step: 213, Batch Loss: 0.617111325263977\n",
      "Epoch: 0, Step: 214, Batch Loss: 0.6110968589782715\n",
      "Epoch: 0, Step: 215, Batch Loss: 0.6284208297729492\n",
      "Epoch: 0, Step: 216, Batch Loss: 0.6329936385154724\n",
      "Epoch: 0, Step: 217, Batch Loss: 0.6051406860351562\n",
      "Epoch: 0, Step: 218, Batch Loss: 0.6078103184700012\n",
      "Epoch: 0, Step: 219, Batch Loss: 0.5881198644638062\n",
      "Epoch: 0, Step: 220, Batch Loss: 0.6119509339332581\n",
      "Epoch: 0, Step: 221, Batch Loss: 0.6119179129600525\n",
      "Epoch: 0, Step: 222, Batch Loss: 0.6645546555519104\n",
      "Epoch: 0, Step: 223, Batch Loss: 0.6157510876655579\n",
      "Epoch: 0, Step: 224, Batch Loss: 0.6413629651069641\n",
      "Epoch: 0, Step: 225, Batch Loss: 0.6267274618148804\n",
      "Epoch: 0, Step: 226, Batch Loss: 0.6137775182723999\n",
      "Epoch: 0, Step: 227, Batch Loss: 0.6192373633384705\n",
      "Epoch: 0, Step: 228, Batch Loss: 0.5899473428726196\n",
      "Epoch: 0, Step: 229, Batch Loss: 0.6003243923187256\n",
      "Epoch: 0, Step: 230, Batch Loss: 0.6325660943984985\n",
      "Epoch: 0, Step: 231, Batch Loss: 0.5986343622207642\n",
      "Epoch: 0, Step: 232, Batch Loss: 0.6277653574943542\n",
      "Epoch: 0, Step: 233, Batch Loss: 0.6073322296142578\n",
      "Epoch: 0, Step: 234, Batch Loss: 0.6156195998191833\n",
      "Epoch: 0, Step: 235, Batch Loss: 0.6140577793121338\n",
      "Epoch: 0, Step: 236, Batch Loss: 0.6105831265449524\n",
      "Epoch: 0, Step: 237, Batch Loss: 0.6099918484687805\n",
      "Epoch: 0, Step: 238, Batch Loss: 0.602423906326294\n",
      "Epoch: 0, Step: 239, Batch Loss: 0.6411926746368408\n",
      "Epoch: 0, Step: 240, Batch Loss: 0.7105100154876709\n",
      "Epoch: 0, Step: 241, Batch Loss: 0.6356591582298279\n",
      "Epoch: 0, Step: 242, Batch Loss: 0.5985020995140076\n",
      "Epoch: 0, Step: 243, Batch Loss: 0.6237812042236328\n",
      "Epoch: 0, Step: 244, Batch Loss: 0.6356732845306396\n",
      "Epoch: 0, Step: 245, Batch Loss: 0.6260218620300293\n",
      "Epoch: 0, Step: 246, Batch Loss: 0.6550648808479309\n",
      "Epoch: 0, Step: 247, Batch Loss: 0.6256013512611389\n",
      "Epoch: 0, Step: 248, Batch Loss: 0.6061750054359436\n",
      "Epoch: 0, Step: 249, Batch Loss: 0.6063568592071533\n",
      "Epoch: 0, Step: 250, Batch Loss: 0.5985973477363586\n",
      "Epoch: 0, Step: 251, Batch Loss: 0.6042813062667847\n",
      "Epoch: 0, Step: 252, Batch Loss: 0.6158525943756104\n",
      "Epoch: 0, Step: 253, Batch Loss: 0.6044561862945557\n",
      "Epoch: 0, Step: 254, Batch Loss: 0.6080155372619629\n",
      "Epoch: 0, Step: 255, Batch Loss: 0.6012788414955139\n",
      "Epoch: 0, Step: 256, Batch Loss: 0.6098325848579407\n",
      "Epoch: 0, Step: 257, Batch Loss: 0.1507699191570282\n",
      "Epoch: 0, Step: 258, Batch Loss: 0.12287532538175583\n",
      "Epoch: 0, Step: 259, Batch Loss: 0.1827751100063324\n",
      "Epoch: 0, Step: 260, Batch Loss: 0.09883598983287811\n",
      "Epoch: 0, Step: 261, Batch Loss: 0.10322157293558121\n",
      "Epoch: 0, Step: 262, Batch Loss: 0.1263064295053482\n",
      "Epoch: 0, Step: 263, Batch Loss: 0.13402526080608368\n",
      "Epoch: 0, Step: 264, Batch Loss: 0.12834319472312927\n",
      "Epoch: 0, Step: 265, Batch Loss: 0.12970608472824097\n",
      "Epoch: 0, Step: 266, Batch Loss: 0.13087889552116394\n",
      "Epoch: 0, Step: 267, Batch Loss: 0.130306214094162\n",
      "Epoch: 0, Step: 268, Batch Loss: 0.10978460311889648\n",
      "Epoch: 0, Step: 269, Batch Loss: 0.14113815128803253\n",
      "Epoch: 0, Step: 270, Batch Loss: 0.13574928045272827\n",
      "Epoch: 0, Step: 271, Batch Loss: 0.10585909336805344\n",
      "Epoch: 0, Step: 272, Batch Loss: 0.12415898591279984\n",
      "Epoch: 0, Step: 273, Batch Loss: 0.19354544579982758\n",
      "Epoch: 0, Step: 274, Batch Loss: 0.1255326271057129\n",
      "Epoch: 0, Step: 275, Batch Loss: 0.1773546189069748\n",
      "Epoch: 0, Step: 276, Batch Loss: 0.11103982478380203\n",
      "Epoch: 0, Step: 277, Batch Loss: 0.24740463495254517\n",
      "Epoch: 0, Step: 278, Batch Loss: 0.15527763962745667\n",
      "Epoch: 0, Step: 279, Batch Loss: 0.1534435749053955\n",
      "Epoch: 0, Step: 280, Batch Loss: 0.16586747765541077\n",
      "Epoch: 0, Step: 281, Batch Loss: 0.15327508747577667\n",
      "Epoch: 0, Step: 282, Batch Loss: 0.2366144061088562\n",
      "Epoch: 0, Step: 283, Batch Loss: 0.11574479192495346\n",
      "Epoch: 0, Step: 284, Batch Loss: 0.1328313648700714\n",
      "Epoch: 0, Step: 285, Batch Loss: 0.10322157293558121\n",
      "Epoch: 0, Step: 286, Batch Loss: 0.13332825899124146\n",
      "Epoch: 0, Step: 287, Batch Loss: 0.12630115449428558\n",
      "Epoch: 0, Step: 288, Batch Loss: 0.1391994059085846\n",
      "Epoch: 0, Step: 289, Batch Loss: 0.12483314424753189\n",
      "Epoch: 0, Step: 290, Batch Loss: 0.14890612661838531\n",
      "Epoch: 0, Step: 291, Batch Loss: 0.13683950901031494\n",
      "Epoch: 0, Step: 292, Batch Loss: 0.13392528891563416\n",
      "Epoch: 0, Step: 293, Batch Loss: 0.12512722611427307\n",
      "Epoch: 0, Step: 294, Batch Loss: 0.13655684888362885\n",
      "Epoch: 0, Step: 295, Batch Loss: 0.12336364388465881\n",
      "Epoch: 0, Step: 296, Batch Loss: 0.15517190098762512\n",
      "Epoch: 0, Step: 297, Batch Loss: 0.13654951751232147\n",
      "Epoch: 0, Step: 298, Batch Loss: 0.1276857852935791\n",
      "Epoch: 0, Step: 299, Batch Loss: 0.15851812064647675\n",
      "Epoch: 0, Step: 300, Batch Loss: 0.12772054970264435\n",
      "Step: 300, Validation Loss: 0.14121511585912422, Validation Price Error (Normalized): 1021.3947368421053\n",
      "Epoch: 0, Step: 301, Batch Loss: 0.17987000942230225\n",
      "Epoch: 0, Step: 302, Batch Loss: 0.13432657718658447\n",
      "Epoch: 0, Step: 303, Batch Loss: 0.12352308630943298\n",
      "Epoch: 0, Step: 304, Batch Loss: 0.12970608472824097\n",
      "Epoch: 0, Step: 305, Batch Loss: 0.1337783932685852\n",
      "Epoch: 0, Step: 306, Batch Loss: 0.1679549515247345\n",
      "Epoch: 0, Step: 307, Batch Loss: 0.13439664244651794\n",
      "Epoch: 0, Step: 308, Batch Loss: 0.19105106592178345\n",
      "Epoch: 0, Step: 309, Batch Loss: 0.11040562391281128\n",
      "Epoch: 0, Step: 310, Batch Loss: 0.1520695686340332\n",
      "Epoch: 0, Step: 311, Batch Loss: 0.11662250757217407\n",
      "Epoch: 0, Step: 312, Batch Loss: 0.20693744719028473\n",
      "Epoch: 0, Step: 313, Batch Loss: 0.12929612398147583\n",
      "Epoch: 0, Step: 314, Batch Loss: 0.11545122414827347\n",
      "Epoch: 0, Step: 315, Batch Loss: 0.10870115458965302\n",
      "Epoch: 0, Step: 316, Batch Loss: 0.12908010184764862\n",
      "Epoch: 0, Step: 317, Batch Loss: 0.15430916845798492\n",
      "Epoch: 0, Step: 318, Batch Loss: 0.13471512496471405\n",
      "Epoch: 0, Step: 319, Batch Loss: 0.15681782364845276\n",
      "Epoch: 0, Step: 320, Batch Loss: 0.1178964376449585\n",
      "Epoch: 0, Step: 321, Batch Loss: 0.12254364043474197\n",
      "Epoch: 0, Step: 322, Batch Loss: 0.1459236741065979\n",
      "Epoch: 0, Step: 323, Batch Loss: 0.12363870441913605\n",
      "Epoch: 0, Step: 324, Batch Loss: 0.14867182075977325\n",
      "Epoch: 0, Step: 325, Batch Loss: 0.14700202643871307\n",
      "Epoch: 0, Step: 326, Batch Loss: 0.11485910415649414\n",
      "Epoch: 0, Step: 327, Batch Loss: 0.14036163687705994\n",
      "Epoch: 0, Step: 328, Batch Loss: 0.10943049937486649\n",
      "Epoch: 0, Step: 329, Batch Loss: 0.10997400432825089\n",
      "Epoch: 0, Step: 330, Batch Loss: 0.11554477363824844\n",
      "Epoch: 0, Step: 331, Batch Loss: 0.11183057725429535\n",
      "Epoch: 0, Step: 332, Batch Loss: 0.11704684793949127\n",
      "Epoch: 0, Step: 333, Batch Loss: 0.11245065182447433\n",
      "Epoch: 0, Step: 334, Batch Loss: 0.13828438520431519\n",
      "Epoch: 0, Step: 335, Batch Loss: 0.13749921321868896\n",
      "Epoch: 0, Step: 336, Batch Loss: 0.1122877225279808\n",
      "Epoch: 0, Step: 337, Batch Loss: 0.17303049564361572\n",
      "Epoch: 0, Step: 338, Batch Loss: 0.1729273498058319\n",
      "Epoch: 0, Step: 339, Batch Loss: 0.13201415538787842\n",
      "Epoch: 0, Step: 340, Batch Loss: 0.13242892920970917\n",
      "Epoch: 0, Step: 341, Batch Loss: 0.09300852566957474\n",
      "Epoch: 0, Step: 342, Batch Loss: 0.15362340211868286\n",
      "Epoch: 0, Step: 343, Batch Loss: 0.1990654021501541\n",
      "Epoch: 0, Step: 344, Batch Loss: 0.10916745662689209\n",
      "Epoch: 0, Step: 345, Batch Loss: 0.11056265980005264\n",
      "Epoch: 0, Step: 346, Batch Loss: 0.15183809399604797\n",
      "Epoch: 0, Step: 347, Batch Loss: 0.16218365728855133\n",
      "Epoch: 0, Step: 348, Batch Loss: 0.102318674325943\n",
      "Epoch: 0, Step: 349, Batch Loss: 0.12419392168521881\n",
      "Epoch: 0, Step: 350, Batch Loss: 0.19237861037254333\n",
      "Epoch: 0, Step: 351, Batch Loss: 0.11027858406305313\n",
      "Epoch: 0, Step: 352, Batch Loss: 0.21150606870651245\n",
      "Epoch: 0, Step: 353, Batch Loss: 0.11986015737056732\n",
      "Epoch: 0, Step: 354, Batch Loss: 0.14478334784507751\n",
      "Epoch: 0, Step: 355, Batch Loss: 0.11137013882398605\n",
      "Epoch: 0, Step: 356, Batch Loss: 0.15989232063293457\n",
      "Epoch: 0, Step: 357, Batch Loss: 0.11025354266166687\n",
      "Epoch: 0, Step: 358, Batch Loss: 0.12140537798404694\n",
      "Epoch: 0, Step: 359, Batch Loss: 0.11818891018629074\n",
      "Epoch: 0, Step: 360, Batch Loss: 0.1570928990840912\n",
      "Epoch: 0, Step: 361, Batch Loss: 0.11686588823795319\n",
      "Epoch: 0, Step: 362, Batch Loss: 0.1475466638803482\n",
      "Epoch: 0, Step: 363, Batch Loss: 0.1109548807144165\n",
      "Epoch: 0, Step: 364, Batch Loss: 0.11244218796491623\n",
      "Epoch: 0, Step: 365, Batch Loss: 0.11651421338319778\n",
      "Epoch: 0, Step: 366, Batch Loss: 0.1644333451986313\n",
      "Epoch: 0, Step: 367, Batch Loss: 0.09475358575582504\n",
      "Epoch: 0, Step: 368, Batch Loss: 0.11468741297721863\n",
      "Epoch: 0, Step: 369, Batch Loss: 0.12289544939994812\n",
      "Epoch: 0, Step: 370, Batch Loss: 0.09958159178495407\n",
      "Epoch: 0, Step: 371, Batch Loss: 0.11396487802267075\n",
      "Epoch: 0, Step: 372, Batch Loss: 0.14988259971141815\n",
      "Epoch: 0, Step: 373, Batch Loss: 0.16233333945274353\n",
      "Epoch: 0, Step: 374, Batch Loss: 0.1370948851108551\n",
      "Epoch: 0, Step: 375, Batch Loss: 0.1976371854543686\n",
      "Epoch: 0, Step: 376, Batch Loss: 0.15328733623027802\n",
      "Epoch: 0, Step: 377, Batch Loss: 0.10338010638952255\n",
      "Epoch: 0, Step: 378, Batch Loss: 0.1122877225279808\n",
      "Epoch: 0, Step: 379, Batch Loss: 0.09831193834543228\n",
      "Epoch: 0, Step: 380, Batch Loss: 0.12826195359230042\n",
      "Epoch: 0, Step: 381, Batch Loss: 0.10452201217412949\n",
      "Epoch: 0, Step: 382, Batch Loss: 0.11952197551727295\n",
      "Epoch: 0, Step: 383, Batch Loss: 0.11538239568471909\n",
      "Epoch: 0, Step: 384, Batch Loss: 0.11080627888441086\n",
      "Epoch: 0, Step: 385, Batch Loss: 0.15108029544353485\n",
      "Epoch: 0, Step: 386, Batch Loss: 0.11948093771934509\n",
      "Epoch: 0, Step: 387, Batch Loss: 0.10944372415542603\n",
      "Epoch: 0, Step: 388, Batch Loss: 0.16810277104377747\n",
      "Epoch: 0, Step: 389, Batch Loss: 0.18860094249248505\n",
      "Epoch: 0, Step: 390, Batch Loss: 0.13500812649726868\n",
      "Epoch: 0, Step: 391, Batch Loss: 0.14415764808654785\n",
      "Epoch: 0, Step: 392, Batch Loss: 0.12314274907112122\n",
      "Epoch: 0, Step: 393, Batch Loss: 0.09735001623630524\n",
      "Epoch: 0, Step: 394, Batch Loss: 0.11680762469768524\n",
      "Epoch: 0, Step: 395, Batch Loss: 0.10649023950099945\n",
      "Epoch: 0, Step: 396, Batch Loss: 0.13342371582984924\n",
      "Epoch: 0, Step: 397, Batch Loss: 0.1178298369050026\n",
      "Epoch: 0, Step: 398, Batch Loss: 0.12130238860845566\n",
      "Epoch: 0, Step: 399, Batch Loss: 0.1256968080997467\n",
      "Epoch: 0, Step: 400, Batch Loss: 0.17507939040660858\n",
      "Epoch: 0, Step: 401, Batch Loss: 0.156686931848526\n",
      "Epoch: 0, Step: 402, Batch Loss: 0.12032870948314667\n",
      "Epoch: 0, Step: 403, Batch Loss: 0.11393968015909195\n",
      "Epoch: 0, Step: 404, Batch Loss: 0.10956050455570221\n",
      "Epoch: 0, Step: 405, Batch Loss: 0.12130754441022873\n",
      "Epoch: 0, Step: 406, Batch Loss: 0.1251867711544037\n",
      "Epoch: 0, Step: 407, Batch Loss: 0.12162699550390244\n",
      "Epoch: 0, Step: 408, Batch Loss: 0.11095238476991653\n",
      "Epoch: 0, Step: 409, Batch Loss: 0.11468185484409332\n",
      "Epoch: 0, Step: 410, Batch Loss: 0.1347455084323883\n",
      "Epoch: 0, Step: 411, Batch Loss: 0.12800762057304382\n",
      "Epoch: 0, Step: 412, Batch Loss: 0.11972818523645401\n",
      "Epoch: 0, Step: 413, Batch Loss: 0.22417525947093964\n",
      "Epoch: 0, Step: 414, Batch Loss: 0.1438571810722351\n",
      "Epoch: 0, Step: 415, Batch Loss: 0.1273549348115921\n",
      "Epoch: 0, Step: 416, Batch Loss: 0.1454344540834427\n",
      "Epoch: 0, Step: 417, Batch Loss: 0.1300102025270462\n",
      "Epoch: 0, Step: 418, Batch Loss: 0.1427304446697235\n",
      "Epoch: 0, Step: 419, Batch Loss: 0.12528465688228607\n",
      "Epoch: 0, Step: 420, Batch Loss: 0.1289447844028473\n",
      "Epoch: 0, Step: 421, Batch Loss: 0.19654829800128937\n",
      "Epoch: 0, Step: 422, Batch Loss: 0.22072352468967438\n",
      "Epoch: 0, Step: 423, Batch Loss: 0.12760986387729645\n",
      "Epoch: 0, Step: 424, Batch Loss: 0.12970229983329773\n",
      "Epoch: 0, Step: 425, Batch Loss: 0.09360701590776443\n",
      "Epoch: 0, Step: 426, Batch Loss: 0.16234534978866577\n",
      "Epoch: 0, Step: 427, Batch Loss: 0.14672119915485382\n",
      "Epoch: 0, Step: 428, Batch Loss: 0.14380398392677307\n",
      "Epoch: 0, Step: 429, Batch Loss: 0.10329820960760117\n",
      "Epoch: 0, Step: 430, Batch Loss: 0.1253810077905655\n",
      "Epoch: 0, Step: 431, Batch Loss: 0.15846127271652222\n",
      "Epoch: 0, Step: 432, Batch Loss: 0.1532057821750641\n",
      "Epoch: 0, Step: 433, Batch Loss: 0.14304374158382416\n",
      "Epoch: 0, Step: 434, Batch Loss: 0.12339874356985092\n",
      "Epoch: 0, Step: 435, Batch Loss: 0.14025232195854187\n",
      "Epoch: 0, Step: 436, Batch Loss: 0.09996113181114197\n",
      "Epoch: 0, Step: 437, Batch Loss: 0.13904544711112976\n",
      "Epoch: 0, Step: 438, Batch Loss: 0.1071326956152916\n",
      "Epoch: 0, Step: 439, Batch Loss: 0.12370948493480682\n",
      "Epoch: 0, Step: 440, Batch Loss: 0.11127378791570663\n",
      "Epoch: 0, Step: 441, Batch Loss: 0.14892220497131348\n",
      "Epoch: 0, Step: 442, Batch Loss: 0.1380932629108429\n",
      "Epoch: 0, Step: 443, Batch Loss: 0.12595117092132568\n",
      "Epoch: 0, Step: 444, Batch Loss: 0.1132078543305397\n",
      "Epoch: 0, Step: 445, Batch Loss: 0.13279034197330475\n",
      "Epoch: 0, Step: 446, Batch Loss: 0.10870135575532913\n",
      "Epoch: 0, Step: 447, Batch Loss: 0.1354571431875229\n",
      "Epoch: 0, Step: 448, Batch Loss: 0.14575040340423584\n",
      "Epoch: 0, Step: 449, Batch Loss: 0.16576461493968964\n",
      "Epoch: 0, Step: 450, Batch Loss: 0.11284249275922775\n",
      "Step: 450, Validation Loss: 0.1340661994916828, Validation Price Error (Normalized): 695.3947368421053\n",
      "Epoch: 0, Step: 451, Batch Loss: 0.12567968666553497\n",
      "Epoch: 0, Step: 452, Batch Loss: 0.11840847879648209\n",
      "Epoch: 0, Step: 453, Batch Loss: 0.10963692516088486\n",
      "Epoch: 0, Step: 454, Batch Loss: 0.1156560480594635\n",
      "Epoch: 0, Step: 455, Batch Loss: 0.14846549928188324\n",
      "Epoch: 0, Step: 456, Batch Loss: 0.15958644449710846\n",
      "Epoch: 0, Step: 457, Batch Loss: 0.09706342965364456\n",
      "Epoch: 0, Step: 458, Batch Loss: 0.18868686258792877\n",
      "Epoch: 0, Step: 459, Batch Loss: 0.18788683414459229\n",
      "Epoch: 0, Step: 460, Batch Loss: 0.12609998881816864\n",
      "Epoch: 0, Step: 461, Batch Loss: 0.11840847879648209\n",
      "Epoch: 0, Step: 462, Batch Loss: 0.15634173154830933\n",
      "Epoch: 0, Step: 463, Batch Loss: 0.21217605471611023\n",
      "Epoch: 0, Step: 464, Batch Loss: 0.10712774097919464\n",
      "Epoch: 0, Step: 465, Batch Loss: 0.13087135553359985\n",
      "Epoch: 0, Step: 466, Batch Loss: 0.13326787948608398\n",
      "Epoch: 0, Step: 467, Batch Loss: 0.1394556313753128\n",
      "Epoch: 0, Step: 468, Batch Loss: 0.11446141451597214\n",
      "Epoch: 0, Step: 469, Batch Loss: 0.12609361112117767\n",
      "Epoch: 0, Step: 470, Batch Loss: 0.1645674705505371\n",
      "Epoch: 0, Step: 471, Batch Loss: 0.19241972267627716\n",
      "Epoch: 0, Step: 472, Batch Loss: 0.12984828650951385\n",
      "Epoch: 0, Step: 473, Batch Loss: 0.131944477558136\n",
      "Epoch: 0, Step: 474, Batch Loss: 0.14137214422225952\n",
      "Epoch: 0, Step: 475, Batch Loss: 0.16956350207328796\n",
      "Epoch: 0, Step: 476, Batch Loss: 0.12091683596372604\n",
      "Epoch: 0, Step: 477, Batch Loss: 0.1434875875711441\n",
      "Epoch: 0, Step: 478, Batch Loss: 0.1498870551586151\n",
      "Epoch: 0, Step: 479, Batch Loss: 0.10621816664934158\n",
      "Epoch: 0, Step: 480, Batch Loss: 0.10869026929140091\n",
      "Epoch: 0, Step: 481, Batch Loss: 0.12442115694284439\n",
      "Epoch: 0, Step: 482, Batch Loss: 0.15502919256687164\n",
      "Epoch: 0, Step: 483, Batch Loss: 0.11721038818359375\n",
      "Epoch: 0, Step: 484, Batch Loss: 0.1421993523836136\n",
      "Epoch: 0, Step: 485, Batch Loss: 0.12150925397872925\n",
      "Epoch: 0, Step: 486, Batch Loss: 0.11345349252223969\n",
      "Epoch: 0, Step: 487, Batch Loss: 0.1291075199842453\n",
      "Epoch: 0, Step: 488, Batch Loss: 0.1925515979528427\n",
      "Epoch: 0, Step: 489, Batch Loss: 0.1321079283952713\n",
      "Epoch: 0, Step: 490, Batch Loss: 0.13710170984268188\n",
      "Epoch: 0, Step: 491, Batch Loss: 0.19192355871200562\n",
      "Epoch: 0, Step: 492, Batch Loss: 0.147023543715477\n",
      "Epoch: 0, Step: 493, Batch Loss: 0.1565551459789276\n",
      "Epoch: 0, Step: 494, Batch Loss: 0.14012591540813446\n",
      "Epoch: 0, Step: 495, Batch Loss: 0.1445627510547638\n",
      "Epoch: 0, Step: 496, Batch Loss: 0.10291256755590439\n",
      "Epoch: 0, Step: 497, Batch Loss: 0.11640705913305283\n",
      "Epoch: 0, Step: 498, Batch Loss: 0.10949505120515823\n",
      "Epoch: 0, Step: 499, Batch Loss: 0.15225785970687866\n",
      "Epoch: 0, Step: 500, Batch Loss: 0.1362651139497757\n",
      "Epoch: 0, Step: 501, Batch Loss: 0.1218116506934166\n",
      "Epoch: 0, Step: 502, Batch Loss: 0.15354551374912262\n",
      "Epoch: 0, Step: 503, Batch Loss: 0.10215483605861664\n",
      "Epoch: 0, Step: 504, Batch Loss: 0.10739525407552719\n",
      "Epoch: 0, Step: 505, Batch Loss: 0.11542630940675735\n",
      "Epoch: 0, Step: 506, Batch Loss: 0.2162531316280365\n",
      "Epoch: 0, Step: 507, Batch Loss: 0.1828337013721466\n",
      "Epoch: 0, Step: 508, Batch Loss: 0.13355226814746857\n",
      "Epoch: 0, Step: 509, Batch Loss: 0.10349182784557343\n",
      "Epoch: 0, Step: 510, Batch Loss: 0.10752671957015991\n",
      "Epoch: 0, Step: 511, Batch Loss: 0.11711136996746063\n",
      "Epoch: 0, Step: 512, Batch Loss: 0.12698741257190704\n",
      "Epoch: 0, Step: 513, Batch Loss: 0.1498185396194458\n",
      "Epoch: 0, Step: 514, Batch Loss: 0.11933863908052444\n",
      "Epoch: 0, Step: 515, Batch Loss: 0.1950964778661728\n",
      "Epoch: 0, Step: 516, Batch Loss: 0.11171264946460724\n",
      "Epoch: 0, Step: 517, Batch Loss: 0.13175621628761292\n",
      "Epoch: 0, Step: 518, Batch Loss: 0.1300632655620575\n",
      "Epoch: 0, Step: 519, Batch Loss: 0.13766039907932281\n",
      "Epoch: 0, Step: 520, Batch Loss: 0.19837269186973572\n",
      "Epoch: 0, Step: 521, Batch Loss: 0.11290493607521057\n",
      "Epoch: 0, Step: 522, Batch Loss: 0.17034706473350525\n",
      "Epoch: 0, Step: 523, Batch Loss: 0.1399291753768921\n",
      "Epoch: 0, Step: 524, Batch Loss: 0.11124349385499954\n",
      "Epoch: 0, Step: 525, Batch Loss: 0.10829047858715057\n",
      "Epoch: 0, Step: 526, Batch Loss: 0.11998848617076874\n",
      "Epoch: 0, Step: 527, Batch Loss: 0.17761899530887604\n",
      "Epoch: 0, Step: 528, Batch Loss: 0.12210402637720108\n",
      "Epoch: 0, Step: 529, Batch Loss: 0.12016083300113678\n",
      "Epoch: 0, Step: 530, Batch Loss: 0.11804866045713425\n",
      "Epoch: 0, Step: 531, Batch Loss: 0.1943272352218628\n",
      "Epoch: 0, Step: 532, Batch Loss: 0.12686283886432648\n",
      "Epoch: 0, Step: 533, Batch Loss: 0.1618042290210724\n",
      "Epoch: 0, Step: 534, Batch Loss: 0.14756974577903748\n",
      "Epoch: 0, Step: 535, Batch Loss: 0.11396429687738419\n",
      "Epoch: 0, Step: 536, Batch Loss: 0.13791534304618835\n",
      "Epoch: 0, Step: 537, Batch Loss: 0.12755148112773895\n",
      "Epoch: 0, Step: 538, Batch Loss: 0.11822779476642609\n",
      "Epoch: 0, Step: 539, Batch Loss: 0.13919183611869812\n",
      "Epoch: 0, Step: 540, Batch Loss: 0.12438754737377167\n",
      "Epoch: 0, Step: 541, Batch Loss: 0.15661635994911194\n",
      "Epoch: 0, Step: 542, Batch Loss: 0.12812238931655884\n",
      "Epoch: 0, Step: 543, Batch Loss: 0.10917606949806213\n",
      "Epoch: 0, Step: 544, Batch Loss: 0.20040105283260345\n",
      "Epoch: 0, Step: 545, Batch Loss: 0.1111440360546112\n",
      "Epoch: 0, Step: 546, Batch Loss: 0.11382605880498886\n",
      "Epoch: 0, Step: 547, Batch Loss: 0.11721489578485489\n",
      "Epoch: 0, Step: 548, Batch Loss: 0.1359625905752182\n",
      "Epoch: 0, Step: 549, Batch Loss: 0.1329096555709839\n",
      "Epoch: 0, Step: 550, Batch Loss: 0.11594855040311813\n",
      "Epoch: 0, Step: 551, Batch Loss: 0.11646678298711777\n",
      "Epoch: 0, Step: 552, Batch Loss: 0.12810483574867249\n",
      "Epoch: 0, Step: 553, Batch Loss: 0.11420412361621857\n",
      "Epoch: 0, Step: 554, Batch Loss: 0.12111620604991913\n",
      "Epoch: 0, Step: 555, Batch Loss: 0.12308136373758316\n",
      "Epoch: 0, Step: 556, Batch Loss: 0.14396552741527557\n",
      "Epoch: 0, Step: 557, Batch Loss: 0.12891128659248352\n",
      "Epoch: 0, Step: 558, Batch Loss: 0.16518908739089966\n",
      "Epoch: 0, Step: 559, Batch Loss: 0.12369340658187866\n",
      "Epoch: 0, Step: 560, Batch Loss: 0.1496671438217163\n",
      "Epoch: 0, Step: 561, Batch Loss: 0.186684250831604\n",
      "Epoch: 0, Step: 562, Batch Loss: 0.11545437574386597\n",
      "Epoch: 0, Step: 563, Batch Loss: 0.14178790152072906\n",
      "Epoch: 0, Step: 564, Batch Loss: 0.1736132949590683\n",
      "Epoch: 0, Step: 565, Batch Loss: 0.13254408538341522\n",
      "Epoch: 0, Step: 566, Batch Loss: 0.12751853466033936\n",
      "Epoch: 0, Step: 567, Batch Loss: 0.1254938691854477\n",
      "Epoch: 0, Step: 568, Batch Loss: 0.14141838252544403\n",
      "Epoch: 0, Step: 569, Batch Loss: 0.1257866472005844\n",
      "Epoch: 0, Step: 570, Batch Loss: 0.16020432114601135\n",
      "Epoch: 0, Step: 571, Batch Loss: 0.13449613749980927\n",
      "Epoch: 0, Step: 572, Batch Loss: 0.1126842200756073\n",
      "Epoch: 0, Step: 573, Batch Loss: 0.10387438535690308\n",
      "Epoch: 0, Step: 574, Batch Loss: 0.10680820047855377\n",
      "Epoch: 0, Step: 575, Batch Loss: 0.13065262138843536\n",
      "Epoch: 0, Step: 576, Batch Loss: 0.15452353656291962\n",
      "Epoch: 0, Step: 577, Batch Loss: 0.13957585394382477\n",
      "Epoch: 0, Step: 578, Batch Loss: 0.12115036696195602\n",
      "Epoch: 0, Step: 579, Batch Loss: 0.13985933363437653\n",
      "Epoch: 0, Step: 580, Batch Loss: 0.20057247579097748\n",
      "Epoch: 0, Step: 581, Batch Loss: 0.14004123210906982\n",
      "Epoch: 0, Step: 582, Batch Loss: 0.13652105629444122\n",
      "Epoch: 0, Step: 583, Batch Loss: 0.12147456407546997\n",
      "Epoch: 0, Step: 584, Batch Loss: 0.13510319590568542\n",
      "Epoch: 0, Step: 585, Batch Loss: 0.15358075499534607\n",
      "Epoch: 0, Step: 586, Batch Loss: 0.12441461533308029\n",
      "Epoch: 0, Step: 587, Batch Loss: 0.15427729487419128\n",
      "Epoch: 0, Step: 588, Batch Loss: 0.13322383165359497\n",
      "Epoch: 0, Step: 589, Batch Loss: 0.12146975100040436\n",
      "Epoch: 0, Step: 590, Batch Loss: 0.111066535115242\n",
      "Epoch: 0, Step: 591, Batch Loss: 0.1306600570678711\n",
      "Epoch: 0, Step: 592, Batch Loss: 0.13937123119831085\n",
      "Epoch: 0, Step: 593, Batch Loss: 0.1483703851699829\n",
      "Epoch: 0, Step: 594, Batch Loss: 0.12585972249507904\n",
      "Epoch: 0, Step: 595, Batch Loss: 0.10460104048252106\n",
      "Epoch: 0, Step: 596, Batch Loss: 0.150166317820549\n",
      "Epoch: 0, Step: 597, Batch Loss: 0.22348694503307343\n",
      "Epoch: 0, Step: 598, Batch Loss: 0.1531994640827179\n",
      "Epoch: 0, Step: 599, Batch Loss: 0.1474853903055191\n",
      "Epoch: 0, Step: 600, Batch Loss: 0.10665924847126007\n",
      "Step: 600, Validation Loss: 0.13590934426572762, Validation Price Error (Normalized): 652.5427631578947\n",
      "Epoch: 0, Step: 601, Batch Loss: 0.1229967474937439\n",
      "Epoch: 0, Step: 602, Batch Loss: 0.1131543219089508\n",
      "Epoch: 0, Step: 603, Batch Loss: 0.13148343563079834\n",
      "Epoch: 0, Step: 604, Batch Loss: 0.12598159909248352\n",
      "Epoch: 0, Step: 605, Batch Loss: 0.13048291206359863\n",
      "Epoch: 0, Step: 606, Batch Loss: 0.11136411875486374\n",
      "Epoch: 0, Step: 607, Batch Loss: 0.16863596439361572\n",
      "Epoch: 0, Step: 608, Batch Loss: 0.11585177481174469\n",
      "Epoch: 0, Step: 609, Batch Loss: 0.1250624656677246\n",
      "Epoch: 0, Step: 610, Batch Loss: 0.11173808574676514\n",
      "Epoch: 0, Step: 611, Batch Loss: 0.11902344226837158\n",
      "Epoch: 0, Step: 612, Batch Loss: 0.14417625963687897\n",
      "Epoch: 0, Step: 613, Batch Loss: 0.11610141396522522\n",
      "Epoch: 0, Step: 614, Batch Loss: 0.1535249501466751\n",
      "Epoch: 0, Step: 615, Batch Loss: 0.139863058924675\n",
      "Epoch: 0, Step: 616, Batch Loss: 0.15877388417720795\n",
      "Epoch: 0, Step: 617, Batch Loss: 0.13036024570465088\n",
      "Epoch: 0, Step: 618, Batch Loss: 0.128203883767128\n",
      "Epoch: 0, Step: 619, Batch Loss: 0.13081014156341553\n",
      "Epoch: 0, Step: 620, Batch Loss: 0.12668345868587494\n",
      "Epoch: 0, Step: 621, Batch Loss: 0.1332518309354782\n",
      "Epoch: 0, Step: 622, Batch Loss: 0.11747027188539505\n",
      "Epoch: 0, Step: 623, Batch Loss: 0.1424562931060791\n",
      "Epoch: 0, Step: 624, Batch Loss: 0.11605829745531082\n",
      "Epoch: 0, Step: 625, Batch Loss: 0.14121808111667633\n",
      "Epoch: 0, Step: 626, Batch Loss: 0.10374153405427933\n",
      "Epoch: 0, Step: 627, Batch Loss: 0.1521776169538498\n",
      "Epoch: 0, Step: 628, Batch Loss: 0.17497527599334717\n",
      "Epoch: 0, Step: 629, Batch Loss: 0.09869331121444702\n",
      "Epoch: 0, Step: 630, Batch Loss: 0.11679491400718689\n",
      "Epoch: 0, Step: 631, Batch Loss: 0.11656129360198975\n",
      "Epoch: 0, Step: 632, Batch Loss: 0.11877071857452393\n",
      "Epoch: 0, Step: 633, Batch Loss: 0.11816824972629547\n",
      "Epoch: 0, Step: 634, Batch Loss: 0.1191696748137474\n",
      "Epoch: 0, Step: 635, Batch Loss: 0.11538662016391754\n",
      "Epoch: 0, Step: 636, Batch Loss: 0.13858960568904877\n",
      "Epoch: 0, Step: 637, Batch Loss: 0.10928109288215637\n",
      "Epoch: 0, Step: 638, Batch Loss: 0.17348316311836243\n",
      "Epoch: 0, Step: 639, Batch Loss: 0.15254175662994385\n",
      "Epoch: 0, Step: 640, Batch Loss: 0.13691751658916473\n",
      "Epoch: 0, Step: 641, Batch Loss: 0.20406149327754974\n",
      "Epoch: 0, Step: 642, Batch Loss: 0.12182760238647461\n",
      "Epoch: 0, Step: 643, Batch Loss: 0.14191190898418427\n",
      "Epoch: 0, Step: 644, Batch Loss: 0.1478717178106308\n",
      "Epoch: 0, Step: 645, Batch Loss: 0.10419902950525284\n",
      "Epoch: 0, Step: 646, Batch Loss: 0.12263240665197372\n",
      "Epoch: 0, Step: 647, Batch Loss: 0.147646963596344\n",
      "Epoch: 0, Step: 648, Batch Loss: 0.14862306416034698\n",
      "Epoch: 0, Step: 649, Batch Loss: 0.1127244308590889\n",
      "Epoch: 0, Step: 650, Batch Loss: 0.12050383538007736\n",
      "Epoch: 0, Step: 651, Batch Loss: 0.1103716641664505\n",
      "Epoch: 0, Step: 652, Batch Loss: 0.159835547208786\n",
      "Epoch: 0, Step: 653, Batch Loss: 0.12412247806787491\n",
      "Epoch: 0, Step: 654, Batch Loss: 0.10869307816028595\n",
      "Epoch: 0, Step: 655, Batch Loss: 0.12523360550403595\n",
      "Epoch: 0, Step: 656, Batch Loss: 0.12633340060710907\n",
      "Epoch: 0, Step: 657, Batch Loss: 0.14116482436656952\n",
      "Epoch: 0, Step: 658, Batch Loss: 0.12701523303985596\n",
      "Epoch: 0, Step: 659, Batch Loss: 0.12944039702415466\n",
      "Epoch: 0, Step: 660, Batch Loss: 0.17453888058662415\n",
      "Epoch: 0, Step: 661, Batch Loss: 0.10339772701263428\n",
      "Epoch: 0, Step: 662, Batch Loss: 0.11508136987686157\n",
      "Epoch: 0, Step: 663, Batch Loss: 0.15839284658432007\n",
      "Epoch: 0, Step: 664, Batch Loss: 0.12026672810316086\n",
      "Epoch: 0, Step: 665, Batch Loss: 0.12868443131446838\n",
      "Epoch: 0, Step: 666, Batch Loss: 0.11651190370321274\n",
      "Epoch: 0, Step: 667, Batch Loss: 0.1280873417854309\n",
      "Epoch: 0, Step: 668, Batch Loss: 0.1354336440563202\n",
      "Epoch: 0, Step: 669, Batch Loss: 0.1565367728471756\n",
      "Epoch: 0, Step: 670, Batch Loss: 0.12757600843906403\n",
      "Epoch: 0, Step: 671, Batch Loss: 0.14067387580871582\n",
      "Epoch: 0, Step: 672, Batch Loss: 0.16075187921524048\n",
      "Epoch: 0, Step: 673, Batch Loss: 0.13927467167377472\n",
      "Epoch: 0, Step: 674, Batch Loss: 0.11301636695861816\n",
      "Epoch: 0, Step: 675, Batch Loss: 0.1514357626438141\n",
      "Epoch: 0, Step: 676, Batch Loss: 0.13499382138252258\n",
      "Epoch: 0, Step: 677, Batch Loss: 0.2181611806154251\n",
      "Epoch: 0, Step: 678, Batch Loss: 0.11377647519111633\n",
      "Epoch: 0, Step: 679, Batch Loss: 0.18838120996952057\n",
      "Epoch: 0, Step: 680, Batch Loss: 0.1293928176164627\n",
      "Epoch: 0, Step: 681, Batch Loss: 0.16812457144260406\n",
      "Epoch: 0, Step: 682, Batch Loss: 0.1673261970281601\n",
      "Epoch: 0, Step: 683, Batch Loss: 0.11009258031845093\n",
      "Epoch: 0, Step: 684, Batch Loss: 0.12055406719446182\n",
      "Epoch: 0, Step: 685, Batch Loss: 0.10282007604837418\n",
      "Epoch: 0, Step: 686, Batch Loss: 0.1171431839466095\n",
      "Epoch: 0, Step: 687, Batch Loss: 0.10165565460920334\n",
      "Epoch: 0, Step: 688, Batch Loss: 0.15143007040023804\n",
      "Epoch: 0, Step: 689, Batch Loss: 0.12155844271183014\n",
      "Epoch: 0, Step: 690, Batch Loss: 0.12199565023183823\n",
      "Epoch: 0, Step: 691, Batch Loss: 0.12791544198989868\n",
      "Epoch: 0, Step: 692, Batch Loss: 0.11935184895992279\n",
      "Epoch: 0, Step: 693, Batch Loss: 0.14123626053333282\n",
      "Epoch: 0, Step: 694, Batch Loss: 0.11853652447462082\n",
      "Epoch: 0, Step: 695, Batch Loss: 0.1765034794807434\n",
      "Epoch: 0, Step: 696, Batch Loss: 0.12262924015522003\n",
      "Epoch: 0, Step: 697, Batch Loss: 0.12676537036895752\n",
      "Epoch: 0, Step: 698, Batch Loss: 0.15520834922790527\n",
      "Epoch: 0, Step: 699, Batch Loss: 0.10484959930181503\n",
      "Epoch: 0, Step: 700, Batch Loss: 0.11666227877140045\n",
      "Epoch: 0, Step: 701, Batch Loss: 0.13225102424621582\n",
      "Epoch: 0, Step: 702, Batch Loss: 0.14563964307308197\n",
      "Epoch: 0, Step: 703, Batch Loss: 0.12747891247272491\n",
      "Epoch: 0, Step: 704, Batch Loss: 0.1583878993988037\n",
      "Epoch: 0, Step: 705, Batch Loss: 0.10864818841218948\n",
      "Epoch: 0, Step: 706, Batch Loss: 0.10794844478368759\n",
      "Epoch: 0, Step: 707, Batch Loss: 0.12488532811403275\n",
      "Epoch: 0, Step: 708, Batch Loss: 0.12026137113571167\n",
      "Epoch: 0, Step: 709, Batch Loss: 0.1323801875114441\n",
      "Epoch: 0, Step: 710, Batch Loss: 0.12709470093250275\n",
      "Epoch: 0, Step: 711, Batch Loss: 0.11614587903022766\n",
      "Epoch: 0, Step: 712, Batch Loss: 0.11535836011171341\n",
      "Epoch: 0, Step: 713, Batch Loss: 0.13566750288009644\n",
      "Epoch: 0, Step: 714, Batch Loss: 0.16747616231441498\n",
      "Epoch: 0, Step: 715, Batch Loss: 0.10169468075037003\n",
      "Epoch: 0, Step: 716, Batch Loss: 0.15893568098545074\n",
      "Epoch: 0, Step: 717, Batch Loss: 0.1466723531484604\n",
      "Epoch: 0, Step: 718, Batch Loss: 0.17438332736492157\n",
      "Epoch: 0, Step: 719, Batch Loss: 0.11386360973119736\n",
      "Epoch: 0, Step: 720, Batch Loss: 0.11043059825897217\n",
      "Epoch: 0, Step: 721, Batch Loss: 0.1099565178155899\n",
      "Epoch: 0, Step: 722, Batch Loss: 0.13020101189613342\n",
      "Epoch: 0, Step: 723, Batch Loss: 0.1830192655324936\n",
      "Epoch: 0, Step: 724, Batch Loss: 0.14388294517993927\n",
      "Epoch: 0, Step: 725, Batch Loss: 0.1384476125240326\n",
      "Epoch: 0, Step: 726, Batch Loss: 0.13629378378391266\n",
      "Epoch: 0, Step: 727, Batch Loss: 0.12646742165088654\n",
      "Epoch: 0, Step: 728, Batch Loss: 0.11369828879833221\n",
      "Epoch: 0, Step: 729, Batch Loss: 0.11549721658229828\n",
      "Epoch: 0, Step: 730, Batch Loss: 0.10004182904958725\n",
      "Epoch: 0, Step: 731, Batch Loss: 0.14449235796928406\n",
      "Epoch: 0, Step: 732, Batch Loss: 0.16513657569885254\n",
      "Epoch: 0, Step: 733, Batch Loss: 0.14137303829193115\n",
      "Epoch: 0, Step: 734, Batch Loss: 0.11754173785448074\n",
      "Epoch: 0, Step: 735, Batch Loss: 0.1343991607427597\n",
      "Epoch: 0, Step: 736, Batch Loss: 0.17232224345207214\n",
      "Epoch: 0, Step: 737, Batch Loss: 0.1522264927625656\n",
      "Epoch: 0, Step: 738, Batch Loss: 0.13596661388874054\n",
      "Epoch: 0, Step: 739, Batch Loss: 0.11313964426517487\n",
      "Epoch: 0, Step: 740, Batch Loss: 0.1882009655237198\n",
      "Epoch: 0, Step: 741, Batch Loss: 0.1377314031124115\n",
      "Epoch: 0, Step: 742, Batch Loss: 0.12323693186044693\n",
      "Epoch: 0, Step: 743, Batch Loss: 0.1433573067188263\n",
      "Epoch: 0, Step: 744, Batch Loss: 0.17289258539676666\n",
      "Epoch: 0, Step: 745, Batch Loss: 0.12900543212890625\n",
      "Epoch: 0, Step: 746, Batch Loss: 0.13460855185985565\n",
      "Epoch: 0, Step: 747, Batch Loss: 0.10936115682125092\n",
      "Epoch: 0, Step: 748, Batch Loss: 0.1581042855978012\n",
      "Epoch: 0, Step: 749, Batch Loss: 0.13585424423217773\n",
      "Epoch: 0, Step: 750, Batch Loss: 0.14135991036891937\n",
      "Step: 750, Validation Loss: 0.13681193943576594, Validation Price Error (Normalized): 514.8157894736842\n",
      "Epoch: 0, Step: 751, Batch Loss: 0.1225472018122673\n",
      "Epoch: 0, Step: 752, Batch Loss: 0.10441477596759796\n",
      "Epoch: 0, Step: 753, Batch Loss: 0.12590529024600983\n",
      "Epoch: 0, Step: 754, Batch Loss: 0.12412700802087784\n",
      "Epoch: 0, Step: 755, Batch Loss: 0.19305811822414398\n",
      "Epoch: 0, Step: 756, Batch Loss: 0.1552312672138214\n",
      "Epoch: 0, Step: 757, Batch Loss: 0.15342862904071808\n",
      "Epoch: 0, Step: 758, Batch Loss: 0.15630455315113068\n",
      "Epoch: 0, Step: 759, Batch Loss: 0.1361575424671173\n",
      "Epoch: 0, Step: 760, Batch Loss: 0.12461633235216141\n",
      "Epoch: 0, Step: 761, Batch Loss: 0.1982247680425644\n",
      "Epoch: 0, Step: 762, Batch Loss: 0.17464733123779297\n",
      "Epoch: 0, Step: 763, Batch Loss: 0.12264114618301392\n",
      "Epoch: 0, Step: 764, Batch Loss: 0.16242803633213043\n",
      "Epoch: 0, Step: 765, Batch Loss: 0.1427164226770401\n",
      "Epoch: 0, Step: 766, Batch Loss: 0.11539901793003082\n",
      "Epoch: 0, Step: 767, Batch Loss: 0.11805099248886108\n",
      "Epoch: 0, Step: 768, Batch Loss: 0.1599491536617279\n",
      "Epoch: 0, Step: 769, Batch Loss: 0.12544693052768707\n",
      "Epoch: 0, Step: 770, Batch Loss: 0.12463095784187317\n",
      "Epoch: 0, Step: 771, Batch Loss: 0.12655238807201385\n",
      "Epoch: 0, Step: 772, Batch Loss: 0.1547490805387497\n",
      "Epoch: 0, Step: 773, Batch Loss: 0.1529170125722885\n",
      "Epoch: 0, Step: 774, Batch Loss: 0.149738609790802\n",
      "Epoch: 0, Step: 775, Batch Loss: 0.11966346949338913\n",
      "Epoch: 0, Step: 776, Batch Loss: 0.12405167520046234\n",
      "Epoch: 0, Step: 777, Batch Loss: 0.1601542830467224\n",
      "Epoch: 0, Step: 778, Batch Loss: 0.13890205323696136\n",
      "Epoch: 0, Step: 779, Batch Loss: 0.11331257969141006\n",
      "Epoch: 0, Step: 780, Batch Loss: 0.12600794434547424\n",
      "Epoch: 0, Step: 781, Batch Loss: 0.10563221573829651\n",
      "Epoch: 0, Step: 782, Batch Loss: 0.1874278038740158\n",
      "Epoch: 0, Step: 783, Batch Loss: 0.12497080117464066\n",
      "Epoch: 0, Step: 784, Batch Loss: 0.14661388099193573\n",
      "Epoch: 0, Step: 785, Batch Loss: 0.11887582391500473\n",
      "Epoch: 0, Step: 786, Batch Loss: 0.11855946481227875\n",
      "Epoch: 0, Step: 787, Batch Loss: 0.14000843465328217\n",
      "Epoch: 0, Step: 788, Batch Loss: 0.15297022461891174\n",
      "Epoch: 0, Step: 789, Batch Loss: 0.14419761300086975\n",
      "Epoch: 0, Step: 790, Batch Loss: 0.12063046544790268\n",
      "Epoch: 0, Step: 791, Batch Loss: 0.1510591357946396\n",
      "Epoch: 0, Step: 792, Batch Loss: 0.11544454842805862\n",
      "Epoch: 0, Step: 793, Batch Loss: 0.16449503600597382\n",
      "Epoch: 0, Step: 794, Batch Loss: 0.12434270977973938\n",
      "Epoch: 0, Step: 795, Batch Loss: 0.16543355584144592\n",
      "Epoch: 0, Step: 796, Batch Loss: 0.11147324740886688\n",
      "Epoch: 0, Step: 797, Batch Loss: 0.14577758312225342\n",
      "Epoch: 0, Step: 798, Batch Loss: 0.13869115710258484\n",
      "Epoch: 0, Step: 799, Batch Loss: 0.13644562661647797\n",
      "Epoch: 0, Step: 800, Batch Loss: 0.12798307836055756\n",
      "Epoch: 0, Step: 801, Batch Loss: 0.16421954333782196\n",
      "Epoch: 0, Step: 802, Batch Loss: 0.12608826160430908\n",
      "Epoch: 0, Step: 803, Batch Loss: 0.10906220227479935\n",
      "Epoch: 0, Step: 804, Batch Loss: 0.14853744208812714\n",
      "Epoch: 0, Step: 805, Batch Loss: 0.18775200843811035\n",
      "Epoch: 0, Step: 806, Batch Loss: 0.15370142459869385\n",
      "Epoch: 0, Step: 807, Batch Loss: 0.1674918532371521\n",
      "Epoch: 0, Step: 808, Batch Loss: 0.1125221997499466\n",
      "Epoch: 0, Step: 809, Batch Loss: 0.13460958003997803\n",
      "Epoch: 0, Step: 810, Batch Loss: 0.1384887397289276\n",
      "Epoch: 0, Step: 811, Batch Loss: 0.11830584704875946\n",
      "Epoch: 0, Step: 812, Batch Loss: 0.12434270977973938\n",
      "Epoch: 0, Step: 813, Batch Loss: 0.11560974270105362\n",
      "Epoch: 0, Step: 814, Batch Loss: 0.1430804580450058\n",
      "Epoch: 0, Step: 815, Batch Loss: 0.17039251327514648\n",
      "Epoch: 0, Step: 816, Batch Loss: 0.17176157236099243\n",
      "Epoch: 0, Step: 817, Batch Loss: 0.17192547023296356\n",
      "Epoch: 0, Step: 818, Batch Loss: 0.11212920397520065\n",
      "Epoch: 0, Step: 819, Batch Loss: 0.1483999490737915\n",
      "Epoch: 0, Step: 820, Batch Loss: 0.12711374461650848\n",
      "Epoch: 0, Step: 821, Batch Loss: 0.11324232071638107\n",
      "Epoch: 0, Step: 822, Batch Loss: 0.13655324280261993\n",
      "Epoch: 0, Step: 823, Batch Loss: 0.1322961300611496\n",
      "Epoch: 0, Step: 824, Batch Loss: 0.12872979044914246\n",
      "Epoch: 0, Step: 825, Batch Loss: 0.18162259459495544\n",
      "Epoch: 0, Step: 826, Batch Loss: 0.19848722219467163\n",
      "Epoch: 0, Step: 827, Batch Loss: 0.11929982155561447\n",
      "Epoch: 0, Step: 828, Batch Loss: 0.1488911658525467\n",
      "Epoch: 0, Step: 829, Batch Loss: 0.1286199688911438\n",
      "Epoch: 0, Step: 830, Batch Loss: 0.16020771861076355\n",
      "Epoch: 0, Step: 831, Batch Loss: 0.14483220875263214\n",
      "Epoch: 0, Step: 832, Batch Loss: 0.15375995635986328\n",
      "Epoch: 0, Step: 833, Batch Loss: 0.10728644579648972\n",
      "Epoch: 0, Step: 834, Batch Loss: 0.14750221371650696\n",
      "Epoch: 0, Step: 835, Batch Loss: 0.11739329993724823\n",
      "Epoch: 0, Step: 836, Batch Loss: 0.11462307721376419\n",
      "Epoch: 0, Step: 837, Batch Loss: 0.1351424753665924\n",
      "Epoch: 0, Step: 838, Batch Loss: 0.1145143061876297\n",
      "Epoch: 0, Step: 839, Batch Loss: 0.10767045617103577\n",
      "Epoch: 0, Step: 840, Batch Loss: 0.16899457573890686\n",
      "Epoch: 0, Step: 841, Batch Loss: 0.11446636915206909\n",
      "Epoch: 0, Step: 842, Batch Loss: 0.13232135772705078\n",
      "Epoch: 0, Step: 843, Batch Loss: 0.15885029733181\n",
      "Epoch: 0, Step: 844, Batch Loss: 0.14105968177318573\n",
      "Epoch: 0, Step: 845, Batch Loss: 0.13278701901435852\n",
      "Epoch: 0, Step: 846, Batch Loss: 0.137826070189476\n",
      "Epoch: 0, Step: 847, Batch Loss: 0.16958479583263397\n",
      "Epoch: 0, Step: 848, Batch Loss: 0.1288299411535263\n",
      "Epoch: 0, Step: 849, Batch Loss: 0.117772176861763\n",
      "Epoch: 0, Step: 850, Batch Loss: 0.11673616617918015\n",
      "Epoch: 0, Step: 851, Batch Loss: 0.12395354360342026\n",
      "Epoch: 0, Step: 852, Batch Loss: 0.1770092248916626\n",
      "Epoch: 0, Step: 853, Batch Loss: 0.15301774442195892\n",
      "Epoch: 0, Step: 854, Batch Loss: 0.1343567669391632\n",
      "Epoch: 0, Step: 855, Batch Loss: 0.1460435539484024\n",
      "Epoch: 0, Step: 856, Batch Loss: 0.12061610817909241\n",
      "Epoch: 0, Step: 857, Batch Loss: 0.12974593043327332\n",
      "Epoch: 0, Step: 858, Batch Loss: 0.11588340997695923\n",
      "Epoch: 0, Step: 859, Batch Loss: 0.12104620784521103\n",
      "Epoch: 0, Step: 860, Batch Loss: 0.12480640411376953\n",
      "Epoch: 0, Step: 861, Batch Loss: 0.17984385788440704\n",
      "Epoch: 0, Step: 862, Batch Loss: 0.13185809552669525\n",
      "Epoch: 0, Step: 863, Batch Loss: 0.1525382697582245\n",
      "Epoch: 0, Step: 864, Batch Loss: 0.1232413798570633\n",
      "Epoch: 0, Step: 865, Batch Loss: 0.13196948170661926\n",
      "Epoch: 0, Step: 866, Batch Loss: 0.12446322292089462\n",
      "Epoch: 0, Step: 867, Batch Loss: 0.12923797965049744\n",
      "Epoch: 0, Step: 868, Batch Loss: 0.13785389065742493\n",
      "Epoch: 0, Step: 869, Batch Loss: 0.12982714176177979\n",
      "Epoch: 0, Step: 870, Batch Loss: 0.11516241729259491\n",
      "Epoch: 0, Step: 871, Batch Loss: 0.13848300278186798\n",
      "Epoch: 0, Step: 872, Batch Loss: 0.11484555155038834\n",
      "Epoch: 0, Step: 873, Batch Loss: 0.11823666095733643\n",
      "Epoch: 0, Step: 874, Batch Loss: 0.13844254612922668\n",
      "Epoch: 0, Step: 875, Batch Loss: 0.1712164431810379\n",
      "Epoch: 0, Step: 876, Batch Loss: 0.13352139294147491\n",
      "Epoch: 0, Step: 877, Batch Loss: 0.21640343964099884\n",
      "Epoch: 0, Step: 878, Batch Loss: 0.15668871998786926\n",
      "Epoch: 0, Step: 879, Batch Loss: 0.20859551429748535\n",
      "Epoch: 0, Step: 880, Batch Loss: 0.12772230803966522\n",
      "Epoch: 0, Step: 881, Batch Loss: 0.199881911277771\n",
      "Epoch: 0, Step: 882, Batch Loss: 0.18674540519714355\n",
      "Epoch: 0, Step: 883, Batch Loss: 0.13029377162456512\n",
      "Epoch: 0, Step: 884, Batch Loss: 0.19009597599506378\n",
      "Epoch: 0, Step: 885, Batch Loss: 0.12235698848962784\n",
      "Epoch: 0, Step: 886, Batch Loss: 0.15224966406822205\n",
      "Epoch: 0, Step: 887, Batch Loss: 0.11830416321754456\n",
      "Epoch: 0, Step: 888, Batch Loss: 0.11172524839639664\n",
      "Epoch: 0, Step: 889, Batch Loss: 0.13912542164325714\n",
      "Epoch: 0, Step: 890, Batch Loss: 0.1499926596879959\n",
      "Epoch: 0, Step: 891, Batch Loss: 0.1142241582274437\n",
      "Epoch: 0, Step: 892, Batch Loss: 0.1543976366519928\n",
      "Epoch: 0, Step: 893, Batch Loss: 0.11535593122243881\n",
      "Epoch: 0, Step: 894, Batch Loss: 0.11110229045152664\n",
      "Epoch: 0, Step: 895, Batch Loss: 0.13810458779335022\n",
      "Epoch: 0, Step: 896, Batch Loss: 0.15244464576244354\n",
      "Epoch: 0, Step: 897, Batch Loss: 0.10952948033809662\n",
      "Epoch: 0, Step: 898, Batch Loss: 0.11497067660093307\n",
      "Epoch: 0, Step: 899, Batch Loss: 0.17447859048843384\n",
      "Epoch: 0, Step: 900, Batch Loss: 0.1200021281838417\n",
      "Step: 900, Validation Loss: 0.13583674546527236, Validation Price Error (Normalized): 523.9078947368421\n",
      "Epoch: 0, Step: 901, Batch Loss: 0.13319654762744904\n",
      "Epoch: 0, Step: 902, Batch Loss: 0.13609394431114197\n",
      "Epoch: 0, Step: 903, Batch Loss: 0.11303937435150146\n",
      "Epoch: 0, Step: 904, Batch Loss: 0.1346321851015091\n",
      "Epoch: 0, Step: 905, Batch Loss: 0.14913871884346008\n",
      "Epoch: 0, Step: 906, Batch Loss: 0.1451098471879959\n",
      "Epoch: 0, Step: 907, Batch Loss: 0.14486746490001678\n",
      "Epoch: 0, Step: 908, Batch Loss: 0.19492188096046448\n",
      "Epoch: 0, Step: 909, Batch Loss: 0.1297561079263687\n",
      "Epoch: 0, Step: 910, Batch Loss: 0.10733985155820847\n",
      "Epoch: 0, Step: 911, Batch Loss: 0.1382947415113449\n",
      "Epoch: 0, Step: 912, Batch Loss: 0.14246605336666107\n",
      "Epoch: 0, Step: 913, Batch Loss: 0.1252071112394333\n",
      "Epoch: 0, Step: 914, Batch Loss: 0.11892622709274292\n",
      "Epoch: 0, Step: 915, Batch Loss: 0.15161529183387756\n",
      "Epoch: 0, Step: 916, Batch Loss: 0.15386101603507996\n",
      "Epoch: 0, Step: 917, Batch Loss: 0.13201525807380676\n",
      "Epoch: 0, Step: 918, Batch Loss: 0.1182832419872284\n",
      "Epoch: 0, Step: 919, Batch Loss: 0.127813458442688\n",
      "Epoch: 0, Step: 920, Batch Loss: 0.13524949550628662\n",
      "Epoch: 0, Step: 921, Batch Loss: 0.1262461245059967\n",
      "Epoch: 0, Step: 922, Batch Loss: 0.12315081059932709\n",
      "Epoch: 0, Step: 923, Batch Loss: 0.12463371455669403\n",
      "Epoch: 0, Step: 924, Batch Loss: 0.14540888369083405\n",
      "Epoch: 0, Step: 925, Batch Loss: 0.1368011236190796\n",
      "Epoch: 0, Step: 926, Batch Loss: 0.12209438532590866\n",
      "Epoch: 0, Step: 927, Batch Loss: 0.18506880104541779\n",
      "Epoch: 0, Step: 928, Batch Loss: 0.12743471562862396\n",
      "Epoch: 0, Step: 929, Batch Loss: 0.13769304752349854\n",
      "Epoch: 0, Step: 930, Batch Loss: 0.11399561911821365\n",
      "Epoch: 0, Step: 931, Batch Loss: 0.19358254969120026\n",
      "Epoch: 0, Step: 932, Batch Loss: 0.14486746490001678\n",
      "Epoch: 0, Step: 933, Batch Loss: 0.11614877730607986\n",
      "Epoch: 0, Step: 934, Batch Loss: 0.13714919984340668\n",
      "Epoch: 0, Step: 935, Batch Loss: 0.10827920585870743\n",
      "Epoch: 0, Step: 936, Batch Loss: 0.12456069886684418\n",
      "Epoch: 0, Step: 937, Batch Loss: 0.1507510095834732\n",
      "Epoch: 0, Step: 938, Batch Loss: 0.11960873007774353\n",
      "Epoch: 0, Step: 939, Batch Loss: 0.1461709886789322\n",
      "Epoch: 0, Step: 940, Batch Loss: 0.1550895869731903\n",
      "Epoch: 0, Step: 941, Batch Loss: 0.12376481294631958\n",
      "Epoch: 0, Step: 942, Batch Loss: 0.10367575287818909\n",
      "Epoch: 0, Step: 943, Batch Loss: 0.1840662956237793\n",
      "Epoch: 0, Step: 944, Batch Loss: 0.17422612011432648\n",
      "Epoch: 0, Step: 945, Batch Loss: 0.11495150625705719\n",
      "Epoch: 0, Step: 946, Batch Loss: 0.133945032954216\n",
      "Epoch: 0, Step: 947, Batch Loss: 0.11464722454547882\n",
      "Epoch: 0, Step: 948, Batch Loss: 0.19580385088920593\n",
      "Epoch: 0, Step: 949, Batch Loss: 0.1326455920934677\n",
      "Epoch: 0, Step: 950, Batch Loss: 0.12190738320350647\n",
      "Epoch: 0, Step: 951, Batch Loss: 0.13543196022510529\n",
      "Epoch: 0, Step: 952, Batch Loss: 0.14574763178825378\n",
      "Epoch: 0, Step: 953, Batch Loss: 0.1552540361881256\n",
      "Epoch: 0, Step: 954, Batch Loss: 0.1716761291027069\n",
      "Epoch: 0, Step: 955, Batch Loss: 0.1150951161980629\n",
      "Epoch: 0, Step: 956, Batch Loss: 0.11284461617469788\n",
      "Epoch: 0, Step: 957, Batch Loss: 0.12857748568058014\n",
      "Epoch: 0, Step: 958, Batch Loss: 0.13927632570266724\n",
      "Epoch: 0, Step: 959, Batch Loss: 0.14651690423488617\n",
      "Epoch: 0, Step: 960, Batch Loss: 0.178323894739151\n",
      "Epoch: 0, Step: 961, Batch Loss: 0.1467200666666031\n",
      "Epoch: 0, Step: 962, Batch Loss: 0.11443757265806198\n",
      "Epoch: 0, Step: 963, Batch Loss: 0.13739030063152313\n",
      "Epoch: 0, Step: 964, Batch Loss: 0.15665501356124878\n",
      "Epoch: 0, Step: 965, Batch Loss: 0.12365397065877914\n",
      "Epoch: 0, Step: 966, Batch Loss: 0.14109520614147186\n",
      "Epoch: 0, Step: 967, Batch Loss: 0.12384593486785889\n",
      "Epoch: 0, Step: 968, Batch Loss: 0.13685986399650574\n",
      "Epoch: 0, Step: 969, Batch Loss: 0.12314709275960922\n",
      "Epoch: 0, Step: 970, Batch Loss: 0.12472233921289444\n",
      "Epoch: 0, Step: 971, Batch Loss: 0.10760114341974258\n",
      "Epoch: 0, Step: 972, Batch Loss: 0.1352861523628235\n",
      "Epoch: 0, Step: 973, Batch Loss: 0.12427526712417603\n",
      "Epoch: 0, Step: 974, Batch Loss: 0.11730443686246872\n",
      "Epoch: 0, Step: 975, Batch Loss: 0.1605561375617981\n",
      "Epoch: 0, Step: 976, Batch Loss: 0.137086883187294\n",
      "Epoch: 0, Step: 977, Batch Loss: 0.11996523290872574\n",
      "Epoch: 0, Step: 978, Batch Loss: 0.14455194771289825\n",
      "Epoch: 0, Step: 979, Batch Loss: 0.11660779267549515\n",
      "Epoch: 0, Step: 980, Batch Loss: 0.12234792113304138\n",
      "Epoch: 0, Step: 981, Batch Loss: 0.11778927594423294\n",
      "Epoch: 0, Step: 982, Batch Loss: 0.1412472128868103\n",
      "Epoch: 0, Step: 983, Batch Loss: 0.13864773511886597\n",
      "Epoch: 0, Step: 984, Batch Loss: 0.12241018563508987\n",
      "Epoch: 0, Step: 985, Batch Loss: 0.14503110945224762\n",
      "Epoch: 0, Step: 986, Batch Loss: 0.1069284975528717\n",
      "Epoch: 0, Step: 987, Batch Loss: 0.16103696823120117\n",
      "Epoch: 0, Step: 988, Batch Loss: 0.14155955612659454\n",
      "Epoch: 0, Step: 989, Batch Loss: 0.12378735095262527\n",
      "Epoch: 0, Step: 990, Batch Loss: 0.1600431352853775\n",
      "Epoch: 0, Step: 991, Batch Loss: 0.1286548376083374\n",
      "Epoch: 0, Step: 992, Batch Loss: 0.1584353744983673\n",
      "Epoch: 0, Step: 993, Batch Loss: 0.13676874339580536\n",
      "Epoch: 0, Step: 994, Batch Loss: 0.12491843849420547\n",
      "Epoch: 0, Step: 995, Batch Loss: 0.13568657636642456\n",
      "Epoch: 0, Step: 996, Batch Loss: 0.13895907998085022\n",
      "Epoch: 0, Step: 997, Batch Loss: 0.11575440317392349\n",
      "Epoch: 0, Step: 998, Batch Loss: 0.13757924735546112\n",
      "Epoch: 0, Step: 999, Batch Loss: 0.14597350358963013\n",
      "Epoch: 0, Step: 1000, Batch Loss: 0.14152611792087555\n",
      "Epoch: 0, Step: 1001, Batch Loss: 0.1372622698545456\n",
      "Epoch: 0, Step: 1002, Batch Loss: 0.125186488032341\n",
      "Epoch: 0, Step: 1003, Batch Loss: 0.12337203323841095\n",
      "Epoch: 0, Step: 1004, Batch Loss: 0.153497576713562\n",
      "Epoch: 0, Step: 1005, Batch Loss: 0.12134925276041031\n",
      "Epoch: 0, Step: 1006, Batch Loss: 0.13062436878681183\n",
      "Epoch: 0, Step: 1007, Batch Loss: 0.11518525332212448\n",
      "Epoch: 0, Step: 1008, Batch Loss: 0.1405964493751526\n",
      "Epoch: 0, Step: 1009, Batch Loss: 0.12420141696929932\n",
      "Epoch: 0, Step: 1010, Batch Loss: 0.11191160976886749\n",
      "Epoch: 0, Step: 1011, Batch Loss: 0.14456547796726227\n",
      "Epoch: 0, Step: 1012, Batch Loss: 0.14758644998073578\n",
      "Epoch: 0, Step: 1013, Batch Loss: 0.1520732045173645\n",
      "Epoch: 0, Step: 1014, Batch Loss: 0.12179828435182571\n",
      "Epoch: 0, Step: 1015, Batch Loss: 0.19592376053333282\n",
      "Epoch: 0, Step: 1016, Batch Loss: 0.10323870927095413\n",
      "Epoch: 0, Step: 1017, Batch Loss: 0.13797760009765625\n",
      "Epoch: 0, Step: 1018, Batch Loss: 0.14165520668029785\n",
      "Epoch: 0, Step: 1019, Batch Loss: 0.1561301350593567\n",
      "Epoch: 0, Step: 1020, Batch Loss: 0.19633106887340546\n",
      "Epoch: 0, Step: 1021, Batch Loss: 0.1335003674030304\n",
      "Epoch: 0, Step: 1022, Batch Loss: 0.1321633756160736\n",
      "Epoch: 0, Step: 1023, Batch Loss: 0.1475684940814972\n",
      "Epoch: 0, Step: 1024, Batch Loss: 0.12807023525238037\n",
      "Epoch: 0, Step: 1025, Batch Loss: 0.10132439434528351\n",
      "Epoch: 0, Step: 1026, Batch Loss: 0.10535725951194763\n",
      "Epoch: 0, Step: 1027, Batch Loss: 0.15866421163082123\n",
      "Epoch: 0, Step: 1028, Batch Loss: 0.17830850183963776\n",
      "Epoch: 0, Step: 1029, Batch Loss: 0.11506125330924988\n",
      "Epoch: 0, Step: 1030, Batch Loss: 0.1503671556711197\n",
      "Epoch: 0, Step: 1031, Batch Loss: 0.17095869779586792\n",
      "Epoch: 0, Step: 1032, Batch Loss: 0.14559094607830048\n",
      "Epoch: 0, Step: 1033, Batch Loss: 0.1106107234954834\n",
      "Epoch: 0, Step: 1034, Batch Loss: 0.14700059592723846\n",
      "Epoch: 0, Step: 1035, Batch Loss: 0.120547816157341\n",
      "Epoch: 0, Step: 1036, Batch Loss: 0.13867352902889252\n",
      "Epoch: 0, Step: 1037, Batch Loss: 0.129472017288208\n",
      "Epoch: 0, Step: 1038, Batch Loss: 0.09754608571529388\n",
      "Epoch: 0, Step: 1039, Batch Loss: 0.15301203727722168\n",
      "Epoch: 0, Step: 1040, Batch Loss: 0.13176722824573517\n",
      "Epoch: 0, Step: 1041, Batch Loss: 0.1132386177778244\n",
      "Epoch: 0, Step: 1042, Batch Loss: 0.11309774965047836\n",
      "Epoch: 0, Step: 1043, Batch Loss: 0.19176839292049408\n",
      "Epoch: 0, Step: 1044, Batch Loss: 0.11682993173599243\n",
      "Epoch: 0, Step: 1045, Batch Loss: 0.1493326872587204\n",
      "Epoch: 0, Step: 1046, Batch Loss: 0.14285001158714294\n",
      "Epoch: 0, Step: 1047, Batch Loss: 0.13077834248542786\n",
      "Epoch: 0, Step: 1048, Batch Loss: 0.13896925747394562\n",
      "Epoch: 0, Step: 1049, Batch Loss: 0.1151786595582962\n",
      "Epoch: 0, Step: 1050, Batch Loss: 0.13354116678237915\n",
      "Step: 1050, Validation Loss: 0.13200120395049453, Validation Price Error (Normalized): 494.0394736842105\n",
      "Epoch: 0, Step: 1051, Batch Loss: 0.1439119130373001\n",
      "Epoch: 0, Step: 1052, Batch Loss: 0.13015401363372803\n",
      "Epoch: 0, Step: 1053, Batch Loss: 0.11355483531951904\n",
      "Epoch: 0, Step: 1054, Batch Loss: 0.1318119466304779\n",
      "Epoch: 0, Step: 1055, Batch Loss: 0.14201009273529053\n",
      "Epoch: 0, Step: 1056, Batch Loss: 0.15007814764976501\n",
      "Epoch: 0, Step: 1057, Batch Loss: 0.15622572600841522\n",
      "Epoch: 0, Step: 1058, Batch Loss: 0.1585109382867813\n",
      "Epoch: 0, Step: 1059, Batch Loss: 0.1568664163351059\n",
      "Epoch: 0, Step: 1060, Batch Loss: 0.1505003124475479\n",
      "Epoch: 0, Step: 1061, Batch Loss: 0.14939461648464203\n",
      "Epoch: 0, Step: 1062, Batch Loss: 0.14437651634216309\n",
      "Epoch: 0, Step: 1063, Batch Loss: 0.12169330567121506\n",
      "Epoch: 0, Step: 1064, Batch Loss: 0.12202595919370651\n",
      "Epoch: 0, Step: 1065, Batch Loss: 0.10946215689182281\n",
      "Epoch: 0, Step: 1066, Batch Loss: 0.11770976334810257\n",
      "Epoch: 0, Step: 1067, Batch Loss: 0.12164296209812164\n",
      "Epoch: 0, Step: 1068, Batch Loss: 0.14087186753749847\n",
      "Epoch: 0, Step: 1069, Batch Loss: 0.17341987788677216\n",
      "Epoch: 0, Step: 1070, Batch Loss: 0.12784439325332642\n",
      "Epoch: 0, Step: 1071, Batch Loss: 0.10863296687602997\n",
      "Epoch: 0, Step: 1072, Batch Loss: 0.10527347773313522\n",
      "Epoch: 0, Step: 1073, Batch Loss: 0.11281134188175201\n",
      "Epoch: 0, Step: 1074, Batch Loss: 0.11315754055976868\n",
      "Epoch: 0, Step: 1075, Batch Loss: 0.11555483192205429\n",
      "Epoch: 0, Step: 1076, Batch Loss: 0.12012384831905365\n",
      "Epoch: 0, Step: 1077, Batch Loss: 0.11635306477546692\n",
      "Epoch: 0, Step: 1078, Batch Loss: 0.11569610238075256\n",
      "Epoch: 0, Step: 1079, Batch Loss: 0.12763303518295288\n",
      "Epoch: 0, Step: 1080, Batch Loss: 0.1316716969013214\n",
      "Epoch: 0, Step: 1081, Batch Loss: 0.14330099523067474\n",
      "Epoch: 0, Step: 1082, Batch Loss: 0.14296258985996246\n",
      "Epoch: 0, Step: 1083, Batch Loss: 0.11957935243844986\n",
      "Epoch: 0, Step: 1084, Batch Loss: 0.10801885277032852\n",
      "Epoch: 0, Step: 1085, Batch Loss: 0.10951283574104309\n",
      "Epoch: 0, Step: 1086, Batch Loss: 0.13029178977012634\n",
      "Epoch: 0, Step: 1087, Batch Loss: 0.11699797958135605\n",
      "Epoch: 0, Step: 1088, Batch Loss: 0.10637044906616211\n",
      "Epoch: 0, Step: 1089, Batch Loss: 0.14540936052799225\n",
      "Epoch: 0, Step: 1090, Batch Loss: 0.13809096813201904\n",
      "Epoch: 0, Step: 1091, Batch Loss: 0.13531818985939026\n",
      "Epoch: 0, Step: 1092, Batch Loss: 0.11981850117444992\n",
      "Epoch: 0, Step: 1093, Batch Loss: 0.12557290494441986\n",
      "Epoch: 0, Step: 1094, Batch Loss: 0.1236976832151413\n",
      "Epoch: 0, Step: 1095, Batch Loss: 0.11962541937828064\n",
      "Epoch: 0, Step: 1096, Batch Loss: 0.12593401968479156\n",
      "Epoch: 0, Step: 1097, Batch Loss: 0.1129741370677948\n",
      "Epoch: 0, Step: 1098, Batch Loss: 0.12763197720050812\n",
      "Epoch: 0, Step: 1099, Batch Loss: 0.14686517417430878\n",
      "Epoch: 0, Step: 1100, Batch Loss: 0.13777567446231842\n",
      "Epoch: 0, Step: 1101, Batch Loss: 0.1667310744524002\n",
      "Epoch: 0, Step: 1102, Batch Loss: 0.14024019241333008\n",
      "Epoch: 0, Step: 1103, Batch Loss: 0.1505277007818222\n",
      "Epoch: 0, Step: 1104, Batch Loss: 0.12292364984750748\n",
      "Epoch: 0, Step: 1105, Batch Loss: 0.12608854472637177\n",
      "Epoch: 0, Step: 1106, Batch Loss: 0.1447990983724594\n",
      "Epoch: 0, Step: 1107, Batch Loss: 0.11334822326898575\n",
      "Epoch: 0, Step: 1108, Batch Loss: 0.1440270096063614\n",
      "Epoch: 0, Step: 1109, Batch Loss: 0.1538449078798294\n",
      "Epoch: 0, Step: 1110, Batch Loss: 0.12556786835193634\n",
      "Epoch: 0, Step: 1111, Batch Loss: 0.13038817048072815\n",
      "Epoch: 0, Step: 1112, Batch Loss: 0.14614340662956238\n",
      "Epoch: 0, Step: 1113, Batch Loss: 0.15119494497776031\n",
      "Epoch: 0, Step: 1114, Batch Loss: 0.10777562856674194\n",
      "Epoch: 0, Step: 1115, Batch Loss: 0.12615810334682465\n",
      "Epoch: 0, Step: 1116, Batch Loss: 0.16641634702682495\n",
      "Epoch: 0, Step: 1117, Batch Loss: 0.15008433163166046\n",
      "Epoch: 0, Step: 1118, Batch Loss: 0.1757485419511795\n",
      "Epoch: 0, Step: 1119, Batch Loss: 0.11486919224262238\n",
      "Epoch: 0, Step: 1120, Batch Loss: 0.10906244814395905\n",
      "Epoch: 0, Step: 1121, Batch Loss: 0.16458557546138763\n",
      "Epoch: 0, Step: 1122, Batch Loss: 0.12827904522418976\n",
      "Epoch: 0, Step: 1123, Batch Loss: 0.15334373712539673\n",
      "Epoch: 0, Step: 1124, Batch Loss: 0.15945668518543243\n",
      "Epoch: 0, Step: 1125, Batch Loss: 0.12141624093055725\n",
      "Epoch: 0, Step: 1126, Batch Loss: 0.10512624680995941\n",
      "Epoch: 0, Step: 1127, Batch Loss: 0.11136165261268616\n",
      "Epoch: 0, Step: 1128, Batch Loss: 0.11456780135631561\n",
      "Epoch: 0, Step: 1129, Batch Loss: 0.11641021817922592\n",
      "Epoch: 0, Step: 1130, Batch Loss: 0.14263665676116943\n",
      "Epoch: 0, Step: 1131, Batch Loss: 0.1251923143863678\n",
      "Epoch: 0, Step: 1132, Batch Loss: 0.11431071162223816\n",
      "Epoch: 0, Step: 1133, Batch Loss: 0.11966050416231155\n",
      "Epoch: 0, Step: 1134, Batch Loss: 0.1605347990989685\n",
      "Epoch: 0, Step: 1135, Batch Loss: 0.13647282123565674\n",
      "Epoch: 0, Step: 1136, Batch Loss: 0.11284787207841873\n",
      "Epoch: 0, Step: 1137, Batch Loss: 0.11006386578083038\n",
      "Epoch: 0, Step: 1138, Batch Loss: 0.12579089403152466\n",
      "Epoch: 0, Step: 1139, Batch Loss: 0.11040478944778442\n",
      "Epoch: 0, Step: 1140, Batch Loss: 0.14883582293987274\n",
      "Epoch: 0, Step: 1141, Batch Loss: 0.10834965109825134\n",
      "Epoch: 0, Step: 1142, Batch Loss: 0.11585026979446411\n",
      "Epoch: 0, Step: 1143, Batch Loss: 0.11325342208147049\n",
      "Epoch: 0, Step: 1144, Batch Loss: 0.1283244788646698\n",
      "Epoch: 0, Step: 1145, Batch Loss: 0.12504857778549194\n",
      "Epoch: 0, Step: 1146, Batch Loss: 0.14660179615020752\n",
      "Epoch: 0, Step: 1147, Batch Loss: 0.11775025725364685\n",
      "Epoch: 0, Step: 1148, Batch Loss: 0.10892326384782791\n",
      "Epoch: 0, Step: 1149, Batch Loss: 0.13787272572517395\n",
      "Epoch: 0, Step: 1150, Batch Loss: 0.10663466155529022\n",
      "Epoch: 0, Step: 1151, Batch Loss: 0.11175067722797394\n",
      "Epoch: 0, Step: 1152, Batch Loss: 0.11326906830072403\n",
      "Epoch: 0, Step: 1153, Batch Loss: 0.17188343405723572\n",
      "Epoch: 0, Step: 1154, Batch Loss: 0.12700346112251282\n",
      "Epoch: 0, Step: 1155, Batch Loss: 0.11594782769680023\n",
      "Epoch: 0, Step: 1156, Batch Loss: 0.10481727123260498\n",
      "Epoch: 0, Step: 1157, Batch Loss: 0.11313977837562561\n",
      "Epoch: 0, Step: 1158, Batch Loss: 0.13477347791194916\n",
      "Epoch: 0, Step: 1159, Batch Loss: 0.1496553272008896\n",
      "Epoch: 0, Step: 1160, Batch Loss: 0.12609367072582245\n",
      "Epoch: 0, Step: 1161, Batch Loss: 0.10507228225469589\n",
      "Epoch: 0, Step: 1162, Batch Loss: 0.16379529237747192\n",
      "Epoch: 0, Step: 1163, Batch Loss: 0.10716697573661804\n",
      "Epoch: 0, Step: 1164, Batch Loss: 0.12433881312608719\n",
      "Epoch: 0, Step: 1165, Batch Loss: 0.11111230403184891\n",
      "Epoch: 0, Step: 1166, Batch Loss: 0.12808752059936523\n",
      "Epoch: 0, Step: 1167, Batch Loss: 0.11661316454410553\n",
      "Epoch: 0, Step: 1168, Batch Loss: 0.10953208059072495\n",
      "Epoch: 0, Step: 1169, Batch Loss: 0.13896498084068298\n",
      "Epoch: 0, Step: 1170, Batch Loss: 0.1340370774269104\n",
      "Epoch: 0, Step: 1171, Batch Loss: 0.1412702053785324\n",
      "Epoch: 0, Step: 1172, Batch Loss: 0.13272050023078918\n",
      "Epoch: 0, Step: 1173, Batch Loss: 0.1574285626411438\n",
      "Epoch: 0, Step: 1174, Batch Loss: 0.13147708773612976\n",
      "Epoch: 0, Step: 1175, Batch Loss: 0.1503545641899109\n",
      "Epoch: 0, Step: 1176, Batch Loss: 0.12436091154813766\n",
      "Epoch: 0, Step: 1177, Batch Loss: 0.10589618980884552\n",
      "Epoch: 0, Step: 1178, Batch Loss: 0.1202271431684494\n",
      "Epoch: 0, Step: 1179, Batch Loss: 0.10556299239397049\n",
      "Epoch: 0, Step: 1180, Batch Loss: 0.11175777018070221\n",
      "Epoch: 0, Step: 1181, Batch Loss: 0.1359405219554901\n",
      "Epoch: 0, Step: 1182, Batch Loss: 0.12549424171447754\n",
      "Epoch: 0, Step: 1183, Batch Loss: 0.10656692087650299\n",
      "Epoch: 0, Step: 1184, Batch Loss: 0.12203949689865112\n",
      "Epoch: 0, Step: 1185, Batch Loss: 0.1275729238986969\n",
      "Epoch: 0, Step: 1186, Batch Loss: 0.14316248893737793\n",
      "Epoch: 0, Step: 1187, Batch Loss: 0.16613991558551788\n",
      "Epoch: 0, Step: 1188, Batch Loss: 0.12798674404621124\n",
      "Epoch: 0, Step: 1189, Batch Loss: 0.1161189153790474\n",
      "Epoch: 0, Step: 1190, Batch Loss: 0.12218675762414932\n",
      "Epoch: 0, Step: 1191, Batch Loss: 0.13577526807785034\n",
      "Epoch: 0, Step: 1192, Batch Loss: 0.12008050084114075\n",
      "Epoch: 0, Step: 1193, Batch Loss: 0.1191524788737297\n",
      "Epoch: 0, Step: 1194, Batch Loss: 0.16481468081474304\n",
      "Epoch: 0, Step: 1195, Batch Loss: 0.13177838921546936\n",
      "Epoch: 0, Step: 1196, Batch Loss: 0.14270059764385223\n",
      "Epoch: 0, Step: 1197, Batch Loss: 0.1175495982170105\n",
      "Epoch: 0, Step: 1198, Batch Loss: 0.12516629695892334\n",
      "Epoch: 0, Step: 1199, Batch Loss: 0.10721283406019211\n",
      "Epoch: 0, Step: 1200, Batch Loss: 0.09794995933771133\n",
      "Step: 1200, Validation Loss: 0.1290428815409541, Validation Price Error (Normalized): 489.4868421052632\n",
      "Epoch: 0, Step: 1201, Batch Loss: 0.1272316575050354\n",
      "Epoch: 0, Step: 1202, Batch Loss: 0.1393815279006958\n",
      "Epoch: 0, Step: 1203, Batch Loss: 0.1127416342496872\n",
      "Epoch: 0, Step: 1204, Batch Loss: 0.11716131120920181\n",
      "Epoch: 0, Step: 1205, Batch Loss: 0.10563245415687561\n",
      "Epoch: 0, Step: 1206, Batch Loss: 0.13649490475654602\n",
      "Epoch: 0, Step: 1207, Batch Loss: 0.15511827170848846\n",
      "Epoch: 0, Step: 1208, Batch Loss: 0.14746546745300293\n",
      "Epoch: 0, Step: 1209, Batch Loss: 0.14803026616573334\n",
      "Epoch: 0, Step: 1210, Batch Loss: 0.12541413307189941\n",
      "Epoch: 0, Step: 1211, Batch Loss: 0.11008954793214798\n",
      "Epoch: 0, Step: 1212, Batch Loss: 0.10937919467687607\n",
      "Epoch: 0, Step: 1213, Batch Loss: 0.11153793334960938\n",
      "Epoch: 0, Step: 1214, Batch Loss: 0.10635270923376083\n",
      "Epoch: 0, Step: 1215, Batch Loss: 0.10612167418003082\n",
      "Epoch: 0, Step: 1216, Batch Loss: 0.12071732431650162\n",
      "Epoch: 0, Step: 1217, Batch Loss: 0.10832978039979935\n",
      "Epoch: 0, Step: 1218, Batch Loss: 0.17494115233421326\n",
      "Epoch: 0, Step: 1219, Batch Loss: 0.10481801629066467\n",
      "Epoch: 0, Step: 1220, Batch Loss: 0.1143132895231247\n",
      "Epoch: 0, Step: 1221, Batch Loss: 0.11263386905193329\n",
      "Epoch: 0, Step: 1222, Batch Loss: 0.10315371304750443\n",
      "Epoch: 0, Step: 1223, Batch Loss: 0.12213289737701416\n",
      "Epoch: 0, Step: 1224, Batch Loss: 0.13513784110546112\n",
      "Epoch: 0, Step: 1225, Batch Loss: 0.14844928681850433\n",
      "Epoch: 0, Step: 1226, Batch Loss: 0.10116957128047943\n",
      "Epoch: 0, Step: 1227, Batch Loss: 0.12676142156124115\n",
      "Epoch: 0, Step: 1228, Batch Loss: 0.15691691637039185\n",
      "Epoch: 0, Step: 1229, Batch Loss: 0.11423918604850769\n",
      "Epoch: 0, Step: 1230, Batch Loss: 0.11438823491334915\n",
      "Epoch: 0, Step: 1231, Batch Loss: 0.12831147015094757\n",
      "Epoch: 0, Step: 1232, Batch Loss: 0.11393433809280396\n",
      "Epoch: 0, Step: 1233, Batch Loss: 0.11406856030225754\n",
      "Epoch: 0, Step: 1234, Batch Loss: 0.11930187791585922\n",
      "Epoch: 0, Step: 1235, Batch Loss: 0.11132707446813583\n",
      "Epoch: 0, Step: 1236, Batch Loss: 0.11059857159852982\n",
      "Epoch: 0, Step: 1237, Batch Loss: 0.09646934270858765\n",
      "Epoch: 0, Step: 1238, Batch Loss: 0.16192828118801117\n",
      "Epoch: 0, Step: 1239, Batch Loss: 0.10898134857416153\n",
      "Epoch: 0, Step: 1240, Batch Loss: 0.11753641068935394\n",
      "Epoch: 0, Step: 1241, Batch Loss: 0.13197383284568787\n",
      "Epoch: 0, Step: 1242, Batch Loss: 0.11352087557315826\n",
      "Epoch: 0, Step: 1243, Batch Loss: 0.1207011342048645\n",
      "Epoch: 0, Step: 1244, Batch Loss: 0.11065276712179184\n",
      "Epoch: 0, Step: 1245, Batch Loss: 0.18357300758361816\n",
      "Epoch: 0, Step: 1246, Batch Loss: 0.11315233260393143\n",
      "Epoch: 0, Step: 1247, Batch Loss: 0.11721964925527573\n",
      "Epoch: 0, Step: 1248, Batch Loss: 0.1106683686375618\n",
      "Epoch: 0, Step: 1249, Batch Loss: 0.12247896939516068\n",
      "Epoch: 0, Step: 1250, Batch Loss: 0.10305748134851456\n",
      "Epoch: 0, Step: 1251, Batch Loss: 0.13547517359256744\n",
      "Epoch: 0, Step: 1252, Batch Loss: 0.11097054183483124\n",
      "Epoch: 0, Step: 1253, Batch Loss: 0.11997490376234055\n",
      "Epoch: 0, Step: 1254, Batch Loss: 0.12000774592161179\n",
      "Epoch: 0, Step: 1255, Batch Loss: 0.13990908861160278\n",
      "Epoch: 0, Step: 1256, Batch Loss: 0.11580651253461838\n",
      "Epoch: 0, Step: 1257, Batch Loss: 0.14382705092430115\n",
      "Epoch: 0, Step: 1258, Batch Loss: 0.11611567437648773\n",
      "Epoch: 0, Step: 1259, Batch Loss: 0.10515912622213364\n",
      "Epoch: 0, Step: 1260, Batch Loss: 0.18105852603912354\n",
      "Epoch: 0, Step: 1261, Batch Loss: 0.12457422912120819\n",
      "Epoch: 0, Step: 1262, Batch Loss: 0.15502531826496124\n",
      "Epoch: 0, Step: 1263, Batch Loss: 0.12521138787269592\n",
      "Epoch: 0, Step: 1264, Batch Loss: 0.12915991246700287\n",
      "Epoch: 0, Step: 1265, Batch Loss: 0.10896077752113342\n",
      "Epoch: 0, Step: 1266, Batch Loss: 0.11110934615135193\n",
      "Epoch: 0, Step: 1267, Batch Loss: 0.11103920638561249\n",
      "Epoch: 0, Step: 1268, Batch Loss: 0.16014260053634644\n",
      "Epoch: 0, Step: 1269, Batch Loss: 0.14124469459056854\n",
      "Epoch: 0, Step: 1270, Batch Loss: 0.11753641068935394\n",
      "Epoch: 0, Step: 1271, Batch Loss: 0.13802963495254517\n",
      "Epoch: 0, Step: 1272, Batch Loss: 0.10387340188026428\n",
      "Epoch: 0, Step: 1273, Batch Loss: 0.11693758517503738\n",
      "Epoch: 0, Step: 1274, Batch Loss: 0.1036088615655899\n",
      "Epoch: 0, Step: 1275, Batch Loss: 0.11511163413524628\n",
      "Epoch: 0, Step: 1276, Batch Loss: 0.13039901852607727\n",
      "Epoch: 0, Step: 1277, Batch Loss: 0.1470804512500763\n",
      "Epoch: 0, Step: 1278, Batch Loss: 0.1283293515443802\n",
      "Epoch: 0, Step: 1279, Batch Loss: 0.13620471954345703\n",
      "Epoch: 0, Step: 1280, Batch Loss: 0.1349560171365738\n",
      "Epoch: 0, Step: 1281, Batch Loss: 0.12804710865020752\n",
      "Epoch: 0, Step: 1282, Batch Loss: 0.10173927247524261\n",
      "Epoch: 0, Step: 1283, Batch Loss: 0.11384192854166031\n",
      "Epoch: 0, Step: 1284, Batch Loss: 0.11054380983114243\n",
      "Epoch: 0, Step: 1285, Batch Loss: 0.13946844637393951\n",
      "Epoch: 0, Step: 1286, Batch Loss: 0.12031266838312149\n",
      "Epoch: 0, Step: 1287, Batch Loss: 0.11332851648330688\n",
      "Epoch: 0, Step: 1288, Batch Loss: 0.11747915297746658\n",
      "Epoch: 0, Step: 1289, Batch Loss: 0.15714985132217407\n",
      "Epoch: 0, Step: 1290, Batch Loss: 0.12390337884426117\n",
      "Epoch: 0, Step: 1291, Batch Loss: 0.1393451988697052\n",
      "Epoch: 0, Step: 1292, Batch Loss: 0.13520561158657074\n",
      "Epoch: 0, Step: 1293, Batch Loss: 0.10204946994781494\n",
      "Epoch: 0, Step: 1294, Batch Loss: 0.12295098602771759\n",
      "Epoch: 0, Step: 1295, Batch Loss: 0.11476854979991913\n",
      "Epoch: 0, Step: 1296, Batch Loss: 0.12813936173915863\n",
      "Epoch: 0, Step: 1297, Batch Loss: 0.1284097284078598\n",
      "Epoch: 0, Step: 1298, Batch Loss: 0.11869416385889053\n",
      "Epoch: 0, Step: 1299, Batch Loss: 0.14702938497066498\n",
      "Epoch: 0, Step: 1300, Batch Loss: 0.11351780593395233\n",
      "Epoch: 0, Step: 1301, Batch Loss: 0.10133511573076248\n",
      "Epoch: 0, Step: 1302, Batch Loss: 0.1076333150267601\n",
      "Epoch: 0, Step: 1303, Batch Loss: 0.1319269984960556\n",
      "Epoch: 0, Step: 1304, Batch Loss: 0.13844983279705048\n",
      "Epoch: 0, Step: 1305, Batch Loss: 0.10213921219110489\n",
      "Epoch: 0, Step: 1306, Batch Loss: 0.1779029667377472\n",
      "Epoch: 0, Step: 1307, Batch Loss: 0.13930517435073853\n",
      "Epoch: 0, Step: 1308, Batch Loss: 0.09656774997711182\n",
      "Epoch: 0, Step: 1309, Batch Loss: 0.1325695514678955\n",
      "Epoch: 0, Step: 1310, Batch Loss: 0.12277641892433167\n",
      "Epoch: 0, Step: 1311, Batch Loss: 0.14586542546749115\n",
      "Epoch: 0, Step: 1312, Batch Loss: 0.12596552073955536\n",
      "Epoch: 0, Step: 1313, Batch Loss: 0.11894300580024719\n",
      "Epoch: 0, Step: 1314, Batch Loss: 0.11040878295898438\n",
      "Epoch: 0, Step: 1315, Batch Loss: 0.1342957317829132\n",
      "Epoch: 0, Step: 1316, Batch Loss: 0.11439616233110428\n",
      "Epoch: 0, Step: 1317, Batch Loss: 0.15116795897483826\n",
      "Epoch: 0, Step: 1318, Batch Loss: 0.13376638293266296\n",
      "Epoch: 0, Step: 1319, Batch Loss: 0.11172846704721451\n",
      "Epoch: 0, Step: 1320, Batch Loss: 0.09892252087593079\n",
      "Epoch: 0, Step: 1321, Batch Loss: 0.12043242156505585\n",
      "Epoch: 0, Step: 1322, Batch Loss: 0.12795008718967438\n",
      "Epoch: 0, Step: 1323, Batch Loss: 0.13001514971256256\n",
      "Epoch: 0, Step: 1324, Batch Loss: 0.131127268075943\n",
      "Epoch: 0, Step: 1325, Batch Loss: 0.09623236209154129\n",
      "Epoch: 0, Step: 1326, Batch Loss: 0.0994233712553978\n",
      "Epoch: 0, Step: 1327, Batch Loss: 0.12206295132637024\n",
      "Epoch: 0, Step: 1328, Batch Loss: 0.11524638533592224\n",
      "Epoch: 0, Step: 1329, Batch Loss: 0.11199910938739777\n",
      "Epoch: 0, Step: 1330, Batch Loss: 0.118771031498909\n",
      "Epoch: 0, Step: 1331, Batch Loss: 0.12244170159101486\n",
      "Epoch: 0, Step: 1332, Batch Loss: 0.1476154327392578\n",
      "Epoch: 0, Step: 1333, Batch Loss: 0.11744333058595657\n",
      "Epoch: 0, Step: 1334, Batch Loss: 0.17834718525409698\n",
      "Epoch: 0, Step: 1335, Batch Loss: 0.10718092322349548\n",
      "Epoch: 0, Step: 1336, Batch Loss: 0.12403140217065811\n",
      "Epoch: 0, Step: 1337, Batch Loss: 0.12220274657011032\n",
      "Epoch: 0, Step: 1338, Batch Loss: 0.09839347749948502\n",
      "Epoch: 0, Step: 1339, Batch Loss: 0.11915912479162216\n",
      "Epoch: 0, Step: 1340, Batch Loss: 0.10062877833843231\n",
      "Epoch: 0, Step: 1341, Batch Loss: 0.12599574029445648\n",
      "Epoch: 0, Step: 1342, Batch Loss: 0.12122465670108795\n",
      "Epoch: 0, Step: 1343, Batch Loss: 0.12523536384105682\n",
      "Epoch: 0, Step: 1344, Batch Loss: 0.10216422379016876\n",
      "Epoch: 0, Step: 1345, Batch Loss: 0.12791402637958527\n",
      "Epoch: 0, Step: 1346, Batch Loss: 0.12367679923772812\n",
      "Epoch: 0, Step: 1347, Batch Loss: 0.09424807876348495\n",
      "Epoch: 0, Step: 1348, Batch Loss: 0.10247555375099182\n",
      "Epoch: 0, Step: 1349, Batch Loss: 0.1232670322060585\n",
      "Epoch: 0, Step: 1350, Batch Loss: 0.1274733692407608\n",
      "Step: 1350, Validation Loss: 0.12363945903550637, Validation Price Error (Normalized): 576.0986842105264\n",
      "Epoch: 0, Step: 1351, Batch Loss: 0.09891951829195023\n",
      "Epoch: 0, Step: 1352, Batch Loss: 0.12197339534759521\n",
      "Epoch: 0, Step: 1353, Batch Loss: 0.13922978937625885\n",
      "Epoch: 0, Step: 1354, Batch Loss: 0.13927166163921356\n",
      "Epoch: 0, Step: 1355, Batch Loss: 0.1061704158782959\n",
      "Epoch: 0, Step: 1356, Batch Loss: 0.11071492731571198\n",
      "Epoch: 0, Step: 1357, Batch Loss: 0.1169682964682579\n",
      "Epoch: 0, Step: 1358, Batch Loss: 0.11114215105772018\n",
      "Epoch: 0, Step: 1359, Batch Loss: 0.1304941624403\n",
      "Epoch: 0, Step: 1360, Batch Loss: 0.11571043729782104\n",
      "Epoch: 0, Step: 1361, Batch Loss: 0.11140531301498413\n",
      "Epoch: 0, Step: 1362, Batch Loss: 0.11989431828260422\n",
      "Epoch: 0, Step: 1363, Batch Loss: 0.12855824828147888\n",
      "Epoch: 0, Step: 1364, Batch Loss: 0.12263579666614532\n",
      "Epoch: 0, Step: 1365, Batch Loss: 0.11179761588573456\n",
      "Epoch: 0, Step: 1366, Batch Loss: 0.14205685257911682\n",
      "Epoch: 0, Step: 1367, Batch Loss: 0.1279003620147705\n",
      "Epoch: 0, Step: 1368, Batch Loss: 0.12196557223796844\n",
      "Epoch: 0, Step: 1369, Batch Loss: 0.09897230565547943\n",
      "Epoch: 0, Step: 1370, Batch Loss: 0.1434379667043686\n",
      "Epoch: 0, Step: 1371, Batch Loss: 0.09961237758398056\n",
      "Epoch: 0, Step: 1372, Batch Loss: 0.11187221109867096\n",
      "Epoch: 0, Step: 1373, Batch Loss: 0.12130804359912872\n",
      "Epoch: 0, Step: 1374, Batch Loss: 0.10295335203409195\n",
      "Epoch: 0, Step: 1375, Batch Loss: 0.12060974538326263\n",
      "Epoch: 0, Step: 1376, Batch Loss: 0.11147090792655945\n",
      "Epoch: 0, Step: 1377, Batch Loss: 0.12733161449432373\n",
      "Epoch: 0, Step: 1378, Batch Loss: 0.11861670017242432\n",
      "Epoch: 0, Step: 1379, Batch Loss: 0.11137543618679047\n",
      "Epoch: 0, Step: 1380, Batch Loss: 0.10663579404354095\n",
      "Epoch: 0, Step: 1381, Batch Loss: 0.10846240073442459\n",
      "Epoch: 0, Step: 1382, Batch Loss: 0.10661560297012329\n",
      "Epoch: 0, Step: 1383, Batch Loss: 0.11643701791763306\n",
      "Epoch: 0, Step: 1384, Batch Loss: 0.1078614741563797\n",
      "Epoch: 0, Step: 1385, Batch Loss: 0.10791761428117752\n",
      "Epoch: 0, Step: 1386, Batch Loss: 0.10001120716333389\n",
      "Epoch: 0, Step: 1387, Batch Loss: 0.10937373340129852\n",
      "Epoch: 0, Step: 1388, Batch Loss: 0.1527385264635086\n",
      "Epoch: 0, Step: 1389, Batch Loss: 0.13971169292926788\n",
      "Epoch: 0, Step: 1390, Batch Loss: 0.10781293362379074\n",
      "Epoch: 0, Step: 1391, Batch Loss: 0.12612095475196838\n",
      "Epoch: 0, Step: 1392, Batch Loss: 0.11853563785552979\n",
      "Epoch: 0, Step: 1393, Batch Loss: 0.09828441590070724\n",
      "Epoch: 0, Step: 1394, Batch Loss: 0.12390494346618652\n",
      "Epoch: 0, Step: 1395, Batch Loss: 0.11697442084550858\n",
      "Epoch: 0, Step: 1396, Batch Loss: 0.12195940315723419\n",
      "Epoch: 0, Step: 1397, Batch Loss: 0.1328214406967163\n",
      "Epoch: 0, Step: 1398, Batch Loss: 0.11129812896251678\n",
      "Epoch: 0, Step: 1399, Batch Loss: 0.12433723360300064\n",
      "Epoch: 0, Step: 1400, Batch Loss: 0.1343216896057129\n",
      "Epoch: 0, Step: 1401, Batch Loss: 0.14041723310947418\n",
      "Epoch: 0, Step: 1402, Batch Loss: 0.10247555375099182\n",
      "Epoch: 0, Step: 1403, Batch Loss: 0.12952642142772675\n",
      "Epoch: 0, Step: 1404, Batch Loss: 0.12994882464408875\n",
      "Epoch: 0, Step: 1405, Batch Loss: 0.1331237554550171\n",
      "Epoch: 0, Step: 1406, Batch Loss: 0.11179761588573456\n",
      "Epoch: 0, Step: 1407, Batch Loss: 0.15045388042926788\n",
      "Epoch: 0, Step: 1408, Batch Loss: 0.11271576583385468\n",
      "Epoch: 0, Step: 1409, Batch Loss: 0.13255463540554047\n",
      "Epoch: 0, Step: 1410, Batch Loss: 0.1348007321357727\n",
      "Epoch: 0, Step: 1411, Batch Loss: 0.15772263705730438\n",
      "Epoch: 0, Step: 1412, Batch Loss: 0.10495607554912567\n",
      "Epoch: 0, Step: 1413, Batch Loss: 0.14476202428340912\n",
      "Epoch: 0, Step: 1414, Batch Loss: 0.09719535708427429\n",
      "Epoch: 0, Step: 1415, Batch Loss: 0.10684143751859665\n",
      "Epoch: 0, Step: 1416, Batch Loss: 0.13602522015571594\n",
      "Epoch: 0, Step: 1417, Batch Loss: 0.12157430499792099\n",
      "Epoch: 0, Step: 1418, Batch Loss: 0.11077957600355148\n",
      "Epoch: 0, Step: 1419, Batch Loss: 0.11369900405406952\n",
      "Epoch: 0, Step: 1420, Batch Loss: 0.1312640905380249\n",
      "Epoch: 0, Step: 1421, Batch Loss: 0.10687360912561417\n",
      "Epoch: 0, Step: 1422, Batch Loss: 0.10279213637113571\n",
      "Epoch: 0, Step: 1423, Batch Loss: 0.15977725386619568\n",
      "Epoch: 0, Step: 1424, Batch Loss: 0.09605700522661209\n",
      "Epoch: 0, Step: 1425, Batch Loss: 0.12031467258930206\n",
      "Epoch: 0, Step: 1426, Batch Loss: 0.12844900786876678\n",
      "Epoch: 0, Step: 1427, Batch Loss: 0.1360643059015274\n",
      "Epoch: 0, Step: 1428, Batch Loss: 0.14704762399196625\n",
      "Epoch: 0, Step: 1429, Batch Loss: 0.10338995605707169\n",
      "Epoch: 0, Step: 1430, Batch Loss: 0.11307354271411896\n",
      "Epoch: 0, Step: 1431, Batch Loss: 0.11037922650575638\n",
      "Epoch: 0, Step: 1432, Batch Loss: 0.12785902619361877\n",
      "Epoch: 0, Step: 1433, Batch Loss: 0.09878641366958618\n",
      "Epoch: 0, Step: 1434, Batch Loss: 0.1141173467040062\n",
      "Epoch: 0, Step: 1435, Batch Loss: 0.12615743279457092\n",
      "Epoch: 0, Step: 1436, Batch Loss: 0.1139858067035675\n",
      "Epoch: 0, Step: 1437, Batch Loss: 0.13042162358760834\n",
      "Epoch: 0, Step: 1438, Batch Loss: 0.11729756742715836\n",
      "Epoch: 0, Step: 1439, Batch Loss: 0.125062957406044\n",
      "Epoch: 0, Step: 1440, Batch Loss: 0.1270797848701477\n",
      "Epoch: 0, Step: 1441, Batch Loss: 0.09135491400957108\n",
      "Epoch: 0, Step: 1442, Batch Loss: 0.1184287741780281\n",
      "Epoch: 0, Step: 1443, Batch Loss: 0.1188960000872612\n",
      "Epoch: 0, Step: 1444, Batch Loss: 0.12166155129671097\n",
      "Epoch: 0, Step: 1445, Batch Loss: 0.12002897262573242\n",
      "Epoch: 0, Step: 1446, Batch Loss: 0.1239219382405281\n",
      "Epoch: 0, Step: 1447, Batch Loss: 0.12204444408416748\n",
      "Epoch: 0, Step: 1448, Batch Loss: 0.12533198297023773\n",
      "Epoch: 0, Step: 1449, Batch Loss: 0.10711703449487686\n",
      "Epoch: 0, Step: 1450, Batch Loss: 0.12694649398326874\n",
      "Epoch: 0, Step: 1451, Batch Loss: 0.11479512602090836\n",
      "Epoch: 0, Step: 1452, Batch Loss: 0.09889620542526245\n",
      "Epoch: 0, Step: 1453, Batch Loss: 0.15015409886837006\n",
      "Epoch: 0, Step: 1454, Batch Loss: 0.11417613178491592\n",
      "Epoch: 0, Step: 1455, Batch Loss: 0.10348179191350937\n",
      "Epoch: 0, Step: 1456, Batch Loss: 0.09558908641338348\n",
      "Epoch: 0, Step: 1457, Batch Loss: 0.14817661046981812\n",
      "Epoch: 0, Step: 1458, Batch Loss: 0.12247427552938461\n",
      "Epoch: 0, Step: 1459, Batch Loss: 0.09265775978565216\n",
      "Epoch: 0, Step: 1460, Batch Loss: 0.10620588064193726\n",
      "Epoch: 0, Step: 1461, Batch Loss: 0.10517403483390808\n",
      "Epoch: 0, Step: 1462, Batch Loss: 0.12833069264888763\n",
      "Epoch: 0, Step: 1463, Batch Loss: 0.11200641095638275\n",
      "Epoch: 0, Step: 1464, Batch Loss: 0.1212872788310051\n",
      "Epoch: 0, Step: 1465, Batch Loss: 0.12416499853134155\n",
      "Epoch: 0, Step: 1466, Batch Loss: 0.11246304959058762\n",
      "Epoch: 0, Step: 1467, Batch Loss: 0.12523509562015533\n",
      "Epoch: 0, Step: 1468, Batch Loss: 0.11467180401086807\n",
      "Epoch: 0, Step: 1469, Batch Loss: 0.1138453558087349\n",
      "Epoch: 0, Step: 1470, Batch Loss: 0.12331987917423248\n",
      "Epoch: 0, Step: 1471, Batch Loss: 0.1250503808259964\n",
      "Epoch: 0, Step: 1472, Batch Loss: 0.10850946605205536\n",
      "Epoch: 0, Step: 1473, Batch Loss: 0.10396838188171387\n",
      "Epoch: 0, Step: 1474, Batch Loss: 0.1161022037267685\n",
      "Epoch: 0, Step: 1475, Batch Loss: 0.1059006005525589\n",
      "Epoch: 0, Step: 1476, Batch Loss: 0.08793317526578903\n",
      "Epoch: 0, Step: 1477, Batch Loss: 0.09500113129615784\n",
      "Epoch: 0, Step: 1478, Batch Loss: 0.10889693349599838\n",
      "Epoch: 0, Step: 1479, Batch Loss: 0.09939799457788467\n",
      "Epoch: 0, Step: 1480, Batch Loss: 0.10653874278068542\n",
      "Epoch: 0, Step: 1481, Batch Loss: 0.1190067008137703\n",
      "Epoch: 0, Step: 1482, Batch Loss: 0.1210380345582962\n",
      "Epoch: 0, Step: 1483, Batch Loss: 0.11715971678495407\n",
      "Epoch: 0, Step: 1484, Batch Loss: 0.16166439652442932\n",
      "Epoch: 0, Step: 1485, Batch Loss: 0.13822557032108307\n",
      "Epoch: 0, Step: 1486, Batch Loss: 0.09506260603666306\n",
      "Epoch: 0, Step: 1487, Batch Loss: 0.12718211114406586\n",
      "Epoch: 0, Step: 1488, Batch Loss: 0.1174267828464508\n",
      "Epoch: 0, Step: 1489, Batch Loss: 0.10551667958498001\n",
      "Epoch: 0, Step: 1490, Batch Loss: 0.12418369948863983\n",
      "Epoch: 0, Step: 1491, Batch Loss: 0.14313705265522003\n",
      "Epoch: 0, Step: 1492, Batch Loss: 0.10573666542768478\n",
      "Epoch: 0, Step: 1493, Batch Loss: 0.09745004028081894\n",
      "Epoch: 0, Step: 1494, Batch Loss: 0.09700950235128403\n",
      "Epoch: 0, Step: 1495, Batch Loss: 0.11713879555463791\n",
      "Epoch: 0, Step: 1496, Batch Loss: 0.11196649819612503\n",
      "Epoch: 0, Step: 1497, Batch Loss: 0.10345429182052612\n",
      "Epoch: 0, Step: 1498, Batch Loss: 0.08842525631189346\n",
      "Epoch: 0, Step: 1499, Batch Loss: 0.0916610136628151\n",
      "Epoch: 0, Step: 1500, Batch Loss: 0.0960666760802269\n",
      "Step: 1500, Validation Loss: 0.1147386904217695, Validation Price Error (Normalized): 495.3519736842105\n",
      "Epoch: 0, Step: 1501, Batch Loss: 0.0914689376950264\n",
      "Epoch: 0, Step: 1502, Batch Loss: 0.1320456564426422\n",
      "Epoch: 0, Step: 1503, Batch Loss: 0.09938238561153412\n",
      "Epoch: 0, Step: 1504, Batch Loss: 0.10760542005300522\n",
      "Epoch: 0, Step: 1505, Batch Loss: 0.12430808693170547\n",
      "Epoch: 0, Step: 1506, Batch Loss: 0.12835100293159485\n",
      "Epoch: 0, Step: 1507, Batch Loss: 0.1061565950512886\n",
      "Epoch: 0, Step: 1508, Batch Loss: 0.11559775471687317\n",
      "Epoch: 0, Step: 1509, Batch Loss: 0.1172432228922844\n",
      "Epoch: 0, Step: 1510, Batch Loss: 0.12178017199039459\n",
      "Epoch: 0, Step: 1511, Batch Loss: 0.12372226268053055\n",
      "Epoch: 0, Step: 1512, Batch Loss: 0.09554467350244522\n",
      "Epoch: 0, Step: 1513, Batch Loss: 0.09672817587852478\n",
      "Epoch: 0, Step: 1514, Batch Loss: 0.11408241838216782\n",
      "Epoch: 0, Step: 1515, Batch Loss: 0.08577270805835724\n",
      "Epoch: 0, Step: 1516, Batch Loss: 0.1166686937212944\n",
      "Epoch: 0, Step: 1517, Batch Loss: 0.12692564725875854\n",
      "Epoch: 0, Step: 1518, Batch Loss: 0.12501296401023865\n",
      "Epoch: 0, Step: 1519, Batch Loss: 0.13303335011005402\n",
      "Epoch: 0, Step: 1520, Batch Loss: 0.10637879371643066\n",
      "Epoch: 0, Step: 1521, Batch Loss: 0.13154926896095276\n",
      "Epoch: 0, Step: 1522, Batch Loss: 0.09981489181518555\n",
      "Epoch: 0, Step: 1523, Batch Loss: 0.09751976281404495\n",
      "Epoch: 0, Step: 1524, Batch Loss: 0.09394323825836182\n",
      "Epoch: 0, Step: 1525, Batch Loss: 0.10932660102844238\n",
      "Epoch: 0, Step: 1526, Batch Loss: 0.11334193497896194\n",
      "Epoch: 0, Step: 1527, Batch Loss: 0.08871889859437943\n",
      "Epoch: 0, Step: 1528, Batch Loss: 0.1320817917585373\n",
      "Epoch: 0, Step: 1529, Batch Loss: 0.1226591095328331\n",
      "Epoch: 0, Step: 1530, Batch Loss: 0.2198261320590973\n",
      "Epoch: 0, Step: 1531, Batch Loss: 0.1228853240609169\n",
      "Epoch: 0, Step: 1532, Batch Loss: 0.10550165176391602\n",
      "Epoch: 0, Step: 1533, Batch Loss: 0.10653247684240341\n",
      "Epoch: 0, Step: 1534, Batch Loss: 0.11575574427843094\n",
      "Epoch: 0, Step: 1535, Batch Loss: 0.11088690906763077\n",
      "Epoch: 0, Step: 1536, Batch Loss: 0.1247856393456459\n",
      "Epoch: 0, Step: 1537, Batch Loss: 0.1030903086066246\n",
      "Epoch: 0, Step: 1538, Batch Loss: 0.10929973423480988\n",
      "Epoch: 0, Step: 1539, Batch Loss: 0.13940991461277008\n",
      "Epoch: 0, Step: 1540, Batch Loss: 0.10749158263206482\n",
      "Epoch: 0, Step: 1541, Batch Loss: 0.10614018142223358\n",
      "Epoch: 0, Step: 1542, Batch Loss: 0.1295863538980484\n",
      "Epoch: 0, Step: 1543, Batch Loss: 0.13022038340568542\n",
      "Epoch: 0, Step: 1544, Batch Loss: 0.12375779449939728\n",
      "Epoch: 0, Step: 1545, Batch Loss: 0.11538760364055634\n",
      "Epoch: 0, Step: 1546, Batch Loss: 0.10344002395868301\n",
      "Epoch: 0, Step: 1547, Batch Loss: 0.10753647983074188\n",
      "Epoch: 0, Step: 1548, Batch Loss: 0.12556315958499908\n",
      "Epoch: 0, Step: 1549, Batch Loss: 0.1065799742937088\n",
      "Epoch: 0, Step: 1550, Batch Loss: 0.10139681398868561\n",
      "Epoch: 0, Step: 1551, Batch Loss: 0.11760786920785904\n",
      "Epoch: 0, Step: 1552, Batch Loss: 0.1216261014342308\n",
      "Epoch: 0, Step: 1553, Batch Loss: 0.09645868092775345\n",
      "Epoch: 0, Step: 1554, Batch Loss: 0.11550083011388779\n",
      "Epoch: 0, Step: 1555, Batch Loss: 0.08986709266901016\n",
      "Epoch: 0, Step: 1556, Batch Loss: 0.11539094895124435\n",
      "Epoch: 0, Step: 1557, Batch Loss: 0.09143876284360886\n",
      "Epoch: 0, Step: 1558, Batch Loss: 0.11666698008775711\n",
      "Epoch: 0, Step: 1559, Batch Loss: 0.12264425307512283\n",
      "Epoch: 0, Step: 1560, Batch Loss: 0.08633621037006378\n",
      "Epoch: 0, Step: 1561, Batch Loss: 0.08200613409280777\n",
      "Epoch: 0, Step: 1562, Batch Loss: 0.11152645200490952\n",
      "Epoch: 0, Step: 1563, Batch Loss: 0.10750492662191391\n",
      "Epoch: 0, Step: 1564, Batch Loss: 0.1089797168970108\n",
      "Epoch: 0, Step: 1565, Batch Loss: 0.14366114139556885\n",
      "Epoch: 0, Step: 1566, Batch Loss: 0.11635174602270126\n",
      "Epoch: 0, Step: 1567, Batch Loss: 0.11077462881803513\n",
      "Epoch: 0, Step: 1568, Batch Loss: 0.10273411124944687\n",
      "Epoch: 0, Step: 1569, Batch Loss: 0.09367809444665909\n",
      "Epoch: 0, Step: 1570, Batch Loss: 0.10254780948162079\n",
      "Epoch: 0, Step: 1571, Batch Loss: 0.08799956738948822\n",
      "Epoch: 0, Step: 1572, Batch Loss: 0.11555703729391098\n",
      "Epoch: 0, Step: 1573, Batch Loss: 0.10078852623701096\n",
      "Epoch: 0, Step: 1574, Batch Loss: 0.10599148273468018\n",
      "Epoch: 0, Step: 1575, Batch Loss: 0.0974661186337471\n",
      "Epoch: 0, Step: 1576, Batch Loss: 0.11503764986991882\n",
      "Epoch: 0, Step: 1577, Batch Loss: 0.13316527009010315\n",
      "Epoch: 0, Step: 1578, Batch Loss: 0.09122500568628311\n",
      "Epoch: 0, Step: 1579, Batch Loss: 0.09044310450553894\n",
      "Epoch: 0, Step: 1580, Batch Loss: 0.10536826401948929\n",
      "Epoch: 0, Step: 1581, Batch Loss: 0.12011843174695969\n",
      "Epoch: 0, Step: 1582, Batch Loss: 0.09206388890743256\n",
      "Epoch: 0, Step: 1583, Batch Loss: 0.1379091739654541\n",
      "Epoch: 0, Step: 1584, Batch Loss: 0.09371355175971985\n",
      "Epoch: 0, Step: 1585, Batch Loss: 0.107499860227108\n",
      "Epoch: 0, Step: 1586, Batch Loss: 0.09686735272407532\n",
      "Epoch: 0, Step: 1587, Batch Loss: 0.10011814534664154\n",
      "Epoch: 0, Step: 1588, Batch Loss: 0.101840078830719\n",
      "Epoch: 0, Step: 1589, Batch Loss: 0.10090091824531555\n",
      "Epoch: 0, Step: 1590, Batch Loss: 0.12689314782619476\n",
      "Epoch: 0, Step: 1591, Batch Loss: 0.1133258119225502\n",
      "Epoch: 0, Step: 1592, Batch Loss: 0.11613598465919495\n",
      "Epoch: 0, Step: 1593, Batch Loss: 0.12280336767435074\n",
      "Epoch: 0, Step: 1594, Batch Loss: 0.11776037514209747\n",
      "Epoch: 0, Step: 1595, Batch Loss: 0.09757804125547409\n",
      "Epoch: 0, Step: 1596, Batch Loss: 0.09181652963161469\n",
      "Epoch: 0, Step: 1597, Batch Loss: 0.1152859479188919\n",
      "Epoch: 0, Step: 1598, Batch Loss: 0.11437859386205673\n",
      "Epoch: 0, Step: 1599, Batch Loss: 0.08606089651584625\n",
      "Epoch: 0, Step: 1600, Batch Loss: 0.10157719999551773\n",
      "Epoch: 0, Step: 1601, Batch Loss: 0.09582361578941345\n",
      "Epoch: 0, Step: 1602, Batch Loss: 0.10907434672117233\n",
      "Epoch: 0, Step: 1603, Batch Loss: 0.1119314655661583\n",
      "Epoch: 0, Step: 1604, Batch Loss: 0.1020878478884697\n",
      "Epoch: 0, Step: 1605, Batch Loss: 0.1256827712059021\n",
      "Epoch: 0, Step: 1606, Batch Loss: 0.08619026094675064\n",
      "Epoch: 0, Step: 1607, Batch Loss: 0.0834982767701149\n",
      "Epoch: 0, Step: 1608, Batch Loss: 0.08154793083667755\n",
      "Epoch: 0, Step: 1609, Batch Loss: 0.08642177283763885\n",
      "Epoch: 0, Step: 1610, Batch Loss: 0.10863052308559418\n",
      "Epoch: 0, Step: 1611, Batch Loss: 0.11148487776517868\n",
      "Epoch: 0, Step: 1612, Batch Loss: 0.10619857907295227\n",
      "Epoch: 0, Step: 1613, Batch Loss: 0.10905054956674576\n",
      "Epoch: 0, Step: 1614, Batch Loss: 0.12003348022699356\n",
      "Epoch: 0, Step: 1615, Batch Loss: 0.1090303510427475\n",
      "Epoch: 0, Step: 1616, Batch Loss: 0.09034986793994904\n",
      "Epoch: 0, Step: 1617, Batch Loss: 0.09669512510299683\n",
      "Epoch: 0, Step: 1618, Batch Loss: 0.10558801144361496\n",
      "Epoch: 0, Step: 1619, Batch Loss: 0.08936839550733566\n",
      "Epoch: 0, Step: 1620, Batch Loss: 0.09768214076757431\n",
      "Epoch: 0, Step: 1621, Batch Loss: 0.12981638312339783\n",
      "Epoch: 0, Step: 1622, Batch Loss: 0.13405194878578186\n",
      "Epoch: 0, Step: 1623, Batch Loss: 0.09011121839284897\n",
      "Epoch: 0, Step: 1624, Batch Loss: 0.09936483949422836\n",
      "Epoch: 0, Step: 1625, Batch Loss: 0.10823102295398712\n",
      "Epoch: 0, Step: 1626, Batch Loss: 0.14427876472473145\n",
      "Epoch: 0, Step: 1627, Batch Loss: 0.1245088055729866\n",
      "Epoch: 0, Step: 1628, Batch Loss: 0.08952096104621887\n",
      "Epoch: 0, Step: 1629, Batch Loss: 0.1017061397433281\n",
      "Epoch: 0, Step: 1630, Batch Loss: 0.09484408795833588\n",
      "Epoch: 0, Step: 1631, Batch Loss: 0.09762144833803177\n",
      "Epoch: 0, Step: 1632, Batch Loss: 0.13230304419994354\n",
      "Epoch: 0, Step: 1633, Batch Loss: 0.09734660387039185\n",
      "Epoch: 0, Step: 1634, Batch Loss: 0.09854073822498322\n",
      "Epoch: 0, Step: 1635, Batch Loss: 0.1138409823179245\n",
      "Epoch: 0, Step: 1636, Batch Loss: 0.09504489600658417\n",
      "Epoch: 0, Step: 1637, Batch Loss: 0.09859736263751984\n",
      "Epoch: 0, Step: 1638, Batch Loss: 0.09286453574895859\n",
      "Epoch: 0, Step: 1639, Batch Loss: 0.1077674850821495\n",
      "Epoch: 0, Step: 1640, Batch Loss: 0.12306217849254608\n",
      "Epoch: 0, Step: 1641, Batch Loss: 0.10393361002206802\n",
      "Epoch: 0, Step: 1642, Batch Loss: 0.10992822796106339\n",
      "Epoch: 0, Step: 1643, Batch Loss: 0.09393633157014847\n",
      "Epoch: 0, Step: 1644, Batch Loss: 0.11467903852462769\n",
      "Epoch: 0, Step: 1645, Batch Loss: 0.11654512584209442\n",
      "Epoch: 0, Step: 1646, Batch Loss: 0.08931299299001694\n",
      "Epoch: 0, Step: 1647, Batch Loss: 0.1213703453540802\n",
      "Epoch: 0, Step: 1648, Batch Loss: 0.12452862411737442\n",
      "Epoch: 0, Step: 1649, Batch Loss: 0.10162854939699173\n",
      "Epoch: 0, Step: 1650, Batch Loss: 0.14020183682441711\n",
      "Step: 1650, Validation Loss: 0.10680471943985474, Validation Price Error (Normalized): 426.7467105263158\n",
      "Epoch: 0, Step: 1651, Batch Loss: 0.08936839550733566\n",
      "Epoch: 0, Step: 1652, Batch Loss: 0.0885855183005333\n",
      "Epoch: 0, Step: 1653, Batch Loss: 0.12257728725671768\n",
      "Epoch: 0, Step: 1654, Batch Loss: 0.0969507172703743\n",
      "Epoch: 0, Step: 1655, Batch Loss: 0.09639224410057068\n",
      "Epoch: 0, Step: 1656, Batch Loss: 0.18360409140586853\n",
      "Epoch: 0, Step: 1657, Batch Loss: 0.09617356956005096\n",
      "Epoch: 0, Step: 1658, Batch Loss: 0.10552757978439331\n",
      "Epoch: 0, Step: 1659, Batch Loss: 0.0879187136888504\n",
      "Epoch: 0, Step: 1660, Batch Loss: 0.10449524223804474\n",
      "Epoch: 0, Step: 1661, Batch Loss: 0.10327445715665817\n",
      "Epoch: 0, Step: 1662, Batch Loss: 0.09279268234968185\n",
      "Epoch: 0, Step: 1663, Batch Loss: 0.11227720230817795\n",
      "Epoch: 0, Step: 1664, Batch Loss: 0.10330162197351456\n",
      "Epoch: 0, Step: 1665, Batch Loss: 0.10336244851350784\n",
      "Epoch: 0, Step: 1666, Batch Loss: 0.12997786700725555\n",
      "Epoch: 0, Step: 1667, Batch Loss: 0.08694025874137878\n",
      "Epoch: 0, Step: 1668, Batch Loss: 0.11853429675102234\n",
      "Epoch: 0, Step: 1669, Batch Loss: 0.0887228474020958\n",
      "Epoch: 0, Step: 1670, Batch Loss: 0.09181001782417297\n",
      "Epoch: 0, Step: 1671, Batch Loss: 0.13573847711086273\n",
      "Epoch: 0, Step: 1672, Batch Loss: 0.08395536243915558\n",
      "Epoch: 0, Step: 1673, Batch Loss: 0.07309480756521225\n",
      "Epoch: 0, Step: 1674, Batch Loss: 0.08845660090446472\n",
      "Epoch: 0, Step: 1675, Batch Loss: 0.09597442299127579\n",
      "Epoch: 0, Step: 1676, Batch Loss: 0.09018342941999435\n",
      "Epoch: 0, Step: 1677, Batch Loss: 0.10558708757162094\n",
      "Epoch: 0, Step: 1678, Batch Loss: 0.1134161427617073\n",
      "Epoch: 0, Step: 1679, Batch Loss: 0.10836505144834518\n",
      "Epoch: 0, Step: 1680, Batch Loss: 0.10856601595878601\n",
      "Epoch: 0, Step: 1681, Batch Loss: 0.10611426830291748\n",
      "Epoch: 0, Step: 1682, Batch Loss: 0.09248826652765274\n",
      "Epoch: 0, Step: 1683, Batch Loss: 0.13502231240272522\n",
      "Epoch: 0, Step: 1684, Batch Loss: 0.08949480950832367\n",
      "Epoch: 0, Step: 1685, Batch Loss: 0.08664275705814362\n",
      "Epoch: 0, Step: 1686, Batch Loss: 0.10486480593681335\n",
      "Epoch: 0, Step: 1687, Batch Loss: 0.08545568585395813\n",
      "Epoch: 0, Step: 1688, Batch Loss: 0.1067313551902771\n",
      "Epoch: 0, Step: 1689, Batch Loss: 0.08560188859701157\n",
      "Epoch: 0, Step: 1690, Batch Loss: 0.08399593085050583\n",
      "Epoch: 0, Step: 1691, Batch Loss: 0.09417492896318436\n",
      "Epoch: 0, Step: 1692, Batch Loss: 0.09881392866373062\n",
      "Epoch: 0, Step: 1693, Batch Loss: 0.08067069947719574\n",
      "Epoch: 0, Step: 1694, Batch Loss: 0.09398496896028519\n",
      "Epoch: 0, Step: 1695, Batch Loss: 0.11603311449289322\n",
      "Epoch: 0, Step: 1696, Batch Loss: 0.08732622116804123\n",
      "Epoch: 0, Step: 1697, Batch Loss: 0.08311773091554642\n",
      "Epoch: 0, Step: 1698, Batch Loss: 0.09561146050691605\n",
      "Epoch: 0, Step: 1699, Batch Loss: 0.08219397068023682\n",
      "Epoch: 0, Step: 1700, Batch Loss: 0.08731813728809357\n",
      "Epoch: 0, Step: 1701, Batch Loss: 0.11479466408491135\n",
      "Epoch: 0, Step: 1702, Batch Loss: 0.08935695141553879\n",
      "Epoch: 0, Step: 1703, Batch Loss: 0.07557039707899094\n",
      "Epoch: 0, Step: 1704, Batch Loss: 0.09827816486358643\n",
      "Epoch: 0, Step: 1705, Batch Loss: 0.09082368016242981\n",
      "Epoch: 0, Step: 1706, Batch Loss: 0.08837039768695831\n",
      "Epoch: 0, Step: 1707, Batch Loss: 0.09818504005670547\n",
      "Epoch: 0, Step: 1708, Batch Loss: 0.08291448652744293\n",
      "Epoch: 0, Step: 1709, Batch Loss: 0.08624780923128128\n",
      "Epoch: 0, Step: 1710, Batch Loss: 0.09222903102636337\n",
      "Epoch: 0, Step: 1711, Batch Loss: 0.0905417650938034\n",
      "Epoch: 0, Step: 1712, Batch Loss: 0.10110703855752945\n",
      "Epoch: 0, Step: 1713, Batch Loss: 0.10858980566263199\n",
      "Epoch: 0, Step: 1714, Batch Loss: 0.07850008457899094\n",
      "Epoch: 0, Step: 1715, Batch Loss: 0.1166018694639206\n",
      "Epoch: 0, Step: 1716, Batch Loss: 0.09182366728782654\n",
      "Epoch: 0, Step: 1717, Batch Loss: 0.10910632461309433\n",
      "Epoch: 0, Step: 1718, Batch Loss: 0.1132846400141716\n",
      "Epoch: 0, Step: 1719, Batch Loss: 0.10809213668107986\n",
      "Epoch: 0, Step: 1720, Batch Loss: 0.07984394580125809\n",
      "Epoch: 0, Step: 1721, Batch Loss: 0.093600332736969\n",
      "Epoch: 0, Step: 1722, Batch Loss: 0.09563597291707993\n",
      "Epoch: 0, Step: 1723, Batch Loss: 0.1279236376285553\n",
      "Epoch: 0, Step: 1724, Batch Loss: 0.09118035435676575\n",
      "Epoch: 0, Step: 1725, Batch Loss: 0.09264950454235077\n",
      "Epoch: 0, Step: 1726, Batch Loss: 0.0942152589559555\n",
      "Epoch: 0, Step: 1727, Batch Loss: 0.0964166522026062\n",
      "Epoch: 0, Step: 1728, Batch Loss: 0.09648269414901733\n",
      "Epoch: 0, Step: 1729, Batch Loss: 0.09835636615753174\n",
      "Epoch: 0, Step: 1730, Batch Loss: 0.1208392009139061\n",
      "Epoch: 0, Step: 1731, Batch Loss: 0.12161089479923248\n",
      "Epoch: 0, Step: 1732, Batch Loss: 0.07560412585735321\n",
      "Epoch: 0, Step: 1733, Batch Loss: 0.10535730421543121\n",
      "Epoch: 0, Step: 1734, Batch Loss: 0.10213467478752136\n",
      "Epoch: 0, Step: 1735, Batch Loss: 0.08618173748254776\n",
      "Epoch: 0, Step: 1736, Batch Loss: 0.11101371049880981\n",
      "Epoch: 0, Step: 1737, Batch Loss: 0.09063152968883514\n",
      "Epoch: 0, Step: 1738, Batch Loss: 0.1439605951309204\n",
      "Epoch: 0, Step: 1739, Batch Loss: 0.09807883203029633\n",
      "Epoch: 0, Step: 1740, Batch Loss: 0.08644356578588486\n",
      "Epoch: 0, Step: 1741, Batch Loss: 0.11169098317623138\n",
      "Epoch: 0, Step: 1742, Batch Loss: 0.08967375755310059\n",
      "Epoch: 0, Step: 1743, Batch Loss: 0.08056982606649399\n",
      "Epoch: 0, Step: 1744, Batch Loss: 0.0952763482928276\n",
      "Epoch: 0, Step: 1745, Batch Loss: 0.10002937912940979\n",
      "Epoch: 0, Step: 1746, Batch Loss: 0.11346423625946045\n",
      "Epoch: 0, Step: 1747, Batch Loss: 0.08228348940610886\n",
      "Epoch: 0, Step: 1748, Batch Loss: 0.1152166873216629\n",
      "Epoch: 0, Step: 1749, Batch Loss: 0.08872029185295105\n",
      "Epoch: 0, Step: 1750, Batch Loss: 0.09326980262994766\n",
      "Epoch: 0, Step: 1751, Batch Loss: 0.13070663809776306\n",
      "Epoch: 0, Step: 1752, Batch Loss: 0.10415414720773697\n",
      "Epoch: 0, Step: 1753, Batch Loss: 0.10236331820487976\n",
      "Epoch: 0, Step: 1754, Batch Loss: 0.10119183361530304\n",
      "Epoch: 0, Step: 1755, Batch Loss: 0.09365446865558624\n",
      "Epoch: 0, Step: 1756, Batch Loss: 0.09324191510677338\n",
      "Epoch: 0, Step: 1757, Batch Loss: 0.1079108864068985\n",
      "Epoch: 0, Step: 1758, Batch Loss: 0.1200982928276062\n",
      "Epoch: 0, Step: 1759, Batch Loss: 0.1149132251739502\n",
      "Epoch: 0, Step: 1760, Batch Loss: 0.09731588512659073\n",
      "Epoch: 0, Step: 1761, Batch Loss: 0.09497276693582535\n",
      "Epoch: 0, Step: 1762, Batch Loss: 0.0727611556649208\n",
      "Epoch: 0, Step: 1763, Batch Loss: 0.0694517120718956\n",
      "Epoch: 0, Step: 1764, Batch Loss: 0.15460553765296936\n",
      "Epoch: 0, Step: 1765, Batch Loss: 0.11598172038793564\n",
      "Epoch: 0, Step: 1766, Batch Loss: 0.13402627408504486\n",
      "Epoch: 0, Step: 1767, Batch Loss: 0.1188584417104721\n",
      "Epoch: 0, Step: 1768, Batch Loss: 0.08564483374357224\n",
      "Epoch: 0, Step: 1769, Batch Loss: 0.1037343218922615\n",
      "Epoch: 0, Step: 1770, Batch Loss: 0.08655383437871933\n",
      "Epoch: 0, Step: 1771, Batch Loss: 0.09645203500986099\n",
      "Epoch: 0, Step: 1772, Batch Loss: 0.10922610014677048\n",
      "Epoch: 0, Step: 1773, Batch Loss: 0.10850971937179565\n",
      "Epoch: 0, Step: 1774, Batch Loss: 0.08677517622709274\n",
      "Epoch: 0, Step: 1775, Batch Loss: 0.11356863379478455\n",
      "Epoch: 0, Step: 1776, Batch Loss: 0.11221083998680115\n",
      "Epoch: 0, Step: 1777, Batch Loss: 0.09298273921012878\n",
      "Epoch: 0, Step: 1778, Batch Loss: 0.08018730580806732\n",
      "Epoch: 0, Step: 1779, Batch Loss: 0.1269804984331131\n",
      "Epoch: 0, Step: 1780, Batch Loss: 0.08455957472324371\n",
      "Epoch: 0, Step: 1781, Batch Loss: 0.0924728587269783\n",
      "Epoch: 0, Step: 1782, Batch Loss: 0.10478898137807846\n",
      "Epoch: 0, Step: 1783, Batch Loss: 0.08152119070291519\n",
      "Epoch: 0, Step: 1784, Batch Loss: 0.09867292642593384\n",
      "Epoch: 0, Step: 1785, Batch Loss: 0.12482409179210663\n",
      "Epoch: 0, Step: 1786, Batch Loss: 0.11139601469039917\n",
      "Epoch: 0, Step: 1787, Batch Loss: 0.09784497320652008\n",
      "Epoch: 0, Step: 1788, Batch Loss: 0.1220519170165062\n",
      "Epoch: 0, Step: 1789, Batch Loss: 0.10525467246770859\n",
      "Epoch: 0, Step: 1790, Batch Loss: 0.10891475528478622\n",
      "Epoch: 0, Step: 1791, Batch Loss: 0.09012366831302643\n",
      "Epoch: 0, Step: 1792, Batch Loss: 0.10361616313457489\n",
      "Epoch: 0, Step: 1793, Batch Loss: 0.09381116181612015\n",
      "Epoch: 0, Step: 1794, Batch Loss: 0.092317134141922\n",
      "Epoch: 0, Step: 1795, Batch Loss: 0.10779063403606415\n",
      "Epoch: 0, Step: 1796, Batch Loss: 0.07993709295988083\n",
      "Epoch: 0, Step: 1797, Batch Loss: 0.07540993392467499\n",
      "Epoch: 0, Step: 1798, Batch Loss: 0.08859891444444656\n",
      "Epoch: 0, Step: 1799, Batch Loss: 0.08802365511655807\n",
      "Epoch: 0, Step: 1800, Batch Loss: 0.09896446019411087\n",
      "Step: 1800, Validation Loss: 0.09140932594278925, Validation Price Error (Normalized): 451.67763157894734\n",
      "Epoch: 0, Step: 1801, Batch Loss: 0.0992053672671318\n",
      "Epoch: 0, Step: 1802, Batch Loss: 0.07220351696014404\n",
      "Epoch: 0, Step: 1803, Batch Loss: 0.07857131212949753\n",
      "Epoch: 0, Step: 1804, Batch Loss: 0.11598017811775208\n",
      "Epoch: 0, Step: 1805, Batch Loss: 0.07497452944517136\n",
      "Epoch: 0, Step: 1806, Batch Loss: 0.10660141706466675\n",
      "Epoch: 0, Step: 1807, Batch Loss: 0.10196131467819214\n",
      "Epoch: 0, Step: 1808, Batch Loss: 0.10035748034715652\n",
      "Epoch: 0, Step: 1809, Batch Loss: 0.0894918441772461\n",
      "Epoch: 0, Step: 1810, Batch Loss: 0.10761997848749161\n",
      "Epoch: 0, Step: 1811, Batch Loss: 0.07804244011640549\n",
      "Epoch: 0, Step: 1812, Batch Loss: 0.10306321084499359\n",
      "Epoch: 0, Step: 1813, Batch Loss: 0.07433605194091797\n",
      "Epoch: 0, Step: 1814, Batch Loss: 0.07930318266153336\n",
      "Epoch: 0, Step: 1815, Batch Loss: 0.07675155997276306\n",
      "Epoch: 0, Step: 1816, Batch Loss: 0.11421705037355423\n",
      "Epoch: 0, Step: 1817, Batch Loss: 0.0690036416053772\n",
      "Epoch: 0, Step: 1818, Batch Loss: 0.09739302098751068\n",
      "Epoch: 0, Step: 1819, Batch Loss: 0.12069731205701828\n",
      "Epoch: 0, Step: 1820, Batch Loss: 0.07521896064281464\n",
      "Epoch: 0, Step: 1821, Batch Loss: 0.09794401377439499\n",
      "Epoch: 0, Step: 1822, Batch Loss: 0.07889868319034576\n",
      "Epoch: 0, Step: 1823, Batch Loss: 0.08112399280071259\n",
      "Epoch: 0, Step: 1824, Batch Loss: 0.08286795020103455\n",
      "Epoch: 0, Step: 1825, Batch Loss: 0.07701172679662704\n",
      "Epoch: 0, Step: 1826, Batch Loss: 0.0902862623333931\n",
      "Epoch: 0, Step: 1827, Batch Loss: 0.10170803219079971\n",
      "Epoch: 0, Step: 1828, Batch Loss: 0.07134636491537094\n",
      "Epoch: 0, Step: 1829, Batch Loss: 0.08724306523799896\n",
      "Epoch: 0, Step: 1830, Batch Loss: 0.0888119786977768\n",
      "Epoch: 0, Step: 1831, Batch Loss: 0.12336933612823486\n",
      "Epoch: 0, Step: 1832, Batch Loss: 0.08430637419223785\n",
      "Epoch: 0, Step: 1833, Batch Loss: 0.12654511630535126\n",
      "Epoch: 0, Step: 1834, Batch Loss: 0.07394997030496597\n",
      "Epoch: 0, Step: 1835, Batch Loss: 0.06594059616327286\n",
      "Epoch: 0, Step: 1836, Batch Loss: 0.09278497844934464\n",
      "Epoch: 0, Step: 1837, Batch Loss: 0.13647933304309845\n",
      "Epoch: 0, Step: 1838, Batch Loss: 0.08859891444444656\n",
      "Epoch: 0, Step: 1839, Batch Loss: 0.08094751089811325\n",
      "Epoch: 0, Step: 1840, Batch Loss: 0.10577460378408432\n",
      "Epoch: 0, Step: 1841, Batch Loss: 0.07438227534294128\n",
      "Epoch: 0, Step: 1842, Batch Loss: 0.07213521748781204\n",
      "Epoch: 0, Step: 1843, Batch Loss: 0.11540791392326355\n",
      "Epoch: 0, Step: 1844, Batch Loss: 0.09247907251119614\n",
      "Epoch: 0, Step: 1845, Batch Loss: 0.08213623613119125\n",
      "Epoch: 0, Step: 1846, Batch Loss: 0.07835941761732101\n",
      "Epoch: 0, Step: 1847, Batch Loss: 0.09235317260026932\n",
      "Epoch: 0, Step: 1848, Batch Loss: 0.08107394725084305\n",
      "Epoch: 0, Step: 1849, Batch Loss: 0.10360325127840042\n",
      "Epoch: 0, Step: 1850, Batch Loss: 0.07162933796644211\n",
      "Epoch: 0, Step: 1851, Batch Loss: 0.07037829607725143\n",
      "Epoch: 0, Step: 1852, Batch Loss: 0.0902242437005043\n",
      "Epoch: 0, Step: 1853, Batch Loss: 0.08085910975933075\n",
      "Epoch: 0, Step: 1854, Batch Loss: 0.0891910195350647\n",
      "Epoch: 0, Step: 1855, Batch Loss: 0.07337706536054611\n",
      "Epoch: 0, Step: 1856, Batch Loss: 0.09044753015041351\n",
      "Epoch: 0, Step: 1857, Batch Loss: 0.08454461395740509\n",
      "Epoch: 0, Step: 1858, Batch Loss: 0.0818915143609047\n",
      "Epoch: 0, Step: 1859, Batch Loss: 0.08644040673971176\n",
      "Epoch: 0, Step: 1860, Batch Loss: 0.08343199640512466\n",
      "Epoch: 0, Step: 1861, Batch Loss: 0.09204372018575668\n",
      "Epoch: 0, Step: 1862, Batch Loss: 0.10025914013385773\n",
      "Epoch: 0, Step: 1863, Batch Loss: 0.08437933772802353\n",
      "Epoch: 0, Step: 1864, Batch Loss: 0.09693452715873718\n",
      "Epoch: 0, Step: 1865, Batch Loss: 0.08257797360420227\n",
      "Epoch: 0, Step: 1866, Batch Loss: 0.07362666726112366\n",
      "Epoch: 0, Step: 1867, Batch Loss: 0.08614256232976913\n",
      "Epoch: 0, Step: 1868, Batch Loss: 0.11488592624664307\n",
      "Epoch: 0, Step: 1869, Batch Loss: 0.07375029474496841\n",
      "Epoch: 0, Step: 1870, Batch Loss: 0.062408771365880966\n",
      "Epoch: 0, Step: 1871, Batch Loss: 0.08685509860515594\n",
      "Epoch: 0, Step: 1872, Batch Loss: 0.08478330075740814\n",
      "Epoch: 0, Step: 1873, Batch Loss: 0.07064080983400345\n",
      "Epoch: 0, Step: 1874, Batch Loss: 0.08760252594947815\n",
      "Epoch: 0, Step: 1875, Batch Loss: 0.07635069638490677\n",
      "Epoch: 0, Step: 1876, Batch Loss: 0.15949206054210663\n",
      "Epoch: 0, Step: 1877, Batch Loss: 0.06876499950885773\n",
      "Epoch: 0, Step: 1878, Batch Loss: 0.07315793633460999\n",
      "Epoch: 0, Step: 1879, Batch Loss: 0.11320988833904266\n",
      "Epoch: 0, Step: 1880, Batch Loss: 0.12211974710226059\n",
      "Epoch: 0, Step: 1881, Batch Loss: 0.07154291123151779\n",
      "Epoch: 0, Step: 1882, Batch Loss: 0.07952431589365005\n",
      "Epoch: 0, Step: 1883, Batch Loss: 0.08032092452049255\n",
      "Epoch: 0, Step: 1884, Batch Loss: 0.09558931738138199\n",
      "Epoch: 0, Step: 1885, Batch Loss: 0.08022484928369522\n",
      "Epoch: 0, Step: 1886, Batch Loss: 0.06771569699048996\n",
      "Epoch: 0, Step: 1887, Batch Loss: 0.09660524129867554\n",
      "Epoch: 0, Step: 1888, Batch Loss: 0.08419497311115265\n",
      "Epoch: 0, Step: 1889, Batch Loss: 0.07820110768079758\n",
      "Epoch: 0, Step: 1890, Batch Loss: 0.1111539900302887\n",
      "Epoch: 0, Step: 1891, Batch Loss: 0.0831204205751419\n",
      "Epoch: 0, Step: 1892, Batch Loss: 0.0741545781493187\n",
      "Epoch: 0, Step: 1893, Batch Loss: 0.09150288254022598\n",
      "Epoch: 0, Step: 1894, Batch Loss: 0.09170828759670258\n",
      "Epoch: 0, Step: 1895, Batch Loss: 0.0704643577337265\n",
      "Epoch: 0, Step: 1896, Batch Loss: 0.1451212614774704\n",
      "Epoch: 0, Step: 1897, Batch Loss: 0.10028215497732162\n",
      "Epoch: 0, Step: 1898, Batch Loss: 0.09582576155662537\n",
      "Epoch: 0, Step: 1899, Batch Loss: 0.08499383181333542\n",
      "Epoch: 0, Step: 1900, Batch Loss: 0.06105970963835716\n",
      "Epoch: 0, Step: 1901, Batch Loss: 0.10921285301446915\n",
      "Epoch: 0, Step: 1902, Batch Loss: 0.08435578644275665\n",
      "Epoch: 0, Step: 1903, Batch Loss: 0.10438589006662369\n",
      "Epoch: 0, Step: 1904, Batch Loss: 0.12658844888210297\n",
      "Epoch: 0, Step: 1905, Batch Loss: 0.07241041213274002\n",
      "Epoch: 0, Step: 1906, Batch Loss: 0.06749086081981659\n",
      "Epoch: 0, Step: 1907, Batch Loss: 0.08939245343208313\n",
      "Epoch: 0, Step: 1908, Batch Loss: 0.07568854093551636\n",
      "Epoch: 0, Step: 1909, Batch Loss: 0.0923750102519989\n",
      "Epoch: 0, Step: 1910, Batch Loss: 0.062408771365880966\n",
      "Epoch: 0, Step: 1911, Batch Loss: 0.07665686309337616\n",
      "Epoch: 0, Step: 1912, Batch Loss: 0.07960490137338638\n",
      "Epoch: 0, Step: 1913, Batch Loss: 0.07846256345510483\n",
      "Epoch: 0, Step: 1914, Batch Loss: 0.06756657361984253\n",
      "Epoch: 0, Step: 1915, Batch Loss: 0.0647418424487114\n",
      "Epoch: 0, Step: 1916, Batch Loss: 0.08200284093618393\n",
      "Epoch: 0, Step: 1917, Batch Loss: 0.07947590202093124\n",
      "Epoch: 0, Step: 1918, Batch Loss: 0.08072128146886826\n",
      "Epoch: 0, Step: 1919, Batch Loss: 0.0800284892320633\n",
      "Epoch: 0, Step: 1920, Batch Loss: 0.07884761691093445\n",
      "Epoch: 0, Step: 1921, Batch Loss: 0.08080843836069107\n",
      "Epoch: 0, Step: 1922, Batch Loss: 0.07638861238956451\n",
      "Epoch: 0, Step: 1923, Batch Loss: 0.06187012046575546\n",
      "Epoch: 0, Step: 1924, Batch Loss: 0.09863649308681488\n",
      "Epoch: 0, Step: 1925, Batch Loss: 0.06481950730085373\n",
      "Epoch: 0, Step: 1926, Batch Loss: 0.10423196852207184\n",
      "Epoch: 0, Step: 1927, Batch Loss: 0.08931118994951248\n",
      "Epoch: 0, Step: 1928, Batch Loss: 0.08334710448980331\n",
      "Epoch: 0, Step: 1929, Batch Loss: 0.04783852398395538\n",
      "Epoch: 0, Step: 1930, Batch Loss: 0.07455061376094818\n",
      "Epoch: 0, Step: 1931, Batch Loss: 0.06409166008234024\n",
      "Epoch: 0, Step: 1932, Batch Loss: 0.09475726634263992\n",
      "Epoch: 0, Step: 1933, Batch Loss: 0.06754359602928162\n",
      "Epoch: 0, Step: 1934, Batch Loss: 0.06846129894256592\n",
      "Epoch: 0, Step: 1935, Batch Loss: 0.0849047526717186\n",
      "Epoch: 0, Step: 1936, Batch Loss: 0.06646181643009186\n",
      "Epoch: 0, Step: 1937, Batch Loss: 0.07864055037498474\n",
      "Epoch: 0, Step: 1938, Batch Loss: 0.07408717274665833\n",
      "Epoch: 0, Step: 1939, Batch Loss: 0.07145817577838898\n",
      "Epoch: 0, Step: 1940, Batch Loss: 0.06683514267206192\n",
      "Epoch: 0, Step: 1941, Batch Loss: 0.058280784636735916\n",
      "Epoch: 0, Step: 1942, Batch Loss: 0.07932344824075699\n",
      "Epoch: 0, Step: 1943, Batch Loss: 0.0665706917643547\n",
      "Epoch: 0, Step: 1944, Batch Loss: 0.08602125197649002\n",
      "Epoch: 0, Step: 1945, Batch Loss: 0.07770726084709167\n",
      "Epoch: 0, Step: 1946, Batch Loss: 0.07538904249668121\n",
      "Epoch: 0, Step: 1947, Batch Loss: 0.08067221939563751\n",
      "Epoch: 0, Step: 1948, Batch Loss: 0.0785364955663681\n",
      "Epoch: 0, Step: 1949, Batch Loss: 0.0768369808793068\n",
      "Epoch: 0, Step: 1950, Batch Loss: 0.09449341893196106\n",
      "Step: 1950, Validation Loss: 0.07927438305494816, Validation Price Error (Normalized): 395.125\n",
      "Epoch: 0, Step: 1951, Batch Loss: 0.08859039098024368\n",
      "Epoch: 0, Step: 1952, Batch Loss: 0.071302130818367\n",
      "Epoch: 0, Step: 1953, Batch Loss: 0.06216241419315338\n",
      "Epoch: 0, Step: 1954, Batch Loss: 0.06051056459546089\n",
      "Epoch: 0, Step: 1955, Batch Loss: 0.06523843854665756\n",
      "Epoch: 0, Step: 1956, Batch Loss: 0.09017680585384369\n",
      "Epoch: 0, Step: 1957, Batch Loss: 0.07555006444454193\n",
      "Epoch: 0, Step: 1958, Batch Loss: 0.0684530958533287\n",
      "Epoch: 0, Step: 1959, Batch Loss: 0.05842732638120651\n",
      "Epoch: 0, Step: 1960, Batch Loss: 0.0899800956249237\n",
      "Epoch: 0, Step: 1961, Batch Loss: 0.09092175215482712\n",
      "Epoch: 0, Step: 1962, Batch Loss: 0.0864444226026535\n",
      "Epoch: 0, Step: 1963, Batch Loss: 0.09704112261533737\n",
      "Epoch: 0, Step: 1964, Batch Loss: 0.11967556923627853\n",
      "Epoch: 0, Step: 1965, Batch Loss: 0.08131220936775208\n",
      "Epoch: 0, Step: 1966, Batch Loss: 0.06172307953238487\n",
      "Epoch: 0, Step: 1967, Batch Loss: 0.07154957950115204\n",
      "Epoch: 0, Step: 1968, Batch Loss: 0.08944984525442123\n",
      "Epoch: 0, Step: 1969, Batch Loss: 0.0955032929778099\n",
      "Epoch: 0, Step: 1970, Batch Loss: 0.07197333872318268\n",
      "Epoch: 0, Step: 1971, Batch Loss: 0.0818067267537117\n",
      "Epoch: 0, Step: 1972, Batch Loss: 0.06710732728242874\n",
      "Epoch: 0, Step: 1973, Batch Loss: 0.08037291467189789\n",
      "Epoch: 0, Step: 1974, Batch Loss: 0.11383822560310364\n",
      "Epoch: 0, Step: 1975, Batch Loss: 0.07694564014673233\n",
      "Epoch: 0, Step: 1976, Batch Loss: 0.08458254486322403\n",
      "Epoch: 0, Step: 1977, Batch Loss: 0.08537229150533676\n",
      "Epoch: 0, Step: 1978, Batch Loss: 0.07188507169485092\n",
      "Epoch: 0, Step: 1979, Batch Loss: 0.09245596081018448\n",
      "Epoch: 0, Step: 1980, Batch Loss: 0.06946803629398346\n",
      "Epoch: 0, Step: 1981, Batch Loss: 0.09667307883501053\n",
      "Epoch: 0, Step: 1982, Batch Loss: 0.07051589339971542\n",
      "Epoch: 0, Step: 1983, Batch Loss: 0.06565256416797638\n",
      "Epoch: 0, Step: 1984, Batch Loss: 0.07863209396600723\n",
      "Epoch: 0, Step: 1985, Batch Loss: 0.07914779335260391\n",
      "Epoch: 0, Step: 1986, Batch Loss: 0.06893302500247955\n",
      "Epoch: 0, Step: 1987, Batch Loss: 0.053289588540792465\n",
      "Epoch: 0, Step: 1988, Batch Loss: 0.06111882999539375\n",
      "Epoch: 0, Step: 1989, Batch Loss: 0.06401031464338303\n",
      "Epoch: 0, Step: 1990, Batch Loss: 0.05786451697349548\n",
      "Epoch: 0, Step: 1991, Batch Loss: 0.05669866502285004\n",
      "Epoch: 0, Step: 1992, Batch Loss: 0.05609039217233658\n",
      "Epoch: 0, Step: 1993, Batch Loss: 0.06971774250268936\n",
      "Epoch: 0, Step: 1994, Batch Loss: 0.07593200355768204\n",
      "Epoch: 0, Step: 1995, Batch Loss: 0.04500003904104233\n",
      "Epoch: 0, Step: 1996, Batch Loss: 0.07135118544101715\n",
      "Epoch: 0, Step: 1997, Batch Loss: 0.06500010937452316\n",
      "Epoch: 0, Step: 1998, Batch Loss: 0.06830459833145142\n",
      "Epoch: 0, Step: 1999, Batch Loss: 0.06873355060815811\n",
      "Epoch: 0, Step: 2000, Batch Loss: 0.11438601464033127\n",
      "Epoch: 0, Step: 2001, Batch Loss: 0.0913730189204216\n",
      "Epoch: 0, Step: 2002, Batch Loss: 0.09738551825284958\n",
      "Epoch: 0, Step: 2003, Batch Loss: 0.06843862682580948\n",
      "Epoch: 0, Step: 2004, Batch Loss: 0.049252934753894806\n",
      "Epoch: 0, Step: 2005, Batch Loss: 0.06922470033168793\n",
      "Epoch: 0, Step: 2006, Batch Loss: 0.09003254771232605\n",
      "Epoch: 0, Step: 2007, Batch Loss: 0.07132919132709503\n",
      "Epoch: 0, Step: 2008, Batch Loss: 0.073079414665699\n",
      "Epoch: 0, Step: 2009, Batch Loss: 0.09466102719306946\n",
      "Epoch: 0, Step: 2010, Batch Loss: 0.09486947953701019\n",
      "Epoch: 0, Step: 2011, Batch Loss: 0.10327403247356415\n",
      "Epoch: 0, Step: 2012, Batch Loss: 0.08438856154680252\n",
      "Epoch: 0, Step: 2013, Batch Loss: 0.06918808817863464\n",
      "Epoch: 0, Step: 2014, Batch Loss: 0.07315124571323395\n",
      "Epoch: 0, Step: 2015, Batch Loss: 0.06264796853065491\n",
      "Epoch: 0, Step: 2016, Batch Loss: 0.09260661154985428\n",
      "Epoch: 0, Step: 2017, Batch Loss: 0.08342336863279343\n",
      "Epoch: 0, Step: 2018, Batch Loss: 0.06760209053754807\n",
      "Epoch: 0, Step: 2019, Batch Loss: 0.08177646994590759\n",
      "Epoch: 0, Step: 2020, Batch Loss: 0.07574470341205597\n",
      "Epoch: 0, Step: 2021, Batch Loss: 0.06614641845226288\n",
      "Epoch: 0, Step: 2022, Batch Loss: 0.07261241227388382\n",
      "Epoch: 0, Step: 2023, Batch Loss: 0.06620752811431885\n",
      "Epoch: 0, Step: 2024, Batch Loss: 0.0636124461889267\n",
      "Epoch: 0, Step: 2025, Batch Loss: 0.057721078395843506\n",
      "Epoch: 0, Step: 2026, Batch Loss: 0.08967964351177216\n",
      "Epoch: 0, Step: 2027, Batch Loss: 0.046743083745241165\n",
      "Epoch: 0, Step: 2028, Batch Loss: 0.10593178868293762\n",
      "Epoch: 0, Step: 2029, Batch Loss: 0.061606403440237045\n",
      "Epoch: 0, Step: 2030, Batch Loss: 0.07806991785764694\n",
      "Epoch: 0, Step: 2031, Batch Loss: 0.10664860159158707\n",
      "Epoch: 0, Step: 2032, Batch Loss: 0.06278540194034576\n",
      "Epoch: 0, Step: 2033, Batch Loss: 0.06601972132921219\n",
      "Epoch: 0, Step: 2034, Batch Loss: 0.061033621430397034\n",
      "Epoch: 0, Step: 2035, Batch Loss: 0.0804811343550682\n",
      "Epoch: 0, Step: 2036, Batch Loss: 0.05428818240761757\n",
      "Epoch: 0, Step: 2037, Batch Loss: 0.09222077578306198\n",
      "Epoch: 0, Step: 2038, Batch Loss: 0.04951443523168564\n",
      "Epoch: 0, Step: 2039, Batch Loss: 0.06414762884378433\n",
      "Epoch: 0, Step: 2040, Batch Loss: 0.05260157585144043\n",
      "Epoch: 0, Step: 2041, Batch Loss: 0.06551861017942429\n",
      "Epoch: 0, Step: 2042, Batch Loss: 0.07374289631843567\n",
      "Epoch: 0, Step: 2043, Batch Loss: 0.04899749532341957\n",
      "Epoch: 0, Step: 2044, Batch Loss: 0.08196914941072464\n",
      "Epoch: 0, Step: 2045, Batch Loss: 0.06991630792617798\n",
      "Epoch: 0, Step: 2046, Batch Loss: 0.04601669684052467\n",
      "Epoch: 0, Step: 2047, Batch Loss: 0.06748507916927338\n",
      "Epoch: 0, Step: 2048, Batch Loss: 0.05067111924290657\n",
      "Epoch: 0, Step: 2049, Batch Loss: 0.07320399582386017\n",
      "Epoch: 0, Step: 2050, Batch Loss: 0.0498623326420784\n",
      "Epoch: 0, Step: 2051, Batch Loss: 0.06676322966814041\n",
      "Epoch: 0, Step: 2052, Batch Loss: 0.04930512234568596\n",
      "Epoch: 0, Step: 2053, Batch Loss: 0.05126844346523285\n",
      "Epoch: 0, Step: 2054, Batch Loss: 0.05701597034931183\n",
      "Epoch: 0, Step: 2055, Batch Loss: 0.0586484856903553\n",
      "Epoch: 0, Step: 2056, Batch Loss: 0.09782185405492783\n",
      "Epoch: 0, Step: 2057, Batch Loss: 0.05706488713622093\n",
      "Epoch: 0, Step: 2058, Batch Loss: 0.07081806659698486\n",
      "Epoch: 0, Step: 2059, Batch Loss: 0.05903812497854233\n",
      "Epoch: 0, Step: 2060, Batch Loss: 0.06755183637142181\n",
      "Epoch: 0, Step: 2061, Batch Loss: 0.06272485107183456\n",
      "Epoch: 0, Step: 2062, Batch Loss: 0.05117913708090782\n",
      "Epoch: 0, Step: 2063, Batch Loss: 0.04385280981659889\n",
      "Epoch: 0, Step: 2064, Batch Loss: 0.046381596475839615\n",
      "Epoch: 0, Step: 2065, Batch Loss: 0.06989257037639618\n",
      "Epoch: 0, Step: 2066, Batch Loss: 0.03740513324737549\n",
      "Epoch: 0, Step: 2067, Batch Loss: 0.07187218219041824\n",
      "Epoch: 0, Step: 2068, Batch Loss: 0.07240030914545059\n",
      "Epoch: 0, Step: 2069, Batch Loss: 0.08370261639356613\n",
      "Epoch: 0, Step: 2070, Batch Loss: 0.04738952964544296\n",
      "Epoch: 0, Step: 2071, Batch Loss: 0.08740922063589096\n",
      "Epoch: 0, Step: 2072, Batch Loss: 0.04915444552898407\n",
      "Epoch: 0, Step: 2073, Batch Loss: 0.04474164545536041\n",
      "Epoch: 0, Step: 2074, Batch Loss: 0.046253252774477005\n",
      "Epoch: 0, Step: 2075, Batch Loss: 0.06012635678052902\n",
      "Epoch: 0, Step: 2076, Batch Loss: 0.060897454619407654\n",
      "Epoch: 0, Step: 2077, Batch Loss: 0.08499307930469513\n",
      "Epoch: 0, Step: 2078, Batch Loss: 0.038749005645513535\n",
      "Epoch: 0, Step: 2079, Batch Loss: 0.059007178992033005\n",
      "Epoch: 0, Step: 2080, Batch Loss: 0.04901155084371567\n",
      "Epoch: 0, Step: 2081, Batch Loss: 0.04367177560925484\n",
      "Epoch: 0, Step: 2082, Batch Loss: 0.06239212676882744\n",
      "Epoch: 0, Step: 2083, Batch Loss: 0.0764819085597992\n",
      "Epoch: 0, Step: 2084, Batch Loss: 0.07084129750728607\n",
      "Epoch: 0, Step: 2085, Batch Loss: 0.06583966314792633\n",
      "Epoch: 0, Step: 2086, Batch Loss: 0.05395946279168129\n",
      "Epoch: 0, Step: 2087, Batch Loss: 0.04760374501347542\n",
      "Epoch: 0, Step: 2088, Batch Loss: 0.08271346986293793\n",
      "Epoch: 0, Step: 2089, Batch Loss: 0.08814126253128052\n",
      "Epoch: 0, Step: 2090, Batch Loss: 0.07388939708471298\n",
      "Epoch: 0, Step: 2091, Batch Loss: 0.031767651438713074\n",
      "Epoch: 0, Step: 2092, Batch Loss: 0.07004159688949585\n",
      "Epoch: 0, Step: 2093, Batch Loss: 0.09405508637428284\n",
      "Epoch: 0, Step: 2094, Batch Loss: 0.05164189264178276\n",
      "Epoch: 0, Step: 2095, Batch Loss: 0.05934588983654976\n",
      "Epoch: 0, Step: 2096, Batch Loss: 0.04871189966797829\n",
      "Epoch: 0, Step: 2097, Batch Loss: 0.06145602837204933\n",
      "Epoch: 0, Step: 2098, Batch Loss: 0.06266855448484421\n",
      "Epoch: 0, Step: 2099, Batch Loss: 0.05101272091269493\n",
      "Epoch: 0, Step: 2100, Batch Loss: 0.05365266278386116\n",
      "Step: 2100, Validation Loss: 0.06152568154625202, Validation Price Error (Normalized): 360.92105263157896\n",
      "Epoch: 0, Step: 2101, Batch Loss: 0.031949955970048904\n",
      "Epoch: 0, Step: 2102, Batch Loss: 0.07305257022380829\n",
      "Epoch: 0, Step: 2103, Batch Loss: 0.07244279235601425\n",
      "Epoch: 0, Step: 2104, Batch Loss: 0.06214801222085953\n",
      "Epoch: 0, Step: 2105, Batch Loss: 0.053318627178668976\n",
      "Epoch: 0, Step: 2106, Batch Loss: 0.04771960526704788\n",
      "Epoch: 0, Step: 2107, Batch Loss: 0.05195559188723564\n",
      "Epoch: 0, Step: 2108, Batch Loss: 0.05331696942448616\n",
      "Epoch: 0, Step: 2109, Batch Loss: 0.05998660624027252\n",
      "Epoch: 0, Step: 2110, Batch Loss: 0.058665383607149124\n",
      "Epoch: 0, Step: 2111, Batch Loss: 0.05868574231863022\n",
      "Epoch: 0, Step: 2112, Batch Loss: 0.04244699329137802\n",
      "Epoch: 0, Step: 2113, Batch Loss: 0.46049073338508606\n",
      "Epoch: 0, Step: 2114, Batch Loss: 0.4873981773853302\n",
      "Epoch: 0, Step: 2115, Batch Loss: 0.48986905813217163\n",
      "Epoch: 0, Step: 2116, Batch Loss: 0.46075040102005005\n",
      "Epoch: 0, Step: 2117, Batch Loss: 0.47566360235214233\n",
      "Epoch: 0, Step: 2118, Batch Loss: 0.48757484555244446\n",
      "Epoch: 0, Step: 2119, Batch Loss: 0.48559442162513733\n",
      "Epoch: 0, Step: 2120, Batch Loss: 0.4629490077495575\n",
      "Epoch: 0, Step: 2121, Batch Loss: 0.48712146282196045\n",
      "Epoch: 0, Step: 2122, Batch Loss: 0.46248894929885864\n",
      "Epoch: 0, Step: 2123, Batch Loss: 0.46306881308555603\n",
      "Epoch: 0, Step: 2124, Batch Loss: 0.5040264129638672\n",
      "Epoch: 0, Step: 2125, Batch Loss: 0.47827351093292236\n",
      "Epoch: 0, Step: 2126, Batch Loss: 0.48628756403923035\n",
      "Epoch: 0, Step: 2127, Batch Loss: 0.4685228765010834\n",
      "Epoch: 0, Step: 2128, Batch Loss: 0.4667001962661743\n",
      "Epoch: 0, Step: 2129, Batch Loss: 0.5014867186546326\n",
      "Epoch: 0, Step: 2130, Batch Loss: 0.48296329379081726\n",
      "Epoch: 0, Step: 2131, Batch Loss: 0.46291568875312805\n",
      "Epoch: 0, Step: 2132, Batch Loss: 0.46233803033828735\n",
      "Epoch: 0, Step: 2133, Batch Loss: 0.5237842202186584\n",
      "Epoch: 0, Step: 2134, Batch Loss: 0.4695553183555603\n",
      "Epoch: 0, Step: 2135, Batch Loss: 0.47078779339790344\n",
      "Epoch: 0, Step: 2136, Batch Loss: 0.4765852391719818\n",
      "Epoch: 0, Step: 2137, Batch Loss: 0.48208510875701904\n",
      "Epoch: 0, Step: 2138, Batch Loss: 0.5338402390480042\n",
      "Epoch: 0, Step: 2139, Batch Loss: 0.4664042890071869\n",
      "Epoch: 0, Step: 2140, Batch Loss: 0.4656403660774231\n",
      "Epoch: 0, Step: 2141, Batch Loss: 0.4773572087287903\n",
      "Epoch: 0, Step: 2142, Batch Loss: 0.46478036046028137\n",
      "Epoch: 0, Step: 2143, Batch Loss: 0.4771753251552582\n",
      "Epoch: 0, Step: 2144, Batch Loss: 0.4785165786743164\n",
      "Epoch: 0, Step: 2145, Batch Loss: 0.47867992520332336\n",
      "Epoch: 0, Step: 2146, Batch Loss: 0.4806477427482605\n",
      "Epoch: 0, Step: 2147, Batch Loss: 0.4852871298789978\n",
      "Epoch: 0, Step: 2148, Batch Loss: 0.47352588176727295\n",
      "Epoch: 0, Step: 2149, Batch Loss: 0.4757523834705353\n",
      "Epoch: 0, Step: 2150, Batch Loss: 0.46825578808784485\n",
      "Epoch: 0, Step: 2151, Batch Loss: 0.46607136726379395\n",
      "Epoch: 0, Step: 2152, Batch Loss: 0.4723735451698303\n",
      "Epoch: 0, Step: 2153, Batch Loss: 0.46825578808784485\n",
      "Epoch: 0, Step: 2154, Batch Loss: 0.4877737760543823\n",
      "Epoch: 0, Step: 2155, Batch Loss: 0.500219464302063\n",
      "Epoch: 0, Step: 2156, Batch Loss: 0.4991789162158966\n",
      "Epoch: 0, Step: 2157, Batch Loss: 0.49876847863197327\n",
      "Epoch: 0, Step: 2158, Batch Loss: 0.4769698977470398\n",
      "Epoch: 0, Step: 2159, Batch Loss: 0.48988020420074463\n",
      "Epoch: 0, Step: 2160, Batch Loss: 0.4677535891532898\n",
      "Epoch: 0, Step: 2161, Batch Loss: 0.4827062785625458\n",
      "Epoch: 0, Step: 2162, Batch Loss: 0.47369787096977234\n",
      "Epoch: 0, Step: 2163, Batch Loss: 0.4662244915962219\n",
      "Epoch: 0, Step: 2164, Batch Loss: 0.505821704864502\n",
      "Epoch: 0, Step: 2165, Batch Loss: 0.4793300926685333\n",
      "Epoch: 0, Step: 2166, Batch Loss: 0.4824236333370209\n",
      "Epoch: 0, Step: 2167, Batch Loss: 0.5280992388725281\n",
      "Epoch: 0, Step: 2168, Batch Loss: 0.49301743507385254\n",
      "Epoch: 0, Step: 2169, Batch Loss: 0.483492910861969\n",
      "Epoch: 0, Step: 2170, Batch Loss: 0.45817869901657104\n",
      "Epoch: 0, Step: 2171, Batch Loss: 0.48063722252845764\n",
      "Epoch: 0, Step: 2172, Batch Loss: 0.46527817845344543\n",
      "Epoch: 0, Step: 2173, Batch Loss: 0.4700218737125397\n",
      "Epoch: 0, Step: 2174, Batch Loss: 0.4663856327533722\n",
      "Epoch: 0, Step: 2175, Batch Loss: 0.47940191626548767\n",
      "Epoch: 0, Step: 2176, Batch Loss: 0.4861988425254822\n",
      "Epoch: 0, Step: 2177, Batch Loss: 0.04776078835129738\n",
      "Epoch: 0, Step: 2178, Batch Loss: 0.058814432471990585\n",
      "Epoch: 0, Step: 2179, Batch Loss: 0.06086030229926109\n",
      "Epoch: 0, Step: 2180, Batch Loss: 0.07085803151130676\n",
      "Epoch: 0, Step: 2181, Batch Loss: 0.08006040751934052\n",
      "Epoch: 0, Step: 2182, Batch Loss: 0.06347145885229111\n",
      "Epoch: 0, Step: 2183, Batch Loss: 0.058623939752578735\n",
      "Epoch: 0, Step: 2184, Batch Loss: 0.055385541170835495\n",
      "Epoch: 0, Step: 2185, Batch Loss: 0.050980523228645325\n",
      "Epoch: 0, Step: 2186, Batch Loss: 0.04981501027941704\n",
      "Epoch: 0, Step: 2187, Batch Loss: 0.05512933060526848\n",
      "Epoch: 0, Step: 2188, Batch Loss: 0.0563570111989975\n",
      "Epoch: 0, Step: 2189, Batch Loss: 0.06761610507965088\n",
      "Epoch: 0, Step: 2190, Batch Loss: 0.07680737227201462\n",
      "Epoch: 0, Step: 2191, Batch Loss: 0.05152829736471176\n",
      "Epoch: 0, Step: 2192, Batch Loss: 0.05698951706290245\n",
      "Epoch: 0, Step: 2193, Batch Loss: 0.049723803997039795\n",
      "Epoch: 0, Step: 2194, Batch Loss: 0.07417025417089462\n",
      "Epoch: 0, Step: 2195, Batch Loss: 0.07251457124948502\n",
      "Epoch: 0, Step: 2196, Batch Loss: 0.057615913450717926\n",
      "Epoch: 0, Step: 2197, Batch Loss: 0.05547230690717697\n",
      "Epoch: 0, Step: 2198, Batch Loss: 0.07463432103395462\n",
      "Epoch: 0, Step: 2199, Batch Loss: 0.08773656189441681\n",
      "Epoch: 0, Step: 2200, Batch Loss: 0.06071784719824791\n",
      "Epoch: 0, Step: 2201, Batch Loss: 0.0569596141576767\n",
      "Epoch: 0, Step: 2202, Batch Loss: 0.051063816994428635\n",
      "Epoch: 0, Step: 2203, Batch Loss: 0.053663868457078934\n",
      "Epoch: 0, Step: 2204, Batch Loss: 0.06386593729257584\n",
      "Epoch: 0, Step: 2205, Batch Loss: 0.0501892976462841\n",
      "Epoch: 0, Step: 2206, Batch Loss: 0.04887885972857475\n",
      "Epoch: 0, Step: 2207, Batch Loss: 0.12417460232973099\n",
      "Epoch: 0, Step: 2208, Batch Loss: 0.07058989256620407\n",
      "Epoch: 0, Step: 2209, Batch Loss: 0.04998919740319252\n",
      "Epoch: 0, Step: 2210, Batch Loss: 0.05342913419008255\n",
      "Epoch: 0, Step: 2211, Batch Loss: 0.059725213795900345\n",
      "Epoch: 0, Step: 2212, Batch Loss: 0.05980649217963219\n",
      "Epoch: 0, Step: 2213, Batch Loss: 0.06602534651756287\n",
      "Epoch: 0, Step: 2214, Batch Loss: 0.0614086389541626\n",
      "Epoch: 0, Step: 2215, Batch Loss: 0.05180906131863594\n",
      "Epoch: 0, Step: 2216, Batch Loss: 0.04481010511517525\n",
      "Epoch: 0, Step: 2217, Batch Loss: 0.09720239043235779\n",
      "Epoch: 0, Step: 2218, Batch Loss: 0.10358114540576935\n",
      "Epoch: 0, Step: 2219, Batch Loss: 0.06523153185844421\n",
      "Epoch: 0, Step: 2220, Batch Loss: 0.052231039851903915\n",
      "Epoch: 0, Step: 2221, Batch Loss: 0.08978959918022156\n",
      "Epoch: 0, Step: 2222, Batch Loss: 0.08713330328464508\n",
      "Epoch: 0, Step: 2223, Batch Loss: 0.060198765248060226\n",
      "Epoch: 0, Step: 2224, Batch Loss: 0.057443466037511826\n",
      "Epoch: 0, Step: 2225, Batch Loss: 0.06408257782459259\n",
      "Epoch: 0, Step: 2226, Batch Loss: 0.051899027079343796\n",
      "Epoch: 0, Step: 2227, Batch Loss: 0.06142709031701088\n",
      "Epoch: 0, Step: 2228, Batch Loss: 0.09467219561338425\n",
      "Epoch: 0, Step: 2229, Batch Loss: 0.06162839010357857\n",
      "Epoch: 0, Step: 2230, Batch Loss: 0.09214192628860474\n",
      "Epoch: 0, Step: 2231, Batch Loss: 0.057867687195539474\n",
      "Epoch: 0, Step: 2232, Batch Loss: 0.05417964607477188\n",
      "Epoch: 0, Step: 2233, Batch Loss: 0.05419476702809334\n",
      "Epoch: 0, Step: 2234, Batch Loss: 0.0848550945520401\n",
      "Epoch: 0, Step: 2235, Batch Loss: 0.053506650030612946\n",
      "Epoch: 0, Step: 2236, Batch Loss: 0.06648876518011093\n",
      "Epoch: 0, Step: 2237, Batch Loss: 0.06408445537090302\n",
      "Epoch: 0, Step: 2238, Batch Loss: 0.06829684227705002\n",
      "Epoch: 0, Step: 2239, Batch Loss: 0.0595431923866272\n",
      "Epoch: 0, Step: 2240, Batch Loss: 0.05228111520409584\n",
      "Epoch: 0, Step: 2241, Batch Loss: 0.05836895480751991\n",
      "Epoch: 0, Step: 2242, Batch Loss: 0.1293555200099945\n",
      "Epoch: 0, Step: 2243, Batch Loss: 0.07281431555747986\n",
      "Epoch: 0, Step: 2244, Batch Loss: 0.0745123103260994\n",
      "Epoch: 0, Step: 2245, Batch Loss: 0.0658780038356781\n",
      "Epoch: 0, Step: 2246, Batch Loss: 0.11515557020902634\n",
      "Epoch: 0, Step: 2247, Batch Loss: 0.06843091547489166\n",
      "Epoch: 0, Step: 2248, Batch Loss: 0.08574929088354111\n",
      "Epoch: 0, Step: 2249, Batch Loss: 0.05883754789829254\n",
      "Epoch: 0, Step: 2250, Batch Loss: 0.0665358304977417\n",
      "Step: 2250, Validation Loss: 0.07753106755645651, Validation Price Error (Normalized): 410.50986842105266\n",
      "Epoch: 0, Step: 2251, Batch Loss: 0.05905575677752495\n",
      "Epoch: 0, Step: 2252, Batch Loss: 0.07837265729904175\n",
      "Epoch: 0, Step: 2253, Batch Loss: 0.06161435693502426\n",
      "Epoch: 0, Step: 2254, Batch Loss: 0.09169179946184158\n",
      "Epoch: 0, Step: 2255, Batch Loss: 0.0969746857881546\n",
      "Epoch: 0, Step: 2256, Batch Loss: 0.08365271240472794\n",
      "Epoch: 0, Step: 2257, Batch Loss: 0.06568565964698792\n",
      "Epoch: 0, Step: 2258, Batch Loss: 0.05310746282339096\n",
      "Epoch: 0, Step: 2259, Batch Loss: 0.0806381106376648\n",
      "Epoch: 0, Step: 2260, Batch Loss: 0.07879166305065155\n",
      "Epoch: 0, Step: 2261, Batch Loss: 0.08724106103181839\n",
      "Epoch: 0, Step: 2262, Batch Loss: 0.06162286549806595\n",
      "Epoch: 0, Step: 2263, Batch Loss: 0.06088438257575035\n",
      "Epoch: 0, Step: 2264, Batch Loss: 0.08230427652597427\n",
      "Epoch: 0, Step: 2265, Batch Loss: 0.06840632110834122\n",
      "Epoch: 0, Step: 2266, Batch Loss: 0.08003658056259155\n",
      "Epoch: 0, Step: 2267, Batch Loss: 0.060174133628606796\n",
      "Epoch: 0, Step: 2268, Batch Loss: 0.08314996212720871\n",
      "Epoch: 0, Step: 2269, Batch Loss: 0.0918380469083786\n",
      "Epoch: 0, Step: 2270, Batch Loss: 0.07122910022735596\n",
      "Epoch: 0, Step: 2271, Batch Loss: 0.07550685852766037\n",
      "Epoch: 0, Step: 2272, Batch Loss: 0.08383289724588394\n",
      "Epoch: 0, Step: 2273, Batch Loss: 0.0773998573422432\n",
      "Epoch: 0, Step: 2274, Batch Loss: 0.06837516278028488\n",
      "Epoch: 0, Step: 2275, Batch Loss: 0.06728643923997879\n",
      "Epoch: 0, Step: 2276, Batch Loss: 0.10003596544265747\n",
      "Epoch: 0, Step: 2277, Batch Loss: 0.0759536474943161\n",
      "Epoch: 0, Step: 2278, Batch Loss: 0.049338407814502716\n",
      "Epoch: 0, Step: 2279, Batch Loss: 0.07317165285348892\n",
      "Epoch: 0, Step: 2280, Batch Loss: 0.1261967122554779\n",
      "Epoch: 0, Step: 2281, Batch Loss: 0.087725430727005\n",
      "Epoch: 0, Step: 2282, Batch Loss: 0.06710976362228394\n",
      "Epoch: 0, Step: 2283, Batch Loss: 0.07565491646528244\n",
      "Epoch: 0, Step: 2284, Batch Loss: 0.0664018988609314\n",
      "Epoch: 0, Step: 2285, Batch Loss: 0.07105401903390884\n",
      "Epoch: 0, Step: 2286, Batch Loss: 0.07011992484331131\n",
      "Epoch: 0, Step: 2287, Batch Loss: 0.06996744126081467\n",
      "Epoch: 0, Step: 2288, Batch Loss: 0.08468811959028244\n",
      "Epoch: 0, Step: 2289, Batch Loss: 0.07115147262811661\n",
      "Epoch: 0, Step: 2290, Batch Loss: 0.06795485317707062\n",
      "Epoch: 0, Step: 2291, Batch Loss: 0.07220253348350525\n",
      "Epoch: 0, Step: 2292, Batch Loss: 0.07609918713569641\n",
      "Epoch: 0, Step: 2293, Batch Loss: 0.0621037557721138\n",
      "Epoch: 0, Step: 2294, Batch Loss: 0.08827098459005356\n",
      "Epoch: 0, Step: 2295, Batch Loss: 0.08746722340583801\n",
      "Epoch: 0, Step: 2296, Batch Loss: 0.0706634372472763\n",
      "Epoch: 0, Step: 2297, Batch Loss: 0.07494768500328064\n",
      "Epoch: 0, Step: 2298, Batch Loss: 0.0554516464471817\n",
      "Epoch: 0, Step: 2299, Batch Loss: 0.06758875399827957\n",
      "Epoch: 0, Step: 2300, Batch Loss: 0.06387539952993393\n",
      "Epoch: 0, Step: 2301, Batch Loss: 0.0948023647069931\n",
      "Epoch: 0, Step: 2302, Batch Loss: 0.08568152785301208\n",
      "Epoch: 0, Step: 2303, Batch Loss: 0.0674365684390068\n",
      "Epoch: 0, Step: 2304, Batch Loss: 0.08323297649621964\n",
      "Epoch: 0, Step: 2305, Batch Loss: 0.09445575624704361\n",
      "Epoch: 0, Step: 2306, Batch Loss: 0.08177643269300461\n",
      "Epoch: 0, Step: 2307, Batch Loss: 0.0783407986164093\n",
      "Epoch: 0, Step: 2308, Batch Loss: 0.08348307758569717\n",
      "Epoch: 0, Step: 2309, Batch Loss: 0.08270744234323502\n",
      "Epoch: 0, Step: 2310, Batch Loss: 0.0724932923913002\n",
      "Epoch: 0, Step: 2311, Batch Loss: 0.08529908210039139\n",
      "Epoch: 0, Step: 2312, Batch Loss: 0.08841980993747711\n",
      "Epoch: 0, Step: 2313, Batch Loss: 0.07926496118307114\n",
      "Epoch: 0, Step: 2314, Batch Loss: 0.07601801306009293\n",
      "Epoch: 0, Step: 2315, Batch Loss: 0.08541886508464813\n",
      "Epoch: 0, Step: 2316, Batch Loss: 0.08382706344127655\n",
      "Epoch: 0, Step: 2317, Batch Loss: 0.08555018156766891\n",
      "Epoch: 0, Step: 2318, Batch Loss: 0.12051514536142349\n",
      "Epoch: 0, Step: 2319, Batch Loss: 0.08521944284439087\n",
      "Epoch: 0, Step: 2320, Batch Loss: 0.09606063365936279\n",
      "Epoch: 0, Step: 2321, Batch Loss: 0.10807372629642487\n",
      "Epoch: 0, Step: 2322, Batch Loss: 0.0880805253982544\n",
      "Epoch: 0, Step: 2323, Batch Loss: 0.11136747896671295\n",
      "Epoch: 0, Step: 2324, Batch Loss: 0.08889742195606232\n",
      "Epoch: 0, Step: 2325, Batch Loss: 0.07924757152795792\n",
      "Epoch: 0, Step: 2326, Batch Loss: 0.08529338985681534\n",
      "Epoch: 0, Step: 2327, Batch Loss: 0.10625818371772766\n",
      "Epoch: 0, Step: 2328, Batch Loss: 0.07302172482013702\n",
      "Epoch: 0, Step: 2329, Batch Loss: 0.09249542653560638\n",
      "Epoch: 0, Step: 2330, Batch Loss: 0.11193493753671646\n",
      "Epoch: 0, Step: 2331, Batch Loss: 0.09796929359436035\n",
      "Epoch: 0, Step: 2332, Batch Loss: 0.08816951513290405\n",
      "Epoch: 0, Step: 2333, Batch Loss: 0.12799012660980225\n",
      "Epoch: 0, Step: 2334, Batch Loss: 0.08446698635816574\n",
      "Epoch: 0, Step: 2335, Batch Loss: 0.12044579535722733\n",
      "Epoch: 0, Step: 2336, Batch Loss: 0.0839793011546135\n",
      "Epoch: 0, Step: 2337, Batch Loss: 0.07311998307704926\n",
      "Epoch: 0, Step: 2338, Batch Loss: 0.07935035228729248\n",
      "Epoch: 0, Step: 2339, Batch Loss: 0.07963442802429199\n",
      "Epoch: 0, Step: 2340, Batch Loss: 0.11207371205091476\n",
      "Epoch: 0, Step: 2341, Batch Loss: 0.09106559306383133\n",
      "Epoch: 0, Step: 2342, Batch Loss: 0.08015565574169159\n",
      "Epoch: 0, Step: 2343, Batch Loss: 0.07877545803785324\n",
      "Epoch: 0, Step: 2344, Batch Loss: 0.09936738014221191\n",
      "Epoch: 0, Step: 2345, Batch Loss: 0.07837678492069244\n",
      "Epoch: 0, Step: 2346, Batch Loss: 0.08262470364570618\n",
      "Epoch: 0, Step: 2347, Batch Loss: 0.08595570921897888\n",
      "Epoch: 0, Step: 2348, Batch Loss: 0.07964181900024414\n",
      "Epoch: 0, Step: 2349, Batch Loss: 0.07686525583267212\n",
      "Epoch: 0, Step: 2350, Batch Loss: 0.0848715752363205\n",
      "Epoch: 0, Step: 2351, Batch Loss: 0.08005539327859879\n",
      "Epoch: 0, Step: 2352, Batch Loss: 0.0850856602191925\n",
      "Epoch: 0, Step: 2353, Batch Loss: 0.09378839284181595\n",
      "Epoch: 0, Step: 2354, Batch Loss: 0.10910964012145996\n",
      "Epoch: 0, Step: 2355, Batch Loss: 0.10846932977437973\n",
      "Epoch: 0, Step: 2356, Batch Loss: 0.07674693316221237\n",
      "Epoch: 0, Step: 2357, Batch Loss: 0.09800684452056885\n",
      "Epoch: 0, Step: 2358, Batch Loss: 0.10392861813306808\n",
      "Epoch: 0, Step: 2359, Batch Loss: 0.07323933392763138\n",
      "Epoch: 0, Step: 2360, Batch Loss: 0.08307068794965744\n",
      "Epoch: 0, Step: 2361, Batch Loss: 0.07379499822854996\n",
      "Epoch: 0, Step: 2362, Batch Loss: 0.07262881845235825\n",
      "Epoch: 0, Step: 2363, Batch Loss: 0.09877468645572662\n",
      "Epoch: 0, Step: 2364, Batch Loss: 0.07346432656049728\n",
      "Epoch: 0, Step: 2365, Batch Loss: 0.10155906528234482\n",
      "Epoch: 0, Step: 2366, Batch Loss: 0.07319005578756332\n",
      "Epoch: 0, Step: 2367, Batch Loss: 0.10034991800785065\n",
      "Epoch: 0, Step: 2368, Batch Loss: 0.09405387192964554\n",
      "Epoch: 0, Step: 2369, Batch Loss: 0.08497718721628189\n",
      "Epoch: 0, Step: 2370, Batch Loss: 0.08997348695993423\n",
      "Epoch: 0, Step: 2371, Batch Loss: 0.07472369074821472\n",
      "Epoch: 0, Step: 2372, Batch Loss: 0.09150277078151703\n",
      "Epoch: 0, Step: 2373, Batch Loss: 0.08336836099624634\n",
      "Epoch: 0, Step: 2374, Batch Loss: 0.09195786714553833\n",
      "Epoch: 0, Step: 2375, Batch Loss: 0.09180440753698349\n",
      "Epoch: 0, Step: 2376, Batch Loss: 0.09060124307870865\n",
      "Epoch: 0, Step: 2377, Batch Loss: 0.09134850651025772\n",
      "Epoch: 0, Step: 2378, Batch Loss: 0.1328229159116745\n",
      "Epoch: 0, Step: 2379, Batch Loss: 0.12483461946249008\n",
      "Epoch: 0, Step: 2380, Batch Loss: 0.10409700870513916\n",
      "Epoch: 0, Step: 2381, Batch Loss: 0.09158936142921448\n",
      "Epoch: 0, Step: 2382, Batch Loss: 0.07824176549911499\n",
      "Epoch: 0, Step: 2383, Batch Loss: 0.10558830946683884\n",
      "Epoch: 0, Step: 2384, Batch Loss: 0.0960301011800766\n",
      "Epoch: 0, Step: 2385, Batch Loss: 0.08556043356657028\n",
      "Epoch: 0, Step: 2386, Batch Loss: 0.10798171907663345\n",
      "Epoch: 0, Step: 2387, Batch Loss: 0.08291833102703094\n",
      "Epoch: 0, Step: 2388, Batch Loss: 0.10585222393274307\n",
      "Epoch: 0, Step: 2389, Batch Loss: 0.0835079774260521\n",
      "Epoch: 0, Step: 2390, Batch Loss: 0.10008054971694946\n",
      "Epoch: 0, Step: 2391, Batch Loss: 0.08842909336090088\n",
      "Epoch: 0, Step: 2392, Batch Loss: 0.09716878831386566\n",
      "Epoch: 0, Step: 2393, Batch Loss: 0.10471469163894653\n",
      "Epoch: 0, Step: 2394, Batch Loss: 0.12298942357301712\n",
      "Epoch: 0, Step: 2395, Batch Loss: 0.10882463306188583\n",
      "Epoch: 0, Step: 2396, Batch Loss: 0.0826224759221077\n",
      "Epoch: 0, Step: 2397, Batch Loss: 0.12717293202877045\n",
      "Epoch: 0, Step: 2398, Batch Loss: 0.09078733623027802\n",
      "Epoch: 0, Step: 2399, Batch Loss: 0.09060124307870865\n",
      "Epoch: 0, Step: 2400, Batch Loss: 0.08863241970539093\n",
      "Step: 2400, Validation Loss: 0.1026110882674785, Validation Price Error (Normalized): 366.7763157894737\n",
      "Epoch: 0, Step: 2401, Batch Loss: 0.12729188799858093\n",
      "Epoch: 0, Step: 2402, Batch Loss: 0.12357445061206818\n",
      "Epoch: 0, Step: 2403, Batch Loss: 0.09198400378227234\n",
      "Epoch: 0, Step: 2404, Batch Loss: 0.11745531111955643\n",
      "Epoch: 0, Step: 2405, Batch Loss: 0.09377636760473251\n",
      "Epoch: 0, Step: 2406, Batch Loss: 0.09217637032270432\n",
      "Epoch: 0, Step: 2407, Batch Loss: 0.08678887784481049\n",
      "Epoch: 0, Step: 2408, Batch Loss: 0.09035158157348633\n",
      "Epoch: 0, Step: 2409, Batch Loss: 0.15051135420799255\n",
      "Epoch: 0, Step: 2410, Batch Loss: 0.10219919681549072\n",
      "Epoch: 0, Step: 2411, Batch Loss: 0.11026141047477722\n",
      "Epoch: 0, Step: 2412, Batch Loss: 0.13593506813049316\n",
      "Epoch: 0, Step: 2413, Batch Loss: 0.093937948346138\n",
      "Epoch: 0, Step: 2414, Batch Loss: 0.10210330784320831\n",
      "Epoch: 0, Step: 2415, Batch Loss: 0.1036653071641922\n",
      "Epoch: 0, Step: 2416, Batch Loss: 0.08795899152755737\n",
      "Epoch: 0, Step: 2417, Batch Loss: 0.13200941681861877\n",
      "Epoch: 0, Step: 2418, Batch Loss: 0.10408846288919449\n",
      "Epoch: 0, Step: 2419, Batch Loss: 0.09316691756248474\n",
      "Epoch: 0, Step: 2420, Batch Loss: 0.09060124307870865\n",
      "Epoch: 0, Step: 2421, Batch Loss: 0.0991971418261528\n",
      "Epoch: 0, Step: 2422, Batch Loss: 0.14824508130550385\n",
      "Epoch: 0, Step: 2423, Batch Loss: 0.10139654576778412\n",
      "Epoch: 0, Step: 2424, Batch Loss: 0.11745531111955643\n",
      "Epoch: 0, Step: 2425, Batch Loss: 0.11692464351654053\n",
      "Epoch: 0, Step: 2426, Batch Loss: 0.10188093781471252\n",
      "Epoch: 0, Step: 2427, Batch Loss: 0.08350592106580734\n",
      "Epoch: 0, Step: 2428, Batch Loss: 0.084210105240345\n",
      "Epoch: 0, Step: 2429, Batch Loss: 0.10870356112718582\n",
      "Epoch: 0, Step: 2430, Batch Loss: 0.1077834963798523\n",
      "Epoch: 0, Step: 2431, Batch Loss: 0.08995147049427032\n",
      "Epoch: 0, Step: 2432, Batch Loss: 0.15041865408420563\n",
      "Epoch: 0, Step: 2433, Batch Loss: 0.1441839039325714\n",
      "Epoch: 0, Step: 2434, Batch Loss: 0.08809629082679749\n",
      "Epoch: 0, Step: 2435, Batch Loss: 0.09181302040815353\n",
      "Epoch: 0, Step: 2436, Batch Loss: 0.10074581950902939\n",
      "Epoch: 0, Step: 2437, Batch Loss: 0.11436071246862411\n",
      "Epoch: 0, Step: 2438, Batch Loss: 0.09077825397253036\n",
      "Epoch: 0, Step: 2439, Batch Loss: 0.11446765065193176\n",
      "Epoch: 0, Step: 2440, Batch Loss: 0.10840025544166565\n",
      "Epoch: 0, Step: 2441, Batch Loss: 0.11675409972667694\n",
      "Epoch: 0, Step: 2442, Batch Loss: 0.10676051676273346\n",
      "Epoch: 0, Step: 2443, Batch Loss: 0.12270484119653702\n",
      "Epoch: 0, Step: 2444, Batch Loss: 0.09758999198675156\n",
      "Epoch: 0, Step: 2445, Batch Loss: 0.09433425217866898\n",
      "Epoch: 0, Step: 2446, Batch Loss: 0.14441055059432983\n",
      "Epoch: 0, Step: 2447, Batch Loss: 0.0989978238940239\n",
      "Epoch: 0, Step: 2448, Batch Loss: 0.1352532058954239\n",
      "Epoch: 0, Step: 2449, Batch Loss: 0.0935518741607666\n",
      "Epoch: 0, Step: 2450, Batch Loss: 0.08324193954467773\n",
      "Epoch: 0, Step: 2451, Batch Loss: 0.0950746163725853\n",
      "Epoch: 0, Step: 2452, Batch Loss: 0.08684416115283966\n",
      "Epoch: 0, Step: 2453, Batch Loss: 0.08684416115283966\n",
      "Epoch: 0, Step: 2454, Batch Loss: 0.10877025127410889\n",
      "Epoch: 0, Step: 2455, Batch Loss: 0.10785909742116928\n",
      "Epoch: 0, Step: 2456, Batch Loss: 0.10070475190877914\n",
      "Epoch: 0, Step: 2457, Batch Loss: 0.08411047607660294\n",
      "Epoch: 0, Step: 2458, Batch Loss: 0.11158445477485657\n",
      "Epoch: 0, Step: 2459, Batch Loss: 0.10592792183160782\n",
      "Epoch: 0, Step: 2460, Batch Loss: 0.09453801065683365\n",
      "Epoch: 0, Step: 2461, Batch Loss: 0.11712171882390976\n",
      "Epoch: 0, Step: 2462, Batch Loss: 0.09248607605695724\n",
      "Epoch: 0, Step: 2463, Batch Loss: 0.11782827228307724\n",
      "Epoch: 0, Step: 2464, Batch Loss: 0.085060253739357\n",
      "Epoch: 0, Step: 2465, Batch Loss: 0.0994998961687088\n",
      "Epoch: 0, Step: 2466, Batch Loss: 0.10483381897211075\n",
      "Epoch: 0, Step: 2467, Batch Loss: 0.10676051676273346\n",
      "Epoch: 0, Step: 2468, Batch Loss: 0.09504833817481995\n",
      "Epoch: 0, Step: 2469, Batch Loss: 0.11656639724969864\n",
      "Epoch: 0, Step: 2470, Batch Loss: 0.11125693470239639\n",
      "Epoch: 0, Step: 2471, Batch Loss: 0.09453801065683365\n",
      "Epoch: 0, Step: 2472, Batch Loss: 0.09969623386859894\n",
      "Epoch: 0, Step: 2473, Batch Loss: 0.0964282900094986\n",
      "Epoch: 0, Step: 2474, Batch Loss: 0.131288081407547\n",
      "Epoch: 0, Step: 2475, Batch Loss: 0.10756411403417587\n",
      "Epoch: 0, Step: 2476, Batch Loss: 0.1065150648355484\n",
      "Epoch: 0, Step: 2477, Batch Loss: 0.11414488404989243\n",
      "Epoch: 0, Step: 2478, Batch Loss: 0.09795063734054565\n",
      "Epoch: 0, Step: 2479, Batch Loss: 0.11404325813055038\n",
      "Epoch: 0, Step: 2480, Batch Loss: 0.1090630367398262\n",
      "Epoch: 0, Step: 2481, Batch Loss: 0.1225360557436943\n",
      "Epoch: 0, Step: 2482, Batch Loss: 0.10749521851539612\n",
      "Epoch: 0, Step: 2483, Batch Loss: 0.0989978238940239\n",
      "Epoch: 0, Step: 2484, Batch Loss: 0.11669306457042694\n",
      "Epoch: 0, Step: 2485, Batch Loss: 0.10859038680791855\n",
      "Epoch: 0, Step: 2486, Batch Loss: 0.08841390907764435\n",
      "Epoch: 0, Step: 2487, Batch Loss: 0.0897245928645134\n",
      "Epoch: 0, Step: 2488, Batch Loss: 0.12390606850385666\n",
      "Epoch: 0, Step: 2489, Batch Loss: 0.11908372491598129\n",
      "Epoch: 0, Step: 2490, Batch Loss: 0.10649745911359787\n",
      "Epoch: 0, Step: 2491, Batch Loss: 0.10137027502059937\n",
      "Epoch: 0, Step: 2492, Batch Loss: 0.11118891835212708\n",
      "Epoch: 0, Step: 2493, Batch Loss: 0.10328209400177002\n",
      "Epoch: 0, Step: 2494, Batch Loss: 0.09962096065282822\n",
      "Epoch: 0, Step: 2495, Batch Loss: 0.0926349014043808\n",
      "Epoch: 0, Step: 2496, Batch Loss: 0.11883138120174408\n",
      "Epoch: 0, Step: 2497, Batch Loss: 0.1001933366060257\n",
      "Epoch: 0, Step: 2498, Batch Loss: 0.10485595464706421\n",
      "Epoch: 0, Step: 2499, Batch Loss: 0.10666903853416443\n",
      "Epoch: 0, Step: 2500, Batch Loss: 0.10258758813142776\n",
      "Epoch: 0, Step: 2501, Batch Loss: 0.09813271462917328\n",
      "Epoch: 0, Step: 2502, Batch Loss: 0.09779029339551926\n",
      "Epoch: 0, Step: 2503, Batch Loss: 0.09676613658666611\n",
      "Epoch: 0, Step: 2504, Batch Loss: 0.10548096895217896\n",
      "Epoch: 0, Step: 2505, Batch Loss: 0.1164931058883667\n",
      "Epoch: 0, Step: 2506, Batch Loss: 0.09546124190092087\n",
      "Epoch: 0, Step: 2507, Batch Loss: 0.14619511365890503\n",
      "Epoch: 0, Step: 2508, Batch Loss: 0.12173236906528473\n",
      "Epoch: 0, Step: 2509, Batch Loss: 0.11057624220848083\n",
      "Epoch: 0, Step: 2510, Batch Loss: 0.10824709385633469\n",
      "Epoch: 0, Step: 2511, Batch Loss: 0.11251326650381088\n",
      "Epoch: 0, Step: 2512, Batch Loss: 0.12921670079231262\n",
      "Epoch: 0, Step: 2513, Batch Loss: 0.10435641556978226\n",
      "Epoch: 0, Step: 2514, Batch Loss: 0.0937415212392807\n",
      "Epoch: 0, Step: 2515, Batch Loss: 0.11688144505023956\n",
      "Epoch: 0, Step: 2516, Batch Loss: 0.096839040517807\n",
      "Epoch: 0, Step: 2517, Batch Loss: 0.14282025396823883\n",
      "Epoch: 0, Step: 2518, Batch Loss: 0.09736939519643784\n",
      "Epoch: 0, Step: 2519, Batch Loss: 0.10383249819278717\n",
      "Epoch: 0, Step: 2520, Batch Loss: 0.11073330044746399\n",
      "Epoch: 0, Step: 2521, Batch Loss: 0.11877620965242386\n",
      "Epoch: 0, Step: 2522, Batch Loss: 0.09855996817350388\n",
      "Epoch: 0, Step: 2523, Batch Loss: 0.1386876404285431\n",
      "Epoch: 0, Step: 2524, Batch Loss: 0.09796056896448135\n",
      "Epoch: 0, Step: 2525, Batch Loss: 0.10571660846471786\n",
      "Epoch: 0, Step: 2526, Batch Loss: 0.11025471985340118\n",
      "Epoch: 0, Step: 2527, Batch Loss: 0.11427567154169083\n",
      "Epoch: 0, Step: 2528, Batch Loss: 0.11091583222150803\n",
      "Epoch: 0, Step: 2529, Batch Loss: 0.1060725599527359\n",
      "Epoch: 0, Step: 2530, Batch Loss: 0.09300291538238525\n",
      "Epoch: 0, Step: 2531, Batch Loss: 0.09962811321020126\n",
      "Epoch: 0, Step: 2532, Batch Loss: 0.09991558641195297\n",
      "Epoch: 0, Step: 2533, Batch Loss: 0.10937417298555374\n",
      "Epoch: 0, Step: 2534, Batch Loss: 0.11563694477081299\n",
      "Epoch: 0, Step: 2535, Batch Loss: 0.12878327071666718\n",
      "Epoch: 0, Step: 2536, Batch Loss: 0.1198352798819542\n",
      "Epoch: 0, Step: 2537, Batch Loss: 0.09736939519643784\n",
      "Epoch: 0, Step: 2538, Batch Loss: 0.14172334969043732\n",
      "Epoch: 0, Step: 2539, Batch Loss: 0.10696553438901901\n",
      "Epoch: 0, Step: 2540, Batch Loss: 0.13440807163715363\n",
      "Epoch: 0, Step: 2541, Batch Loss: 0.11384555697441101\n",
      "Epoch: 0, Step: 2542, Batch Loss: 0.10713490098714828\n",
      "Epoch: 0, Step: 2543, Batch Loss: 0.1290181577205658\n",
      "Epoch: 0, Step: 2544, Batch Loss: 0.1213320791721344\n",
      "Epoch: 0, Step: 2545, Batch Loss: 0.10131360590457916\n",
      "Epoch: 0, Step: 2546, Batch Loss: 0.15748856961727142\n",
      "Epoch: 0, Step: 2547, Batch Loss: 0.09784862399101257\n",
      "Epoch: 0, Step: 2548, Batch Loss: 0.10737046599388123\n",
      "Epoch: 0, Step: 2549, Batch Loss: 0.11213278770446777\n",
      "Epoch: 0, Step: 2550, Batch Loss: 0.09812260419130325\n",
      "Step: 2550, Validation Loss: 0.11235162056982517, Validation Price Error (Normalized): 417.4703947368421\n",
      "Epoch: 0, Step: 2551, Batch Loss: 0.09779009222984314\n",
      "Epoch: 0, Step: 2552, Batch Loss: 0.15320070087909698\n",
      "Epoch: 0, Step: 2553, Batch Loss: 0.08603927493095398\n",
      "Epoch: 0, Step: 2554, Batch Loss: 0.09677113592624664\n",
      "Epoch: 0, Step: 2555, Batch Loss: 0.11507511883974075\n",
      "Epoch: 0, Step: 2556, Batch Loss: 0.1299397200345993\n",
      "Epoch: 0, Step: 2557, Batch Loss: 0.11067597568035126\n",
      "Epoch: 0, Step: 2558, Batch Loss: 0.10177600383758545\n",
      "Epoch: 0, Step: 2559, Batch Loss: 0.13181830942630768\n",
      "Epoch: 0, Step: 2560, Batch Loss: 0.10376283526420593\n",
      "Epoch: 0, Step: 2561, Batch Loss: 0.11111695319414139\n",
      "Epoch: 0, Step: 2562, Batch Loss: 0.11398017406463623\n",
      "Epoch: 0, Step: 2563, Batch Loss: 0.12955276668071747\n",
      "Epoch: 0, Step: 2564, Batch Loss: 0.10392729192972183\n",
      "Epoch: 0, Step: 2565, Batch Loss: 0.1248876303434372\n",
      "Epoch: 0, Step: 2566, Batch Loss: 0.10526304692029953\n",
      "Epoch: 0, Step: 2567, Batch Loss: 0.11947355419397354\n",
      "Epoch: 0, Step: 2568, Batch Loss: 0.10507073253393173\n",
      "Epoch: 0, Step: 2569, Batch Loss: 0.11028128862380981\n",
      "Epoch: 0, Step: 2570, Batch Loss: 0.1049683466553688\n",
      "Epoch: 0, Step: 2571, Batch Loss: 0.13854031264781952\n",
      "Epoch: 0, Step: 2572, Batch Loss: 0.10103173553943634\n",
      "Epoch: 0, Step: 2573, Batch Loss: 0.11278732120990753\n",
      "Epoch: 0, Step: 2574, Batch Loss: 0.14361393451690674\n",
      "Epoch: 0, Step: 2575, Batch Loss: 0.1337776780128479\n",
      "Epoch: 0, Step: 2576, Batch Loss: 0.10939032584428787\n",
      "Epoch: 0, Step: 2577, Batch Loss: 0.14366552233695984\n",
      "Epoch: 0, Step: 2578, Batch Loss: 0.1210111454129219\n",
      "Epoch: 0, Step: 2579, Batch Loss: 0.1290407031774521\n",
      "Epoch: 0, Step: 2580, Batch Loss: 0.09758269041776657\n",
      "Epoch: 0, Step: 2581, Batch Loss: 0.11370369046926498\n",
      "Epoch: 0, Step: 2582, Batch Loss: 0.11289256811141968\n",
      "Epoch: 0, Step: 2583, Batch Loss: 0.10718509554862976\n",
      "Epoch: 0, Step: 2584, Batch Loss: 0.11643405258655548\n",
      "Epoch: 0, Step: 2585, Batch Loss: 0.09618723392486572\n",
      "Epoch: 0, Step: 2586, Batch Loss: 0.10457775741815567\n",
      "Epoch: 0, Step: 2587, Batch Loss: 0.11108766496181488\n",
      "Epoch: 0, Step: 2588, Batch Loss: 0.10890823602676392\n",
      "Epoch: 0, Step: 2589, Batch Loss: 0.12091869115829468\n",
      "Epoch: 0, Step: 2590, Batch Loss: 0.10297609120607376\n",
      "Epoch: 0, Step: 2591, Batch Loss: 0.11984521150588989\n",
      "Epoch: 0, Step: 2592, Batch Loss: 0.09890056401491165\n",
      "Epoch: 0, Step: 2593, Batch Loss: 0.11084023118019104\n",
      "Epoch: 0, Step: 2594, Batch Loss: 0.12205313891172409\n",
      "Epoch: 0, Step: 2595, Batch Loss: 0.1335206776857376\n",
      "Epoch: 0, Step: 2596, Batch Loss: 0.12169990688562393\n",
      "Epoch: 0, Step: 2597, Batch Loss: 0.11292736232280731\n",
      "Epoch: 0, Step: 2598, Batch Loss: 0.1303204745054245\n",
      "Epoch: 0, Step: 2599, Batch Loss: 0.12233448773622513\n",
      "Epoch: 0, Step: 2600, Batch Loss: 0.12096263468265533\n",
      "Epoch: 0, Step: 2601, Batch Loss: 0.11406005918979645\n",
      "Epoch: 0, Step: 2602, Batch Loss: 0.11169631034135818\n",
      "Epoch: 0, Step: 2603, Batch Loss: 0.10238675028085709\n",
      "Epoch: 0, Step: 2604, Batch Loss: 0.0969298779964447\n",
      "Epoch: 0, Step: 2605, Batch Loss: 0.11337834596633911\n",
      "Epoch: 0, Step: 2606, Batch Loss: 0.10669679194688797\n",
      "Epoch: 0, Step: 2607, Batch Loss: 0.09995047748088837\n",
      "Epoch: 0, Step: 2608, Batch Loss: 0.12033336609601974\n",
      "Epoch: 0, Step: 2609, Batch Loss: 0.09988411515951157\n",
      "Epoch: 0, Step: 2610, Batch Loss: 0.14216405153274536\n",
      "Epoch: 0, Step: 2611, Batch Loss: 0.09989012777805328\n",
      "Epoch: 0, Step: 2612, Batch Loss: 0.10140436887741089\n",
      "Epoch: 0, Step: 2613, Batch Loss: 0.10675046592950821\n",
      "Epoch: 0, Step: 2614, Batch Loss: 0.11082175374031067\n",
      "Epoch: 0, Step: 2615, Batch Loss: 0.12809814512729645\n",
      "Epoch: 0, Step: 2616, Batch Loss: 0.11557873338460922\n",
      "Epoch: 0, Step: 2617, Batch Loss: 0.13272374868392944\n",
      "Epoch: 0, Step: 2618, Batch Loss: 0.1042780950665474\n",
      "Epoch: 0, Step: 2619, Batch Loss: 0.11547793447971344\n",
      "Epoch: 0, Step: 2620, Batch Loss: 0.11277461051940918\n",
      "Epoch: 0, Step: 2621, Batch Loss: 0.11328551918268204\n",
      "Epoch: 0, Step: 2622, Batch Loss: 0.10680223256349564\n",
      "Epoch: 0, Step: 2623, Batch Loss: 0.11551382392644882\n",
      "Epoch: 0, Step: 2624, Batch Loss: 0.1253160983324051\n",
      "Epoch: 0, Step: 2625, Batch Loss: 0.1126408725976944\n",
      "Epoch: 0, Step: 2626, Batch Loss: 0.12053750455379486\n",
      "Epoch: 0, Step: 2627, Batch Loss: 0.10750703513622284\n",
      "Epoch: 0, Step: 2628, Batch Loss: 0.21301151812076569\n",
      "Epoch: 0, Step: 2629, Batch Loss: 0.11593031883239746\n",
      "Epoch: 0, Step: 2630, Batch Loss: 0.10681943595409393\n",
      "Epoch: 0, Step: 2631, Batch Loss: 0.10771548748016357\n",
      "Epoch: 0, Step: 2632, Batch Loss: 0.11419054865837097\n",
      "Epoch: 0, Step: 2633, Batch Loss: 0.12073078006505966\n",
      "Epoch: 0, Step: 2634, Batch Loss: 0.10087878257036209\n",
      "Epoch: 0, Step: 2635, Batch Loss: 0.13682088255882263\n",
      "Epoch: 0, Step: 2636, Batch Loss: 0.1182197704911232\n",
      "Epoch: 0, Step: 2637, Batch Loss: 0.12263482064008713\n",
      "Epoch: 0, Step: 2638, Batch Loss: 0.1360590159893036\n",
      "Epoch: 0, Step: 2639, Batch Loss: 0.10584177076816559\n",
      "Epoch: 0, Step: 2640, Batch Loss: 0.1413668990135193\n",
      "Epoch: 0, Step: 2641, Batch Loss: 0.12042485922574997\n",
      "Epoch: 0, Step: 2642, Batch Loss: 0.12818212807178497\n",
      "Epoch: 0, Step: 2643, Batch Loss: 0.1285649985074997\n",
      "Epoch: 0, Step: 2644, Batch Loss: 0.11152857542037964\n",
      "Epoch: 0, Step: 2645, Batch Loss: 0.10361240059137344\n",
      "Epoch: 0, Step: 2646, Batch Loss: 0.10310354828834534\n",
      "Epoch: 0, Step: 2647, Batch Loss: 0.1266166716814041\n",
      "Epoch: 0, Step: 2648, Batch Loss: 0.105736643075943\n",
      "Epoch: 0, Step: 2649, Batch Loss: 0.1119152158498764\n",
      "Epoch: 0, Step: 2650, Batch Loss: 0.11673285067081451\n",
      "Epoch: 0, Step: 2651, Batch Loss: 0.12486572563648224\n",
      "Epoch: 0, Step: 2652, Batch Loss: 0.10443156212568283\n",
      "Epoch: 0, Step: 2653, Batch Loss: 0.10870812088251114\n",
      "Epoch: 0, Step: 2654, Batch Loss: 0.11713085323572159\n",
      "Epoch: 0, Step: 2655, Batch Loss: 0.10132823139429092\n",
      "Epoch: 0, Step: 2656, Batch Loss: 0.10835181176662445\n",
      "Epoch: 0, Step: 2657, Batch Loss: 0.10893933475017548\n",
      "Epoch: 0, Step: 2658, Batch Loss: 0.11761468648910522\n",
      "Epoch: 0, Step: 2659, Batch Loss: 0.10835181176662445\n",
      "Epoch: 0, Step: 2660, Batch Loss: 0.11574789136648178\n",
      "Epoch: 0, Step: 2661, Batch Loss: 0.11212650686502457\n",
      "Epoch: 0, Step: 2662, Batch Loss: 0.11588622629642487\n",
      "Epoch: 0, Step: 2663, Batch Loss: 0.1379597783088684\n",
      "Epoch: 0, Step: 2664, Batch Loss: 0.1105765700340271\n",
      "Epoch: 0, Step: 2665, Batch Loss: 0.10023196786642075\n",
      "Epoch: 0, Step: 2666, Batch Loss: 0.11635487526655197\n",
      "Epoch: 0, Step: 2667, Batch Loss: 0.21221353113651276\n",
      "Epoch: 0, Step: 2668, Batch Loss: 0.11354190856218338\n",
      "Epoch: 0, Step: 2669, Batch Loss: 0.1102910041809082\n",
      "Epoch: 0, Step: 2670, Batch Loss: 0.11629869788885117\n",
      "Epoch: 0, Step: 2671, Batch Loss: 0.15200451016426086\n",
      "Epoch: 0, Step: 2672, Batch Loss: 0.115920789539814\n",
      "Epoch: 0, Step: 2673, Batch Loss: 0.09890448302030563\n",
      "Epoch: 0, Step: 2674, Batch Loss: 0.10389180481433868\n",
      "Epoch: 0, Step: 2675, Batch Loss: 0.12049604952335358\n",
      "Epoch: 0, Step: 2676, Batch Loss: 0.10737685114145279\n",
      "Epoch: 0, Step: 2677, Batch Loss: 0.11312713474035263\n",
      "Epoch: 0, Step: 2678, Batch Loss: 0.10717620700597763\n",
      "Epoch: 0, Step: 2679, Batch Loss: 0.15752142667770386\n",
      "Epoch: 0, Step: 2680, Batch Loss: 0.10298838466405869\n",
      "Epoch: 0, Step: 2681, Batch Loss: 0.10498731583356857\n",
      "Epoch: 0, Step: 2682, Batch Loss: 0.11479710787534714\n",
      "Epoch: 0, Step: 2683, Batch Loss: 0.10206498205661774\n",
      "Epoch: 0, Step: 2684, Batch Loss: 0.10867353528738022\n",
      "Epoch: 0, Step: 2685, Batch Loss: 0.10429697483778\n",
      "Epoch: 0, Step: 2686, Batch Loss: 0.11709130555391312\n",
      "Epoch: 0, Step: 2687, Batch Loss: 0.11112048476934433\n",
      "Epoch: 0, Step: 2688, Batch Loss: 0.12404588609933853\n",
      "Epoch: 0, Step: 2689, Batch Loss: 0.11357973515987396\n",
      "Epoch: 0, Step: 2690, Batch Loss: 0.1053575873374939\n",
      "Epoch: 0, Step: 2691, Batch Loss: 0.11439457535743713\n",
      "Epoch: 0, Step: 2692, Batch Loss: 0.1024225503206253\n",
      "Epoch: 0, Step: 2693, Batch Loss: 0.13127677142620087\n",
      "Epoch: 0, Step: 2694, Batch Loss: 0.10894105583429337\n",
      "Epoch: 0, Step: 2695, Batch Loss: 0.11463643610477448\n",
      "Epoch: 0, Step: 2696, Batch Loss: 0.13261333107948303\n",
      "Epoch: 0, Step: 2697, Batch Loss: 0.12056554108858109\n",
      "Epoch: 0, Step: 2698, Batch Loss: 0.11743409186601639\n",
      "Epoch: 0, Step: 2699, Batch Loss: 0.10968957841396332\n",
      "Epoch: 0, Step: 2700, Batch Loss: 0.10020645707845688\n",
      "Step: 2700, Validation Loss: 0.11845762772779715, Validation Price Error (Normalized): 403.80263157894734\n",
      "Epoch: 0, Step: 2701, Batch Loss: 0.11349049210548401\n",
      "Epoch: 0, Step: 2702, Batch Loss: 0.09946611523628235\n",
      "Epoch: 0, Step: 2703, Batch Loss: 0.10386276990175247\n",
      "Epoch: 0, Step: 2704, Batch Loss: 0.10702162981033325\n",
      "Epoch: 0, Step: 2705, Batch Loss: 0.12245286256074905\n",
      "Epoch: 0, Step: 2706, Batch Loss: 0.10846318304538727\n",
      "Epoch: 0, Step: 2707, Batch Loss: 0.15877032279968262\n",
      "Epoch: 0, Step: 2708, Batch Loss: 0.11416476219892502\n",
      "Epoch: 0, Step: 2709, Batch Loss: 0.11881954222917557\n",
      "Epoch: 0, Step: 2710, Batch Loss: 0.11206689476966858\n",
      "Epoch: 0, Step: 2711, Batch Loss: 0.09552658349275589\n",
      "Epoch: 0, Step: 2712, Batch Loss: 0.13827253878116608\n",
      "Epoch: 0, Step: 2713, Batch Loss: 0.13118550181388855\n",
      "Epoch: 0, Step: 2714, Batch Loss: 0.12151475250720978\n",
      "Epoch: 0, Step: 2715, Batch Loss: 0.1094982922077179\n",
      "Epoch: 0, Step: 2716, Batch Loss: 0.1237683817744255\n",
      "Epoch: 0, Step: 2717, Batch Loss: 0.1015511229634285\n",
      "Epoch: 0, Step: 2718, Batch Loss: 0.11612407118082047\n",
      "Epoch: 0, Step: 2719, Batch Loss: 0.11467967182397842\n",
      "Epoch: 0, Step: 2720, Batch Loss: 0.10253282636404037\n",
      "Epoch: 0, Step: 2721, Batch Loss: 0.10001464933156967\n",
      "Epoch: 0, Step: 2722, Batch Loss: 0.1398611217737198\n",
      "Epoch: 0, Step: 2723, Batch Loss: 0.09701056033372879\n",
      "Epoch: 0, Step: 2724, Batch Loss: 0.09582018852233887\n",
      "Epoch: 0, Step: 2725, Batch Loss: 0.1205839291214943\n",
      "Epoch: 0, Step: 2726, Batch Loss: 0.09633665531873703\n",
      "Epoch: 0, Step: 2727, Batch Loss: 0.13083496689796448\n",
      "Epoch: 0, Step: 2728, Batch Loss: 0.11827097088098526\n",
      "Epoch: 0, Step: 2729, Batch Loss: 0.0989232063293457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./saved_models/best_model)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 2730, Batch Loss: 0.10253282636404037\n",
      "Epoch: 0, Step: 2731, Batch Loss: 0.10441512614488602\n",
      "Epoch: 0, Step: 2732, Batch Loss: 0.12080122530460358\n",
      "Epoch: 0, Average Training Loss: 0.6382453013004528, Average Training Price Error: 2377.342259259259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done. 40.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bdd1ac90304c56841e5ebcfb551c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='5.491 MB of 5.491 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Average Training Loss</td><td>â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>Average Training Price Error</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>Batch Loss</td><td>â–ˆâ–…â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>Epoch</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>Step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>Validation Loss</td><td>â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>Validation Price Error (Average)</td><td>â–‡â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–â–‚â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Average Training Loss</td><td>0.63825</td></tr><tr><td>Average Training Price Error</td><td>2377.34226</td></tr><tr><td>Batch Loss</td><td>0.1208</td></tr><tr><td>Epoch</td><td>0</td></tr><tr><td>Step</td><td>2732</td></tr><tr><td>Validation Loss</td><td>0.11846</td></tr><tr><td>Validation Price Error (Average)</td><td>403.80263</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">celestial-sea-5</strong> at: <a href='https://wandb.ai/davidsajare/davidwei-phi3-v/runs/c1x5amoy' target=\"_blank\">https://wandb.ai/davidsajare/davidwei-phi3-v/runs/c1x5amoy</a><br/> View project at: <a href='https://wandb.ai/davidsajare/davidwei-phi3-v' target=\"_blank\">https://wandb.ai/davidsajare/davidwei-phi3-v</a><br/>Synced 6 W&B file(s), 18 media file(s), 198 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240601_103239-c1x5amoy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import resize, to_pil_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "run = wandb.init(project=\"davidwei-phi3-v\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset for Burberry Product Prices and Images\n",
    "class BurberryProductDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length, image_size):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.padding_side = 'left'\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = f\"<|user|>\\n<|image_1|>What is shown in this image?<|end|><|assistant|>\\nProduct: {row['title']}, Category: {row['category3_code']}, Full Price: {row['full_price']}<|end|>\"\n",
    "        image_path = row['local_image_path']\n",
    "        \n",
    "        # Tokenize text\n",
    "        encodings = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        \n",
    "        try:\n",
    "            # Load and transform image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.image_transform_function(image)\n",
    "        except (FileNotFoundError, IOError):\n",
    "            # Skip the sample if the image is not found\n",
    "            return None\n",
    "        \n",
    "        encodings['pixel_values'] = image\n",
    "        encodings['price'] = row['full_price']\n",
    "        \n",
    "        return {key: torch.tensor(val) for key, val in encodings.items()}\n",
    "\n",
    "\n",
    "    def image_transform_function(self, image):\n",
    "        image = np.array(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset from disk\n",
    "dataset_path = './data/burberry_dataset/burberry_dataset.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "# Initialize processor and tokenizer\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.9 * len(df))\n",
    "val_size = len(df) - train_size\n",
    "train_indices, val_indices = random_split(range(len(df)), [train_size, val_size])\n",
    "train_indices = train_indices.indices\n",
    "val_indices = val_indices.indices\n",
    "train_df = df.iloc[train_indices]\n",
    "val_df = df.iloc[val_indices]\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = BurberryProductDataset(train_df, tokenizer, max_length=512, image_size=128)\n",
    "val_dataset = BurberryProductDataset(val_df, tokenizer, max_length=512, image_size=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "eval_interval = 150  # Evaluate every 'eval_interval' steps\n",
    "loss_scaling_factor = 1000.0  # Variable to scale the loss by a certain amount\n",
    "save_dir = './saved_models'\n",
    "step = 0\n",
    "accumulation_steps = 64  # Accumulate gradients over this many steps\n",
    "\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = None\n",
    "\n",
    "\n",
    "# Select 10 images from the validation set for logging\n",
    "num_log_samples = 10\n",
    "log_indices = random.sample(range(len(val_dataset)), num_log_samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_price_from_predictions(predictions, tokenizer):\n",
    "    # Assuming the price is at the end of the text and separated by a space\n",
    "    predicted_text = tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
    "    try:\n",
    "        predicted_price = float(predicted_text.split()[-1].replace(',', ''))\n",
    "    except ValueError:\n",
    "        predicted_price = 0.0\n",
    "    return predicted_price\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, device, tokenizer, step, log_indices, max_samples=None, ):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_price_error = 0\n",
    "    log_images = []\n",
    "    log_gt_texts = []\n",
    "    log_pred_texts = []\n",
    "    table = wandb.Table(columns=[\"Image\", \"Ground Truth Text\", \"Predicted Text\"])\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "\n",
    "\n",
    "            if batch is None:  # Skip if the batch is None\n",
    "                continue\n",
    "\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = input_ids.clone().detach()\n",
    "            actual_price = batch['price'].item()\n",
    "\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                pixel_values=pixel_values, \n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "            # Calculate price error\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            predicted_price = extract_price_from_predictions(predictions, tokenizer)\n",
    "            price_error = abs(predicted_price - actual_price)\n",
    "            total_price_error += price_error\n",
    "\n",
    "\n",
    "            # Log images, ground truth texts, and predicted texts\n",
    "            if i in log_indices:\n",
    "                log_images.append(pixel_values.cpu().squeeze().numpy())\n",
    "                log_gt_texts.append(tokenizer.decode(labels[0], skip_special_tokens=True))\n",
    "                log_pred_texts.append(tokenizer.decode(predictions[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "                # Convert image to PIL format\n",
    "                pil_img = to_pil_image(resize(torch.from_numpy(log_images[-1]).permute(2, 0, 1), (336, 336))).convert(\"RGB\")\n",
    "                \n",
    "                # Add data to the table\n",
    "                table.add_data(wandb.Image(pil_img), log_gt_texts[-1], log_pred_texts[-1])\n",
    "\n",
    "\n",
    "                # Log the table incrementally\n",
    "    \n",
    "    wandb.log({\"Evaluation Results step {}\".format(step): table, \"Step\": step})\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / (i + 1)  # i+1 to account for the loop index\n",
    "    avg_price_error = total_price_error / (i + 1)\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    return avg_loss, avg_price_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):  # Number of epochs\n",
    "    total_train_loss = 0\n",
    "    total_train_price_error = 0\n",
    "    batch_count = 0\n",
    "\n",
    "\n",
    "    for batch in train_loader:\n",
    "        step += 1\n",
    "\n",
    "\n",
    "        if batch is None:  # Skip if the batch is None\n",
    "            continue\n",
    "\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = input_ids.clone().detach()\n",
    "        actual_price = batch['price'].float().to(device)\n",
    "\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            pixel_values=pixel_values, \n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        total_loss = loss\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)            \n",
    "        predicted_price = extract_price_from_predictions(predictions, tokenizer)\n",
    "\n",
    "\n",
    "        \n",
    "        total_loss.backward()\n",
    "\n",
    "\n",
    "        if (step % accumulation_steps) == 0:\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad /= accumulation_steps\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "        total_train_price_error += abs(predicted_price - actual_price.item())\n",
    "        batch_count += 1\n",
    "\n",
    "\n",
    "        # Log batch loss to wandb\n",
    "        wandb.log({\"Batch Loss\": total_loss.item(), \"Step\": step})\n",
    "\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Step: {step}, Batch Loss: {total_loss.item()}\")\n",
    "\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            val_loss, val_price_error = evaluate(model, val_loader, device, tokenizer=tokenizer, log_indices=log_indices, step=step )\n",
    "            wandb.log({\n",
    "                \"Validation Loss\": val_loss,\n",
    "                \"Validation Price Error (Average)\": val_price_error,\n",
    "                \"Step\": step\n",
    "            })\n",
    "            print(f\"Step: {step}, Validation Loss: {val_loss}, Validation Price Error (Normalized): {val_price_error}\")\n",
    "\n",
    "\n",
    "            # Save the best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_path = os.path.join(save_dir, f\"best_model\")\n",
    "                model.save_pretrained(best_model_path, safe_serialization=False)\n",
    "                tokenizer.save_pretrained(best_model_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            avg_train_loss = total_train_loss / batch_count\n",
    "            avg_train_price_error = total_train_price_error / batch_count\n",
    "            wandb.log({\n",
    "                \"Epoch\": epoch,\n",
    "                \"Average Training Loss\": avg_train_loss,\n",
    "                \"Average Training Price Error\": avg_train_price_error\n",
    "            })\n",
    "            \n",
    "    print(f\"Epoch: {epoch}, Average Training Loss: {avg_train_loss}, Average Training Price Error: {avg_train_price_error}\")\n",
    "\n",
    "\n",
    "    if best_model_path:\n",
    "        run.log_model(\n",
    "            path=best_model_path,\n",
    "            name=\"phi3-v-burberry\",\n",
    "            aliases=[\"best\"],\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4nltzzqw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2209caef96344399b0869965ba36cb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4nltzzqw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wandb/run-20240601_130339-adxe85qj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/davidsajare/davidwei-phi3-v-test/runs/adxe85qj' target=\"_blank\">eager-terrain-19</a></strong> to <a href='https://wandb.ai/davidsajare/davidwei-phi3-v-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/davidsajare/davidwei-phi3-v-test' target=\"_blank\">https://wandb.ai/davidsajare/davidwei-phi3-v-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/davidsajare/davidwei-phi3-v-test/runs/adxe85qj' target=\"_blank\">https://wandb.ai/davidsajare/davidwei-phi3-v-test/runs/adxe85qj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact davidwei-phi3-v:v0, 7911.12MB. 8 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
      "Done. 0:0:6.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact downloaded to: /root/artifacts/phi3-v-burberry:v0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8dd3093fa84dd9a4df105939ab52b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: davidsajare.\n",
      "View Weave data at https://wandb.ai/davidsajare/davidwei-phi3-v-test/weave\n",
      "ðŸ© https://wandb.ai/davidsajare/davidwei-phi3-v-test/r/call/5e30baee-2e57-4dbf-8f7f-9c5051197045\n",
      "Predicted Text: 1. The price for size 6.5 is $115.00. 2. The good is Nike Dunk Low DD1503-101 Women's White Black Leather Sneaker Shoes Size 9 PRO43.\n",
      "Image Data URL: data:image/jpeg;base64,/9j/4AAQSkZAJKKKKAP/9k=\n"
     ]
    }
   ],
   "source": [
    "import wandb    \n",
    "import os    \n",
    "import torch    \n",
    "from transformers import AutoModelForCausalLM, AutoProcessor    \n",
    "from PIL import Image    \n",
    "import requests    \n",
    "from io import BytesIO    \n",
    "import base64    \n",
    "from pathlib import Path    \n",
    "import weave  # Add this import  \n",
    "  \n",
    "# Initialize Weights & Biases run    \n",
    "run = wandb.init(project='davidwei-phi3-v-test')    \n",
    "  \n",
    "# Use the artifact    \n",
    "artifact = run.use_artifact('davidsajare/model-registry/davidwei-phi3-v:v0', type='model')    \n",
    "artifact_dir = artifact.download()    \n",
    "print(f\"Artifact downloaded to: {artifact_dir}\")    \n",
    "  \n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\"    \n",
    "  \n",
    "try:    \n",
    "    model = AutoModelForCausalLM.from_pretrained(    \n",
    "        artifact_dir,     \n",
    "        torch_dtype=torch.float16,     \n",
    "        attn_implementation=\"flash_attention_2\",    \n",
    "        trust_remote_code=True    \n",
    "    )    \n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)    \n",
    "except Exception as e:    \n",
    "    print(f\"Error loading model or processor: {e}\")    \n",
    "    raise    \n",
    "  \n",
    "# Ensure the model is on the correct device    \n",
    "device = 'cuda'    \n",
    "model.to(device)    \n",
    "  \n",
    "# Function to convert image to data URL    \n",
    "EXT_TO_MIMETYPE = {    \n",
    "    '.jpg': 'image/jpeg',    \n",
    "    '.jpeg': 'image/jpeg',    \n",
    "    '.png': 'image/png',    \n",
    "    '.svg': 'image/svg+xml'    \n",
    "}    \n",
    "  \n",
    "def image_to_data_url(image: Image.Image, ext: str) -> str:    \n",
    "    ext = ext.lower()    \n",
    "    if ext not in EXT_TO_MIMETYPE:    \n",
    "        ext = '.jpg'  # Default to .jpg if extension is not recognized    \n",
    "    mimetype = EXT_TO_MIMETYPE[ext]    \n",
    "    buffered = BytesIO()    \n",
    "    image_format = 'JPEG' if ext in ['.jpg', '.jpeg'] else ext.replace('.', '').upper()    \n",
    "    image.save(buffered, format=image_format)    \n",
    "    encoded_string = base64.b64encode(buffered.getvalue()).decode('utf-8')    \n",
    "    data_url = f\"data:{mimetype};base64,{encoded_string}\"    \n",
    "    return data_url    \n",
    "  \n",
    "# Function to run inference on a single image    \n",
    "@weave.op()    \n",
    "def run_inference(image_path_or_url: str) -> dict:    \n",
    "    try:    \n",
    "        prompt = \"<|user|>\\n<|image_1|>1.what is the price in 6.5? 2.What is the good?<|end|><|assistant|>\\n\"    \n",
    "        #prompt = \"<|user|>\\n<|image_1|>what is imageï¼Ÿ<|end|><|assistant|>\\n\"    \n",
    "        # Check if the input is a URL or a local file path  \n",
    "        if image_path_or_url.startswith(\"http://\") or image_path_or_url.startswith(\"https://\"):  \n",
    "            # Load image from URL  \n",
    "            image = Image.open(requests.get(image_path_or_url, stream=True).raw)    \n",
    "            ext = Path(image_path_or_url).suffix  \n",
    "        else:  \n",
    "            # Load image from local file path  \n",
    "            image = Image.open(image_path_or_url)  \n",
    "            ext = Path(image_path_or_url).suffix  \n",
    "  \n",
    "        # Convert image to data URL    \n",
    "        data_url = image_to_data_url(image, ext)    \n",
    "        inputs = processor(prompt, [image], return_tensors=\"pt\").to(device)    \n",
    "        generation_args = {     \n",
    "            \"max_new_tokens\": 500,     \n",
    "            \"temperature\": 0.0,     \n",
    "            \"do_sample\": False,     \n",
    "        }    \n",
    "        generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)    \n",
    "        # Remove input tokens    \n",
    "        generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]    \n",
    "        response_text = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]    \n",
    "        return {    \n",
    "            \"predicted_text\": response_text,    \n",
    "            \"image_data_url\": data_url    \n",
    "        }    \n",
    "    except Exception as e:    \n",
    "        print(f\"Error during inference: {e}\")    \n",
    "        raise    \n",
    "  \n",
    "# Initialize Weave project    \n",
    "weave.init('davidwei-phi3-v-test')    \n",
    "  \n",
    "# Example usage    \n",
    "# For local image  \n",
    "# image_path_or_url = \"/path/to/your/local/image.jpg\"  \n",
    "# For URL image  \n",
    "#image_path_or_url = \"https://assets.burberry.com/is/image/Burberryltd/1C09D316-7A71-472C-8877-91CEFBDB268A?$BBY_V3_SL_1$&wid=1501&hei=1500\"  \n",
    "image_path_or_url = \"/root/5.jpg\"  \n",
    "try:    \n",
    "    result = run_inference(image_path_or_url)    \n",
    "    print(\"Predicted Text:\", result['predicted_text'])    \n",
    "    print(\"Image Data URL:\", result['image_data_url'])    \n",
    "except Exception as e:    \n",
    "    print(f\"Error running inference: {e}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw------- 1 root root 75327 Jun  1 09:48 /root/2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -al /root/2.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
