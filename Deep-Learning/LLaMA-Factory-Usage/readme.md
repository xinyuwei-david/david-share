# æ·±å…¥è§£æå¤§æ¨¡å‹å¹¶è¡Œè®­ç»ƒæ–¹æ³•ï¼šDPã€FSDPã€TPã€PP è¯¦è§£ä¸å®è·µæŒ‡å—

è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¿«é€Ÿå‘å±•ï¼Œå‚æ•°è§„æ¨¡è¿…é€Ÿå¢é•¿è‡³æ•°åäº¿ç”šè‡³æ•°åƒäº¿çº§åˆ«ï¼Œæ¨åŠ¨äº† GPTã€LLaMA ç­‰æ¨¡å‹å¹¿æ³›åº”ç”¨ã€‚ä½†æ¨¡å‹å‚æ•°çš„å¢é•¿ä¹Ÿå¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ï¼šå¦‚ä½•é«˜æ•ˆåœ°åœ¨ GPU é›†ç¾¤ä¸Šè¿›è¡Œå¤§æ¨¡å‹è®­ç»ƒæˆ–å¾®è°ƒï¼Ÿé’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œä¸šç•Œå¸¸ç”¨çš„æ¨¡å‹å¹¶è¡Œç­–ç•¥åŒ…æ‹¬æ•°æ®å¹¶è¡Œï¼ˆDP/DDPï¼‰ã€å…¨å‚æ•°åˆ†ç‰‡å¹¶è¡Œï¼ˆZeRO/FSDPï¼‰ã€å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰å’Œæµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰ã€‚ç„¶è€Œï¼Œå¾ˆå¤šäººåœ¨å®è·µæ—¶å®¹æ˜“æ··æ·†å®ƒä»¬ä¹‹é—´çš„æœ¬è´¨åŒºåˆ«ä¸é€‚ç”¨åœºæ™¯ã€‚

æœ¬æ–‡åŸºäºæˆ‘æœ€è¿‘ä¸€æ¬¡ä¸æŠ€æœ¯åŒäº‹çš„é•¿æ—¶é—´æ¢è®¨è®°å½•ï¼Œç³»ç»Ÿå…¨é¢åœ°å‰–æ DPã€FSDPã€TP ä¸ PP çš„åŸç†ã€ä¼˜ç¼ºç‚¹ã€é€‚ç”¨åœºæ™¯ï¼Œå¹¶ç»™å‡ºå®æ“æŒ‡å¯¼ï¼Œä»¥æœŸå¸®åŠ©æ›´å¤šçš„å·¥ç¨‹å¸ˆç‰¢ç‰¢æŒæ¡å¹¶çµæ´»è¿ç”¨å®ƒä»¬ã€‚

## ä¸€. åŸºç¡€çŸ¥è¯†ä¸æ ¸å¿ƒæ¦‚å¿µ

### 1. ä¸€ä¸ª Transformer æ¨¡å‹çš„ç»“æ„ç»„æˆæœ‰å“ªäº›ï¼Ÿ

ä»¥ç»å…¸çš„ Transformer ä¸ºä¾‹ï¼Œç»“æ„åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

- **å¤šå±‚å †å ç»“æ„**ï¼šä¸€ä¸ªTransformeræ¨¡å‹ç”±å¤§é‡å±‚ï¼ˆå¦‚12å±‚ã€24å±‚ã€40å±‚ç”šè‡³æ›´å¤šï¼‰ä¸²è”ç»„æˆï¼›
- å‚æ•° (å‚æ•°/æƒé‡): åœ¨æ¯ä¸€å±‚ä¸­ï¼Œå¯è®­ç»ƒçš„æµ®ç‚¹æ•°å¼ é‡ï¼Œå°±æ˜¯æ¨¡å‹çš„â€œå‚æ•°â€æˆ–â€œæƒé‡â€ï¼Œå¦‚ï¼š
  - è‡ªæ³¨æ„åŠ›ä¸­ç”¨äºè®¡ç®— Qã€Kã€Vã€O çš„çº¿æ€§æ˜ å°„çŸ©é˜µ
  - Feed Forwardï¼ˆMLPï¼‰å±‚ä¸­çš„æŠ•å½±çŸ©é˜µ
  - Layer Norm å±‚ä¸­çš„ Î³ å’Œ Î² å‘é‡
  - åµŒå…¥è¡¨ï¼ˆEmbedding Matrixï¼‰
- **æ¿€æ´»**: å‰å‘ä¼ æ’­æ—¶æ¯å±‚è¾“å‡ºçš„ä¸­é—´ç»“æœ
- **æ¢¯åº¦**: åå‘ä¼ æ’­æ—¶è®¡ç®—çš„ç›¸å¯¹äºæ¯ä¸ªå‚æ•°çš„æŸå¤±å‡½æ•°å¯¼æ•°ï¼Œç»´åº¦ä¸å‚æ•°ç›¸åŒ
- **ä¼˜åŒ–å™¨çŠ¶æ€**: å¦‚ Adam ä¸­çš„ m å’Œ vï¼Œé€šå¸¸å¤§å°çº¦ä¸ºå‚æ•°çš„ 1ï½2 å€

æ˜ç¡®è¿™äº›æ¦‚å¿µï¼Œå¯¹äºç†è§£åç»­çš„å¹¶è¡Œç­–ç•¥è‡³å…³é‡è¦ã€‚

------

## äºŒ. DPã€FSDPã€TPã€PP æ·±å…¥è§£æ

ç°åœ¨æˆ‘ä»¬äº†è§£äº†å¤§æ¨¡å‹åº•å±‚å­˜å‚¨çš„è¿™äº›å¼ é‡ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ DPã€FSDPã€TP å’Œ PP æœ¬è´¨ä¸ŠåŒºåˆ«ä¸è”ç³»ã€‚

### 1ã€Data Parallel (DP/DDP)

æœ€ç»å…¸å’Œä¼ ç»Ÿçš„å¹¶è¡Œæ–¹å¼ï¼Œæ¯ä¸ª GPU éƒ½å®Œæ•´å¤åˆ¶ä¸€ä»½æ¨¡å‹ã€‚

- **æ ¸å¿ƒæ“ä½œ**ï¼š
  - æ¯ä¸ª GPU å­˜å‚¨ä¸€ä»½å®Œæ•´æ¨¡å‹ã€‚
  - batch æ ·æœ¬å‡åŒ€åˆ’åˆ†ï¼Œæ¯ä¸ª GPU ç‹¬ç«‹å‰å‘ä¼ æ’­ã€‚
  - åå‘ä¼ æ’­åï¼Œå„ GPU æƒé‡æ¢¯åº¦è¿›è¡Œä¸€æ¬¡ All-reduce åŒæ­¥ã€‚
- **æ˜¾å­˜æ¶ˆè€—**ï¼š å•ä¸ª GPU æ˜¾å­˜ â‰¥ æ¨¡å‹æ•´ä½“å‚æ•°ï¼ˆå®Œæ•´å­˜å‚¨å‚æ•°/æ¢¯åº¦/m/vï¼‰+ æ‰¹æ¬¡å¯¹åº”çš„æ¿€æ´»å†…å­˜ã€‚
- **ä¼˜ç‚¹**ï¼š æœ€å®¹æ˜“å®ç°ï¼Œå·¥ç¨‹æ˜“ç”¨ï¼Œæ¡†æ¶æˆç†Ÿï¼›
- **ç¼ºç‚¹**ï¼ˆå°¤ä¸ºé‡è¦ï¼‰ï¼š éšæ¨¡å‹å˜å¤§ï¼Œå• GPU æ˜¾å­˜æ˜¾è‘—åˆ¶çº¦ DP è§„æ¨¡ï¼Œæ¨¡å‹å‚æ•°è¶…è¿‡å•ä¸ª GPU æ˜¾å­˜å°±æ— æ³•ä½¿ç”¨ DPã€‚

------

### 2ã€Fully-Sharded Data Parallelï¼ˆFSDP / ZeRO-3ï¼‰

ä¸ºäº†çªç ´ DP å•å¡æ˜¾å­˜çš„é™åˆ¶ï¼ŒFSDP å°†æ‰€æœ‰å‚æ•°å¼ é‡åˆ‡æˆç¢ç‰‡å‡åŒ€åˆ†é…åˆ°å„ GPU å­˜å‚¨ã€‚

- **æ ¸å¿ƒæ“ä½œ**ï¼ˆä½ åå¤å¼ºè°ƒå¹¶æœ€ç»ˆå½»åº•ç†è§£çš„ï¼‰ï¼š

  1. **å‚æ•°å¸¸é©»**ï¼šæ¯å±‚æ¯ä¸ªå‚æ•°å¼ é‡åˆ‡ç‰‡ï¼Œæ¯ä¸ª GPU ä»…ä¿å­˜æ¨¡å‹å…¨å‚æ•°çš„ 1/N ç¢ç‰‡ï¼ˆè®°ä½ä¸æ˜¯â€œéƒ¨åˆ†å±‚â€ï¼Œè€Œæ˜¯â€œæ‰€æœ‰å±‚çš„ 1/N ç‰‡æ®µâ€ï¼‰

  2. è®¡ç®—æ—¶

     ï¼š

     - è®¡ç®—å±‚å‰å…ˆ `all-gather`ï¼šä»å…¶ä»– GPU æ‹‰å–å¯¹åº”å±‚å‰©ä½™çš„ `N-1/N` ç¢ç‰‡ï¼Œæ‹¼è£…å‡ºå®Œæ•´çŸ©é˜µï¼›
     - æ‰§è¡Œå‰å‘ã€åå‘ä¼ æ’­ï¼›
     - è®¡ç®—å®Œæˆåç«‹å³ `reduce-scatter` å°†æ¢¯åº¦çŸ©é˜µæ‹†å›ç¢ç‰‡è¿”å›å¯¹åº” GPUã€‚
     - é‡Šæ”¾ä¸´æ—¶å®Œæ•´æƒé‡çŸ©é˜µçš„æ˜¾å­˜ã€‚

- **æ˜¾å­˜ç”¨é‡å³°å€¼å…³é”®ç‚¹**ï¼ˆä½ åå¤å¤šæ¬¡è¿½é—®å¹¶æœ€ç»ˆç¡®è®¤æ¸…æ¥šçš„è¦ç´ ï¼‰ï¼š

  FSDP çš„é™åˆ¶å› ç´ å¹¶ä¸æ˜¯æ¨¡å‹æ€»å¤§å°ï¼ˆå› ä¸ºå‚æ•°å¹³æ‘Šäº†ï¼‰ï¼Œè€Œæ˜¯**ã€Œæ¨¡å‹çš„å•å±‚å®Œæ•´çŸ©é˜µèƒ½å¦å­˜è¿›å• GPU æ˜¾å­˜ã€**ã€‚

  ç›´è§‚è€Œè¨€ï¼Œå› ä¸ºè®¡ç®—æ¯å±‚æ—¶ï¼Œéƒ½å­˜åœ¨ä¸€æ¬¡å®Œæ•´çŸ©é˜µçš„æ‹¼æ¥åŠ¨ä½œï¼Œå› æ­¤å½“æŸä¸€å±‚çŸ©é˜µæœ¬èº«å°±å¤§äºå•å¼  GPU æ˜¾å­˜æ—¶ï¼ŒFSDP æ— èƒ½ä¸ºåŠ›ï¼Œå¿…é¡»æ±‚åŠ©äºå¼ é‡å¹¶è¡Œï¼ˆTPï¼‰æ–¹æ³•ã€‚

------

### 3ã€Tensor Parallelï¼ˆTPï¼‰

TP æ˜¯ä¸€ç§æ›´åŠ ç²¾å¦™çš„ç­–ç•¥ï¼Œä¸“é—¨å§‘æ¯å•å±‚è¶…å¤§çŸ©é˜µæ— æ³•å¡è¿›å• GPU çš„é—®é¢˜ã€‚

- æ ¸å¿ƒæ“ä½œï¼ˆé‡ç‚¹å¼ºè°ƒï¼Œæä¸ºé‡è¦ï¼‰

  ï¼š

  - åŒå±‚å†…å¤§çŸ©é˜µæƒé‡æŒ‰è¡Œæˆ–åˆ—åˆ‡å‰²è‡³å¤šå¡å­˜å‚¨ï¼›
  - æ¯ä¸ª GPU åªå®Œæˆâ€œå±€éƒ¨â€çŸ©é˜µä¹˜ï¼›
  - å±€éƒ¨ç»“æœç”¨ `all-reduce` æ‹¼å›å…¨å±€ç»“æœã€‚

ä¸ FSDP æœ€æœ¬è´¨åŒºåˆ«æ˜¯ï¼Œ**å¼ é‡å¹¶è¡Œ TP ä»ä¸åœ¨å• GPU ä¸­å‡ºç°å®Œæ•´çŸ©é˜µ**ï¼Œè€Œ FSDP æ¯æ¬¡è®¡ç®—æ—¶å¿…å®šå‡ºç°ä¸€æ¬¡å®Œæ•´çŸ©é˜µæ‹¼æ¥ã€‚å› æ­¤ï¼ŒTP æ‰èƒ½è®­ç»ƒå•ä¸ªè¶…å¤§çŸ©é˜µè¶…è¿‡äº†ä¸€å¼ å¡æ˜¾å­˜çš„æƒ…æ™¯ã€‚

- ç¼ºç‚¹ï¼šé«˜åº¦é¢‘ç¹çš„å±€éƒ¨èšåˆå’Œå…¨å±€é€šä¿¡ï¼Œé€šå¸¸è¦æ±‚ GPU é—´ä¸º NVlink çº§åˆ«çš„é«˜é€Ÿäº’è¿ã€‚

------

### 4ã€Pipeline Parallel (PP)

PP çš„å‡ºå‘ç‚¹æˆªç„¶ä¸åŒï¼Œä»å±‚çš„ç»´åº¦åˆ‡å‰²æ¨¡å‹ï¼Œæ¯å¼  GPU ä¸“é—¨è´Ÿè´£æ¨¡å‹è¿ç»­çš„ä¸€éƒ¨åˆ†å±‚ï¼Œä¾‹å¦‚ï¼š

- GPU-0è´Ÿè´£ç¬¬1-6å±‚ï¼ŒGPU-1è´Ÿè´£ç¬¬7-12å±‚...
- æ¿€æ´»æ•°æ®é€å±‚é¡ºæ¬¡ä¼ é€’ï¼Œè®¡ç®—å¦‚æµæ°´çº¿èˆ¬è¿›è¡Œã€‚
- ä¼˜ç‚¹ï¼š
  - è·¨æœºå™¨åˆ†å±‚å­˜å‚¨çµæ´»ï¼ˆå¤šæœºæ¶é›†ç¾¤å‹å¥½ï¼‰ï¼›
  - åªä¼ é€’æ¿€æ´»æ•°æ®ï¼Œé€šä¿¡å¼€é”€é€‚å½“ã€‚
- å±€é™ï¼š
  - å¯¹å•å±‚å·¨å¤§çŸ©é˜µä¸èµ·ä½œç”¨ï¼ˆé€‚åˆå±‚æ•°ä¼—å¤šä½†å•å±‚çŸ©é˜µé€‚ä¸­çš„æƒ…å†µï¼‰ï¼›
  - æµæ°´çº¿è°ƒåº¦å®¹æ˜“äº§ç”Ÿç©ºé—²æ—¶é—´ï¼ˆæ°”æ³¡bubbleï¼‰ï¼Œéœ€è¦ç²¾ç»†è°ƒä¼˜ã€‚

------

## ä¸‰. å¯¹æ¯”è¡¨æ ¼

**FSDPï¼ˆFully-Sharded Data Parallelï¼‰é€‚ç”¨äºè¿™ç§åœºæ™¯ï¼š**

- æ¨¡å‹æ•´ä½“å‚æ•°é‡å¾ˆå¤§ï¼ˆæ‰€æœ‰å±‚å‚æ•°ä¹‹å’Œå¤§äºå•ä¸ªGPUæ˜¾å­˜ï¼‰ï¼Œä»¥è‡³äºæ— æ³•ç”¨çº¯DP/DDPåŠ è½½å…¨éƒ¨å‚æ•°ã€‚
- ä½†æ˜¯å•ä¸ªæ¨¡å‹å±‚çš„çŸ©é˜µï¼ˆæ¯”å¦‚æœ€å¤§çš„é‚£ä¸ªprojectionçŸ©é˜µï¼‰å¯ä»¥æ”¾è¿›å•ä¸ªGPUæ˜¾å­˜ä¸­ã€‚

è¿™å¥è¯æœ€å…³é”®çš„æ˜¯ï¼š

> â—**FSDPçš„ç“¶é¢ˆä¸åœ¨äºæ¨¡å‹å…¨éƒ¨å‚æ•°æœ‰å¤šå¤§ï¼Œè€Œåœ¨äºæ¨¡å‹çš„å•å±‚çŸ©é˜µå¤§å°ã€‚**
> â—å› æ­¤ï¼Œä½¿ç”¨FSDPå‰å¿…é¡»ä¿è¯æœ€å¤§å•å±‚çš„å®Œæ•´å‚æ•° **(å«æ¢¯åº¦/ä¼˜åŒ–å™¨çŠ¶æ€)** èƒ½å¤Ÿæ”¾å…¥å•ä¸€GPUæ˜¾å­˜ã€‚



## **ä»€ä¹ˆæƒ…å†µä¸‹FSDPä¸è¡Œï¼Ÿ**

- å½“æ¨¡å‹çš„**å•å±‚å®Œæ•´çŸ©é˜µæœ¬èº«**å·²ç»å¤§äºå•ä¸ªGPUæ˜¾å­˜ï¼Œ
  å³ä¾¿æ¯å¼ GPUä¿å­˜çš„åªæ˜¯ç¢ç‰‡ï¼Œä½†åœ¨è®¡ç®—è¯¥å±‚æ—¶æ€»éœ€è¦`all-gather`ä¸€æ¬¡å®Œæ•´çŸ©é˜µï¼Œè¿™ä¸ªç¬é—´å°±ä¼šOOMã€‚
- å› æ­¤ï¼Œè¿™ç§æ—¶å€™FSDPä¸€å®š**æ— æ³•é€‚ç”¨**ï¼Œè€Œåªèƒ½æ±‚åŠ©äºTPï¼ˆTensor Parallelï¼‰æ–¹æ³•ã€‚



- æ¨¡å‹è¾ƒå¤§ï¼Œä½†å•å±‚çŸ©é˜µä¾ç„¶æ¯”å•GPUæ˜¾å­˜å° â‡’ ä½¿ç”¨ FSDP

- æ¨¡å‹å¤§è€Œä¸”å•å±‚æœ€å¤§çŸ©é˜µè¶…å‡ºå•GPUæ˜¾å­˜ â‡’ æ— æ³•ä½¿ç”¨ FSDPï¼Œéœ€é‡‡ç”¨ TPï¼ˆå¦‚ Megatron-LMã€DeepSpeed-TP ç­‰ï¼‰ã€‚

  | æ¨¡å‹åœºæ™¯                 | DP(DDP)             | FSDP/ZeRO           | TP                     | PP                        |
  | ------------------------ | ------------------- | ------------------- | ---------------------- | ------------------------- |
  | â‰¤ å•GPUæ˜¾å­˜              | âœ…ï¼ˆæ¨èï¼‰           | ğŸ”´ï¼ˆä¸å¿…è¦ï¼‰         | ğŸ”´ï¼ˆä¸å¿…è¦ï¼‰            | ğŸ”´ï¼ˆä¸å¿…è¦ï¼‰               |
  | æ•´ä½“å¤§ï¼Œä½†å•å±‚â‰¤å•GPUæ˜¾å­˜ | ğŸ”´(æ— æ³•è£…è½½å…¨éƒ¨æ¨¡å‹) | âœ…ï¼ˆæ¨èï¼‰           | ğŸ”´ï¼ˆä¸å¿…è¦ï¼‰            | ğŸ”¶(è€ƒè™‘å¤šèŠ‚ç‚¹æ—¶å¯é€‰)       |
  | å•å±‚ > å• GPU æ˜¾å­˜       | ğŸ”´(æ— æ³•è£…è½½å…¨éƒ¨æ¨¡å‹) | ğŸ”´ ï¼ˆå•å±‚æ‹¼æ¥æ—¶OOMï¼‰ | âœ…ï¼ˆå”¯ä¸€æ–¹æ¡ˆï¼‰          | ğŸ”¶(ç»“åˆTPæ‰©å±•é›†ç¾¤è§„æ¨¡å¯é€‰) |
  | å±‚æ•°éå¸¸å¤šï¼Œå¤šæœºé›†ç¾¤     | ğŸ”´ï¼ˆå¯èƒ½æ”¾ä¸ä¸‹ï¼‰     | âœ… ï¼ˆå¯é€‰ï¼‰          | ğŸ”¶ ï¼ˆå¯èƒ½éœ€è¦é…åˆFSDPï¼‰ | âœ…ï¼ˆæ¨èï¼‰                 |

------

## å››ã€LLaMA-Factory å·¥å…·ä¸­çš„å®è·µåº”ç”¨

ç›®å‰LLaMA-Factoryï¼š

- é»˜è®¤æ”¯æŒ DPï¼ˆtorchrun + DDPï¼‰ï¼›
- æ”¯æŒ FSDPï¼ˆPyTorchåŸç”Ÿï¼‰ï¼›
- æ”¯æŒ ZeROï¼ˆdeepspeedï¼‰ï¼›
- æš‚ä¸æ”¯æŒ TPã€PPï¼ˆåç»­æ¡†æ¶å¯èƒ½æ”¯æŒï¼‰ã€‚

å¿«é€Ÿè°ƒç”¨ç¤ºä¾‹ï¼š

```
# DPæ¨¡å¼è°ƒç”¨ï¼š
torchrun --nproc-per-node 4 llamafactory-cli train config.json

# åŸç”ŸFSDPæ¨¡å¼è°ƒç”¨ï¼š
torchrun --nproc-per-node 4 llamafactory-cli train config.json --fsdp "full_shard auto_wrap"
```

çœŸæ­£TPã€PPéœ€è¦Megatron-LMã€Colossal-AIã€DeepSpeedæ‰©å±•æ¡†æ¶è¿›è¡Œã€‚

| ç»´åº¦                                 | LLaMA-Factory (åŸºäºHF Trainer)                               | DeepSpeed/Megatron-LM åŸç”Ÿè„šæœ¬                 |
| ------------------------------------ | ------------------------------------------------------------ | ---------------------------------------------- |
| ZeRO (1/2/3) æ•°æ®å¹¶è¡Œä¼˜åŒ–æŠ€æœ¯        | âœ” æ”¯æŒ                                                       | âœ” æ”¯æŒ                                         |
| ZeRO-Infinity (NVMe/CPU Offload)     | â—‘ æ”¯æŒ(ç†è®ºâˆš,å®é™…ç»éªŒå°‘ï¼‰                                    | âœ” è¾ƒæˆç†Ÿã€æœ‰å®è·µæ¡ˆä¾‹                           |
| Pipeline Parallel(PP) æµæ°´çº¿å¹¶è¡Œ     | âœ˜                                                            | âœ” æ·±åº¦åŸç”Ÿé›†æˆ                                 |
| Tensor Parallel(TP) å¼ é‡å¹¶è¡Œ         | âœ˜                                                            | âœ” æ·±åº¦åŸç”Ÿé›†æˆ                                 |
| Mixture-of-Experts(MoE) ä¸“å®¶æ··åˆæ¶æ„ | âœ˜                                                            | âœ” æ·±åº¦åŸç”Ÿæ”¯æŒ                                 |
| è‡ªåŠ¨åŒ–å¯åŠ¨å™¨ä¸å¤šæœºé…ç½®(hostfileç­‰)   | â—‘ éœ€å¤–éƒ¨torchrun / DSå¯åŠ¨æ‰‹åŠ¨é…ç½®                            | âœ” åŸç”Ÿlauncherå¯è‡ªåŠ¨ä¼˜åŒ–                       |
| å®¹é”™ä¸æ–­ç‚¹ç»­è®­                       | â—‘ åŸºäºHFç®€å•checkpointï¼Œå®¹é”™æœ‰é™                             | âœ” Elasticæ€§å¼ºï¼Œå®¹é”™å®Œå–„                        |
| æ·±åº¦è°ƒä¼˜(prefixé˜¶æ®µé‡ç®—ã€è‡ªå®šä¹‰é€šä¿¡) | â—‘ æ·±åº¦åº•å±‚è°ƒå‚éœ€æ”¹æºç                                        | âœ” å……åˆ†æš´éœ²ï¼Œæ·±åº¦å¯å®šåˆ¶                         |
| æ˜“ç”¨æ€§ä¸åˆå­¦è€…å‹å¥½                   | âœ”âœ” ä¸€é”®CLIå¯åŠ¨ä¼˜åŒ–                                           | âœ˜ æ‰‹å†™è„šæœ¬é—¨æ§›ç›¸å¯¹è¾ƒé«˜                         |
| æ”¯æŒPromptæ¨¡æ¿(SFT,DPO,RLHFå†…ç½®)     | âœ”âœ” å†…ç½®20+å¸¸è§Promptæ¨¡æ¿                                     | âœ˜ ä¸€èˆ¬éœ€è‡ªè¡Œå®ç°promptæ¨¡æ¿                     |
| å¤šç§å¾®è°ƒæ–¹æ³•(SFT,DPO,PPO,ORPOä¸€é”®å¼) | âœ”âœ” stageé…ç½®åˆ‡æ¢å³å¯                                         | â—‘ éœ€é‡å†™æˆ–ä¿®æ”¹training loop                    |
| UI/Webç•Œé¢å‹å¥½                       | âœ”âœ” Web UIä¸CLIåŒæ”¯æŒ                                         | âœ˜ å‘½ä»¤è¡Œä¸è„šæœ¬ä¸ºä¸»                             |
| wandb/TensorBoard è§‚æµ‹å·¥å…·æ— ç¼æ”¯æŒ   | âœ”âœ” é»˜è®¤æ”¯æŒ                                                  | â—‘ éœ€é¢å¤–æ’ä»¶æˆ–è‡ªå®šä¹‰                           |
| PEFT (LoRA/QLoRA ç­‰) å¼€ç®±å³ç”¨        | âœ”âœ” å†…ç½® LoRA/QLoRAç¤ºä¾‹                                       | â—‘ é€šå¸¸éœ€æ‰‹å†™æ•´åˆå–Šcallå¾®è°ƒåº“(e.g. PEFT)        |
| vLLMæ¨ç†å¿«é€Ÿè¯„æµ‹å·¥å…·å†…ç½®             | âœ”âœ” ç°æˆscripts/vLLM_infer.pyå¯ç”¨                             | âœ˜ æ¨ç†è¯„æµ‹è„šæœ¬éœ€è¦è‡ªè¡Œæ•´åˆ                     |
| æ¨¡å‹ç”Ÿæ€å…¼å®¹                         | âœ”âœ” å®˜æ–¹æµ‹è¯•äº†ä¸Šç™¾ç§çƒ­é—¨ä¸­æ–‡/è‹±æ–‡Baseæ¨¡å‹                     | â—‘ éœ€æ‰‹åŠ¨é€‚é…ç‰¹å®šæ¨¡å‹å’Œtokenizeræ ¼å¼            |
| æ¨èä½¿ç”¨åœºæ™¯                         | å°è‡³ä¸­è§„æ¨¡ï¼ˆâ‰¤32 GPUï¼‰LoRA/QLoRAå¿«é€Ÿå¾®è°ƒã€å­¦ç”Ÿ/æ¢ç´¢è€…ã€é«˜æ•ˆåŸå‹ | ç™¾å¡çº§å¤§è§„æ¨¡è¿ç»­é¢„è®­ç»ƒã€ä¼ä¸šç”Ÿäº§ä¸Šçº¿ã€å¤æ‚å¹¶è¡Œ |



### å®‰è£…LLaMA-Factoryçš„æ–¹æ³•

```
mkdir /content/
cd /content/
rm -rf LLaMA-Factory
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e .[torch,bitsandbytes]
```



```
cd /content/LLaMA-Factory/
sed -i 's/{{name}}/Llama-3/g' data/identity.json
sed -i 's/{{author}}/LLaMA Factory/g' data/identity.json
```



```
cat train_llama3.json
{
  "stage": "sft",
  "do_train": true,
  "model_name_or_path": "unsloth/llama-3-8b-Instruct-bnb-4bit",
  "dataset": "identity,alpaca_en_demo",
  "template": "llama3",
  "finetuning_type": "lora",
  "lora_target": "all",
  "output_dir": "llama3_lora",
  "per_device_train_batch_size": 2,
  "gradient_accumulation_steps": 4,
  "lr_scheduler_type": "cosine",
  "logging_steps": 5,
  "warmup_ratio": 0.1,
  "save_steps": 1000,
  "learning_rate": 5e-05,
  "num_train_epochs": 3.0,
  "max_samples": 500,
  "max_grad_norm": 1.0,
  "loraplus_lr_ratio": 16.0,
  "fp16": true,
  "report_to": "none"
}
```

å¼€å§‹è®­ç»ƒ

```
(llamafactory) root@h100vm:/content/LLaMA-Factory# llamafactory-cli train train_llama3.json
```

è®­ç»ƒæ—¥å¿—ï¼š

```

[INFO|2025-05-15 07:22:21] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51.1k/51.1k [00:00<00:00, 60.6MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.09M/9.09M [00:00<00:00, 9.47MB/s]
special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 345/345 [00:00<00:00, 3.78MB/s]
[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:24,875 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:24,875 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:24,875 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:24,875 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json
[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:24,875 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:24,875 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2323] 2025-05-15 07:22:25,140 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.26k/1.26k [00:00<00:00, 14.1MB/s]
[INFO|configuration_utils.py:693] 2025-05-15 07:22:26,252 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json
[INFO|configuration_utils.py:765] 2025-05-15 07:22:26,253 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pad_token_id": 128255,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:26,470 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:26,470 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:26,470 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:26,470 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json
[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:26,470 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-05-15 07:22:26,470 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2323] 2025-05-15 07:22:26,719 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-05-15 07:22:26] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.
[WARNING|2025-05-15 07:22:26] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.
[INFO|2025-05-15 07:22:26] llamafactory.data.loader:143 >> Loading dataset identity.json...
Generating train split: 91 examples [00:00, 16289.60 examples/s]
Converting format of dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:00<00:00, 16649.87 examples/s]
[INFO|2025-05-15 07:22:27] llamafactory.data.loader:143 >> Loading dataset alpaca_en_demo.json...
Generating train split: 1000 examples [00:00, 115285.14 examples/s]
Converting format of dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 29695.03 examples/s]
Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 591/591 [00:00<00:00, 3165.15 examples/s]
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]
labels:
Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>
[INFO|configuration_utils.py:693] 2025-05-15 07:22:28,237 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json
[INFO|configuration_utils.py:765] 2025-05-15 07:22:28,237 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pad_token_id": 128255,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|2025-05-15 07:22:28] llamafactory.model.model_utils.quantization:143 >> Loading ?-bit BITSANDBYTES-quantized model.
[INFO|2025-05-15 07:22:28] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|quantization_config.py:436] 2025-05-15 07:22:28,477 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.70G/5.70G [03:42<00:00, 25.6MB/s]
[INFO|modeling_utils.py:1124] 2025-05-15 07:26:15,360 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors
[INFO|modeling_utils.py:2167] 2025-05-15 07:26:15,361 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:1142] 2025-05-15 07:26:15,362 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "pad_token_id": 128255,
  "use_cache": false
}

[INFO|modeling_utils.py:4930] 2025-05-15 07:26:18,224 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4938] 2025-05-15 07:26:18,224 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:00<00:00, 2.82MB/s]
[INFO|configuration_utils.py:1097] 2025-05-15 07:26:18,654 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json
[INFO|configuration_utils.py:1142] 2025-05-15 07:26:18,654 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128009
  ],
  "max_length": 8192,
  "pad_token_id": 128255,
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|2025-05-15 07:26:18] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-05-15 07:26:18] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-05-15 07:26:18] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-05-15 07:26:18] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-05-15 07:26:18] llamafactory.model.model_utils.misc:143 >> Found linear modules: o_proj,down_proj,up_proj,v_proj,q_proj,gate_proj,k_proj
[INFO|2025-05-15 07:26:19] llamafactory.model.loader:143 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
[INFO|trainer.py:748] 2025-05-15 07:26:19,095 >> Using auto half precision backend
[INFO|2025-05-15 07:26:19] llamafactory.train.trainer_utils:143 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.
[INFO|trainer.py:2414] 2025-05-15 07:26:19,308 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-05-15 07:26:19,308 >>   Num examples = 591
[INFO|trainer.py:2416] 2025-05-15 07:26:19,308 >>   Num Epochs = 3
[INFO|trainer.py:2417] 2025-05-15 07:26:19,308 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2420] 2025-05-15 07:26:19,308 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:2421] 2025-05-15 07:26:19,308 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-05-15 07:26:19,308 >>   Total optimization steps = 222
[INFO|trainer.py:2423] 2025-05-15 07:26:19,311 >>   Number of trainable parameters = 20,971,520
{'loss': 1.266, 'grad_norm': 0.8577702641487122, 'learning_rate': 8.695652173913044e-06, 'epoch': 0.07}                                                    
{'loss': 1.2327, 'grad_norm': 0.44337940216064453, 'learning_rate': 1.956521739130435e-05, 'epoch': 0.14}                                                  
{'loss': 1.207, 'grad_norm': 0.6676920652389526, 'learning_rate': 3.0434782608695656e-05, 'epoch': 0.2}                                                    
{'loss': 1.2355, 'grad_norm': 1.0849775075912476, 'learning_rate': 4.130434782608696e-05, 'epoch': 0.27}                                                   
{'loss': 0.9941, 'grad_norm': 0.6350672245025635, 'learning_rate': 4.999688473794144e-05, 'epoch': 0.34}                                                   
{'loss': 1.0698, 'grad_norm': 0.7085476517677307, 'learning_rate': 4.9887932065027656e-05, 'epoch': 0.41}                                                  
{'loss': 1.1396, 'grad_norm': 0.802344560623169, 'learning_rate': 4.962399180850277e-05, 'epoch': 0.47}                                                    
{'loss': 1.0549, 'grad_norm': 0.5175556540489197, 'learning_rate': 4.920670763496268e-05, 'epoch': 0.54}                                                   
{'loss': 0.9872, 'grad_norm': 0.4580995440483093, 'learning_rate': 4.863867814784168e-05, 'epoch': 0.61}                                                   
{'loss': 1.0754, 'grad_norm': 1.2809611558914185, 'learning_rate': 4.792344070481972e-05, 'epoch': 0.68}                                                   
{'loss': 1.0266, 'grad_norm': 0.8734893202781677, 'learning_rate': 4.706544938921368e-05, 'epoch': 0.74}                                                   
{'loss': 1.0538, 'grad_norm': 0.7838897109031677, 'learning_rate': 4.6070047272533765e-05, 'epoch': 0.81}                                                  
{'loss': 1.1168, 'grad_norm': 0.5366086363792419, 'learning_rate': 4.4943433140937986e-05, 'epoch': 0.88}                                                  
{'loss': 1.0687, 'grad_norm': 0.7647867798805237, 'learning_rate': 4.369262289279273e-05, 'epoch': 0.95}                                                   
{'loss': 0.9302, 'grad_norm': 1.7908470630645752, 'learning_rate': 4.2325405847733294e-05, 'epoch': 1.01}                                                  
{'loss': 0.7998, 'grad_norm': 1.6611608266830444, 'learning_rate': 4.085029623930597e-05, 'epoch': 1.08}                                                   
{'loss': 0.6749, 'grad_norm': 0.9149978160858154, 'learning_rate': 3.927648019326737e-05, 'epoch': 1.15}                                                   
{'loss': 0.7292, 'grad_norm': 0.6326663494110107, 'learning_rate': 3.7613758521729436e-05, 'epoch': 1.22}                                                  
{'loss': 0.6618, 'grad_norm': 0.4195830523967743, 'learning_rate': 3.587248568939483e-05, 'epoch': 1.28}                                                   
{'loss': 0.7913, 'grad_norm': 0.6591249704360962, 'learning_rate': 3.406350533196562e-05, 'epoch': 1.35}                                                   
{'loss': 0.7461, 'grad_norm': 0.7141778469085693, 'learning_rate': 3.219808272827917e-05, 'epoch': 1.42}                                                   
{'loss': 0.7119, 'grad_norm': 0.8500687479972839, 'learning_rate': 3.0287834646695477e-05, 'epoch': 1.49}                                                  
{'loss': 0.6357, 'grad_norm': 0.7155753374099731, 'learning_rate': 2.834465700261198e-05, 'epoch': 1.55}                                                   
{'loss': 0.703, 'grad_norm': 0.7760538458824158, 'learning_rate': 2.6380650777612705e-05, 'epoch': 1.62}                                                   
{'loss': 0.6655, 'grad_norm': 0.7388576865196228, 'learning_rate': 2.4408046661584408e-05, 'epoch': 1.69}                                                  
{'loss': 0.7347, 'grad_norm': 0.9201497435569763, 'learning_rate': 2.2439128887084673e-05, 'epoch': 1.76}                                                  
{'loss': 0.7333, 'grad_norm': 1.214197039604187, 'learning_rate': 2.0486158730277454e-05, 'epoch': 1.82}                                                   
{'loss': 0.7217, 'grad_norm': 0.5917288064956665, 'learning_rate': 1.856129815482759e-05, 'epoch': 1.89}                                                   
{'loss': 0.6586, 'grad_norm': 1.7250254154205322, 'learning_rate': 1.667653407425598e-05, 'epoch': 1.96}                                                   
{'loss': 0.556, 'grad_norm': 0.8077885508537292, 'learning_rate': 1.4843603704405279e-05, 'epoch': 2.03}                                                   
{'loss': 0.4381, 'grad_norm': 0.9334748387336731, 'learning_rate': 1.307392147087777e-05, 'epoch': 2.09}                                                   
{'loss': 0.4025, 'grad_norm': 1.1001027822494507, 'learning_rate': 1.1378507926623247e-05, 'epoch': 2.16}                                                  
{'loss': 0.386, 'grad_norm': 0.737207293510437, 'learning_rate': 9.76792112233709e-06, 'epoch': 2.23}                                                      
{'loss': 0.4355, 'grad_norm': 0.9195957183837891, 'learning_rate': 8.252190857053626e-06, 'epoch': 2.3}                                                    
{'loss': 0.4407, 'grad_norm': 0.6842745542526245, 'learning_rate': 6.840756218384023e-06, 'epoch': 2.36}                                                   
{'loss': 0.4001, 'grad_norm': 1.3696553707122803, 'learning_rate': 5.542406801361758e-06, 'epoch': 2.43}                                                   
{'loss': 0.4258, 'grad_norm': 0.599327027797699, 'learning_rate': 4.3652279719506e-06, 'epoch': 2.5}                                                       
{'loss': 0.5361, 'grad_norm': 1.1350016593933105, 'learning_rate': 3.316550516082137e-06, 'epoch': 2.57}                                                   
{'loss': 0.357, 'grad_norm': 0.8104240894317627, 'learning_rate': 2.402904987779414e-06, 'epoch': 2.64}                                                    
{'loss': 0.2796, 'grad_norm': 0.9202777743339539, 'learning_rate': 1.6299810406600419e-06, 'epoch': 2.7}                                                   
{'loss': 0.3755, 'grad_norm': 1.0832188129425049, 'learning_rate': 1.0025919960785724e-06, 'epoch': 2.77}                                                  
{'loss': 0.4879, 'grad_norm': 0.9248091578483582, 'learning_rate': 5.246448685571365e-07, 'epoch': 2.84}                                                   
{'loss': 0.4777, 'grad_norm': 0.7324397563934326, 'learning_rate': 1.9911603516855338e-07, 'epoch': 2.91}                                                  
{'loss': 0.4072, 'grad_norm': 0.7875744700431824, 'learning_rate': 2.8032700388910814e-08, 'epoch': 2.97}                                                  
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 222/222 [04:05<00:00,  1.06s/it][INFO|trainer.py:3984] 2025-05-15 07:30:24,686 >> Saving model checkpoint to llama3_lora/checkpoint-222
[INFO|configuration_utils.py:693] 2025-05-15 07:30:25,167 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json
[INFO|configuration_utils.py:765] 2025-05-15 07:30:25,168 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pad_token_id": 128255,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2510] 2025-05-15 07:30:25,280 >> tokenizer config file saved in llama3_lora/checkpoint-222/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-05-15 07:30:25,280 >> Special tokens file saved in llama3_lora/checkpoint-222/special_tokens_map.json
[INFO|trainer.py:2681] 2025-05-15 07:30:25,556 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 246.2454, 'train_samples_per_second': 7.2, 'train_steps_per_second': 0.902, 'train_loss': 0.742836398852838, 'epoch': 3.0}               
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 222/222 [04:06<00:00,  1.11s/it]
[INFO|trainer.py:3984] 2025-05-15 07:30:25,557 >> Saving model checkpoint to llama3_lora
[INFO|configuration_utils.py:693] 2025-05-15 07:30:26,017 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json
[INFO|configuration_utils.py:765] 2025-05-15 07:30:26,018 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pad_token_id": 128255,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2510] 2025-05-15 07:30:26,121 >> tokenizer config file saved in llama3_lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-05-15 07:30:26,121 >> Special tokens file saved in llama3_lora/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  total_flos               = 16222836GF
  train_loss               =     0.7428
  train_runtime            = 0:04:06.24
  train_samples_per_second =        7.2
  train_steps_per_second   =      0.902
[INFO|modelcard.py:450] 2025-05-15 07:30:26,218 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
```

æ¨ç†éªŒè¯

(llamafactory) root@h100vm:/content/LLaMA-Factory# cat chat_llama3.json 

```
{
  "model_name_or_path": "unsloth/llama-3-8b-Instruct-bnb-4bit",
  "adapter_name_or_path": "/content/LLaMA-Factory/llama3_lora",
  "template": "llama3",
  "finetuning_type": "lora"
}
(llamafactory) root@h100vm:/content/LLaMA-Factory# 
```



(llamafactory) root@h100vm:/content/LLaMA-Factory# llamafactory-cli chat chat_llama3.json

```

[INFO|2025-05-15 07:55:55] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-05-15 07:55:55] llamafactory.model.adapter:143 >> Loaded adapter(s): /content/LLaMA-Factory/llama3_lora
[INFO|2025-05-15 07:55:55] llamafactory.model.loader:143 >> all params: 8,051,232,768
Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.

User: ä½ èƒ½è§£ç­”çš„æœ€éš¾çš„æ•°å­¦é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
Assistant: ä½œä¸º Llama-3ï¼Œæˆ‘èƒ½è§£ç­”çš„æœ€éš¾çš„æ•°å­¦é¢˜å¯èƒ½ä¼šå› äººè€Œå¼‚ï¼Œä½†æ˜¯åœ¨æˆ‘çš„çŸ¥è¯†èŒƒå›´å†…ï¼Œæˆ‘å¯ä»¥å°è¯•è§£ç­”ä¸€äº›è‘—åçš„éš¾é¢˜ã€‚ä¾‹å¦‚ï¼ŒRiemannçŒœæƒ³ã€ Navier-Stokesæ–¹ç¨‹çš„è§£å†³æ–¹æ¡ˆã€PoincarÃ©çŒœæƒ³ç­‰ã€‚è¿™äº›é—®é¢˜éƒ½æ˜¯æ•°å­¦ç•Œçš„æŒ‘æˆ˜ç›®æ ‡ä¹‹ä¸€ï¼Œæˆ‘å¯ä»¥å°è¯•è§£é‡Šå’Œè§£å†³å®ƒä»¬ã€‚

User: å“¥å¾·å·´èµ«çŒœæƒ³å¯ä»¥ä¹ˆ
Assistant: å“¥å¾·å·´èµ«çŒœæƒ³æ˜¯æŒ‡ä»»æ„ä¸€ä¸ªå¤§äº 2 çš„æ•´æ•°éƒ½å¯ä»¥è¢«åˆ†è§£ä¸ºä¸‰ä¸ªæˆ–ä¸‰ä¸ªä»¥ä¸Šçš„ç´ æ•°çš„ç§¯ã€‚è¿™ä¸ªçŒœæƒ³è¢«è¯æ˜äº†å¯¹ä¸€äº›ç‰¹å®šçš„èŒƒå›´å†…çš„æ•´æ•°æ˜¯æ­£ç¡®çš„ï¼Œä½†æ˜¯å¯¹äºæ‰€æœ‰çš„æ•´æ•°æ˜¯å¦æ­£ç¡®ä»ç„¶æ˜¯æœªè§£å†³çš„é—®é¢˜ã€‚ä½œä¸º Llama-3ï¼Œæˆ‘ä¸èƒ½ç›´æ¥è¯æ˜å“¥å¾·å·´èµ«çŒœæƒ³ï¼Œä½†æ˜¯å¯ä»¥è§£é‡Šå’Œæä¾›ç›¸å…³çš„ä¿¡æ¯ã€‚

```

### ä½¿ç”¨UIè¿›è¡Œå¾®è°ƒ

```
cd /content/LLaMA-Factory/
GRADIO_SHARE=1 llamafactory-cli webui
```

æ¥ä¸‹æ¥çš„æ“ä½œï¼Œå°±å¯ä»¥é€šè¿‡æµè§ˆå™¨å®ç°ï¼Œç¡®å®ç”Ÿæ€ååˆ†å¼ºå¤§ã€‚

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/LLaMA-Factory-Usage/images/1.png)

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/LLaMA-Factory-Usage/images/2.png)

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/LLaMA-Factory-Usage/images/3.png)

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/LLaMA-Factory-Usage/images/4.png)

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/LLaMA-Factory-Usage/images/5.png)

![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/LLaMA-Factory-Usage/images/6.png)

