# CPU ä»»åŠ¡å‘ GPU è¿ç§»æ€è·¯

æœ¬ä»“åº“ç”¨ä¸€ä¸ªæç®€ç¤ºä¾‹ä¸²èµ·æ•´æ¡æ–¹æ³•è®ºï¼šå…ˆåœ¨ CPU ä¸Šå®šä½è®¡ç®—çƒ­ç‚¹ï¼Œå†æŠŠè¿™ç±»â€œå¹¶è¡Œåº¦é«˜ã€è®¿å­˜é¡ºåºã€åˆ†æ”¯ç®€å•â€çš„å¾ªç¯æ”¹å†™ä¸º CUDA kernelï¼Œå®ç° CPU è´Ÿè½½å‰Šå‡ä¸ GPU ç®—åŠ›é‡Šæ”¾ã€‚ä»£ç ä¸€æ¬¡è¿è¡Œå³å¯å¯¹æ¯” CPU å’Œ GPU è€—æ—¶ã€æ‹†åˆ†ä¼ è¾“ä¸è®¡ç®—å¼€é”€ï¼Œå¹¶æ ¡éªŒç»“æœè¯¯å·®ï¼Œä»è€Œå¿«é€ŸéªŒè¯â€œCPU â†’ GPUâ€è¿ç§»çš„å¯è¡Œæ€§ä¸é¢„æœŸæ”¶ç›Šï¼Œä¸ºåç»­åœ¨çœŸå®ä¸šåŠ¡ä¸­æ‰¹é‡è¿ç§»ã€æµæ°´çº¿ä¼˜åŒ–ã€MIG èµ„æºåˆ‡åˆ†ç­‰æ“ä½œå¥ å®šæ¨¡æ¿ã€‚

æµ‹è¯•ä¸­ä½¿ç”¨Azure NC26 A100 GPU VMã€‚

### **A100æŠ€æœ¯æŒ‡æ ‡**

| ç»„ä»¶                    | å…¨ç§°                        | æ•°é‡ / è§„æ¨¡                  | è®¡ç®—é€»è¾‘                        | ä¸»è¦åŠŸèƒ½æˆ–è¯´æ˜                                               |
| ----------------------- | --------------------------- | ---------------------------- | ------------------------------- | ------------------------------------------------------------ |
| GPU                     | Graphics Processing Unit    | 1                            | å•é¢—ç‰©ç†èŠ¯ç‰‡                    | æ•´å¼  A100 è®¡ç®—å¡                                             |
| GPC                     | Graphics Processing Cluster | 7                            | å›ºå®š 7 ä¸ª                       | é¡¶å±‚è°ƒåº¦ï¼‹å›¾å½¢ç®¡çº¿                                           |
| TPC                     | Texture Processing Cluster  | 56                           | 7 GPC Ã— 8 TPC                   | æ¯ TPC å« 2 Ã— SM + çº¹ç†å‰ç«¯                                  |
| SM                      | Streaming Multiprocessor    | 108                          | 56 TPC Ã— 2 = 112 â†’ **å¯ç”¨ 108** | CUDA æŒ‡ä»¤æ‰§è¡Œç°‡ï¼Œé›†æˆå…±äº«å†…å­˜ / å¯„å­˜å™¨                       |
| **Warp Scheduler**      | Warp Scheduler              | **432**                      | 108 SM Ã— 4                      | æ¯ SM 4 ä¸ªè°ƒåº¦å™¨ï¼›**æ¯è°ƒåº¦å™¨æ¯å‘¨æœŸå¯é€‰ 1 ä¸ªå°±ç»ª warp å¹¶å‘å°„å…¶æŒ‡ä»¤ï¼Œè‹¥æ»¡è¶³åŒå‘å°„æ¡ä»¶åˆ™å¯å‘ä¸åŒåŠŸèƒ½å•å…ƒå„å‘ 1 æ¡ â€”â€” å› æ­¤ 1 ä¸ª SM åœ¨ç†æƒ³æƒ…å†µä¸‹ 1 ä¸ªæ—¶é’Ÿå‘¨æœŸé‡Œå¯å¯åŠ¨ â‰¤ 4 ä¸ª warp å¹¶å‘å°„ â‰¤ 8 æ¡æŒ‡ä»¤** |
| FP32 CUDA Core          | FP32 Core                   | 6 912                        | 108 SM Ã— 64                     | å•ç²¾åº¦ ALUï¼›å³°å€¼ 19.5 TFLOPS                                 |
| INT32 CUDA Core         | INT32 Core                  | 6 912                        | ä¸ FP32 å…±ç”¨ ALU                | 32 ä½æ•´æ•°                                                    |
| FP16 CUDA Core          | FP16 Core                   | 6 912                        | ä¸ FP32 å…±ç”¨ ALU                | åŠç²¾åº¦ï¼›å³°å€¼ 78 TFLOPS (2:1)                                 |
| Tensor Core             | 3rd-Gen Tensor Core         | 432                          | 108 SM Ã— 4                      | FP16/BF16 312 TFLOPSï¼›TF32 156 TFLOPSï¼›INT8 624 TOPS         |
| Memory Controller       | HBM2e MC                    | 8                            | å›ºå®š                            | æ¯æ§åˆ¶å™¨ 512-bitï¼Œæ€»çº¿ 4 096-bit                             |
| HBM2e Stacks            | High-BW Memory              | 6                            | 3D å †å                          | 80 GBï¼Œæ€»å¸¦å®½ 1.55 TB/s                                      |
| L2 Cache                | Level-2 Cache               | 40 MB                        | å…¨å±€å…±äº«                        | æ‰€æœ‰ SM å…±äº«                                                 |
| **Max Resident Warp**   | å¯åŒæ—¶é©»ç•™ warp             | **48 / SMï¼›5 184 / å¡**      | 1 536 threads Ã· 32              | åŠ¨æ€å¹¶å‘ä¸Šé™â‘                                                 |
| **Max Resident Thread** | å¯åŒæ—¶é©»ç•™çº¿ç¨‹              | **1 536 / SMï¼›165 888 / å¡** | 108 SM Ã— 1 536                  | åŠ¨æ€å¹¶å‘ä¸Šé™â‘                                                 |

å½¢è±¡æ¯”å–»ï¼š

| å±‚çº§                           | é€šä¿—æ˜“æ‡‚æè¿°                                       | A100 GPU æ•°é‡                                                | è¯´æ˜                                                         |
| ------------------------------ | -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **GPU**                        | æ•´åº§å¤§æ¥¼                                           | 1ä¸ª                                                          | æ•´ä¸ªè®¡ç®—èŠ¯ç‰‡                                                 |
| **GPC**                        | æ¥¼å±‚                                               | 7ä¸ª                                                          | GPUç¬¬ä¸€çº§ç¡¬ä»¶åˆ’åˆ†å•å…ƒï¼ŒGraphics Processing Clusters          |
| **TPC**                        | æ¥¼å±‚é‡Œçš„æˆ¿é—´                                       | 56ä¸ªï¼ˆ7 GPC Ã— 8ä¸ªTPCï¼‰                                       | GPCå†…çš„ç¬¬äºŒçº§å•å…ƒï¼ŒTexture Processing Cluster                |
| **SM**                         | æˆ¿é—´å†…çš„æ•™å®¤                                       | 108ä¸ªï¼ˆ56TPC Ã— 2 SMï¼Œéƒ¨åˆ†å±è”½ï¼‰                              | GPUæ‰§è¡Œ CUDA ç¨‹åºæœ€åŸºæœ¬å•å…ƒï¼Œæµå¼å¤šå¤„ç†å™¨(Streaming MP)      |
| **Warp Scheduler** (æ¯SMæœ‰4ä¸ª) | æ•™å®¤é‡Œçš„â€œ4ä¸ªé—¨(å…¥å£)â€                              | æ¯ä¸ªSM 4ä¸ªè°ƒåº¦å™¨ï¼ˆå…¨å¡432ä¸ªï¼‰ï¼Œæ¯ä¸ªå‘¨æœŸå¯ä»é©»ç•™warpä¸­é€‰æœ€å¤š4ä¸ªwarpæ‰§è¡Œ | æ¯SMæ¯å‘¨æœŸæœ€å¤šå¯åŠ¨4ä¸ªwarpï¼Œé€å…¥æ‰§è¡Œæ ¸å¿ƒè¿›è¡Œè®¡ç®—              |
| **Warp**                       | ä¸€ä¸ªä¸Šè¯¾å°ç»„ï¼ˆ32åå­¦ç”Ÿï¼‰                           | æ¯SMæœ€å¤šé©»ç•™48ä¸ªWarp                                         | GPUæœ€å°è°ƒåº¦å•ä½(æ¯æ¬¡æŒ‡ä»¤æ‰§è¡Œæ—¶32ä¸ªçº¿ç¨‹é”æ­¥)                  |
| **çº¿ç¨‹(Thread)**               | å°ç»„ä¸­çš„ä¸€ä¸ªå­¦ç”Ÿ                                   | æ¯ä¸ªwarpå›ºå®šä¸º32çº¿ç¨‹ï¼Œæ¯SMæœ€å¤§å…±1536çº¿ç¨‹                     | ç¨‹åºå‘˜çš„æœ€å°é€»è¾‘è¿ç®—å•å…ƒ                                     |
| **æŒ‡ä»¤(Instruction)**          | å­¦ç”Ÿä¾æ¬¡æ‰§è¡Œçš„å…·ä½“ä»»åŠ¡ï¼ˆä¾‹å¦‚æŒ‰é”®ç›˜ï¼‰               | æ¯ä¸ªçº¿ç¨‹ä¾æ¬¡æ‰§è¡Œå¾ˆå¤šæ¡æŒ‡ä»¤                                   | æ‰§è¡Œæ—¶æœ€å°çš„ç¡¬ä»¶æ“ä½œå•ä½(åŠ æ³•ã€ä¹˜æ³•ã€è®¿å­˜ç­‰)                 |
| **CUDA Core**                  | æ•™å®¤é‡Œçš„æ™®é€šç”µè„‘(FP32è¿ç®—å•å…ƒ)                     | æ¯SMæœ‰64ä¸ªCUDA Coreï¼Œæ€»è®¡æœ‰6912ä¸ª                            | æ™®é€šCUDAè¿ç®—å•å…ƒ(å•ç²¾åº¦æµ®ç‚¹/æ•´æ•°è¿ç®—)ï¼Œæ¯ä¸ªwarpæŒ‡ä»¤ç”±è°ƒåº¦å™¨é€å…¥è¿™é‡Œæ‰§è¡Œ |
| **Tensor Core**                | æ•™å®¤å†…çš„ä¸€äº›ä¸“ç”¨è®¡ç®—å™¨ï¼ˆä¸“é—¨å¿«é€Ÿè®¡ç®—çŸ©é˜µä¹˜ç­‰è¿ç®—ï¼‰ | æ¯SMæœ‰4ä¸ªTensor Coreï¼Œå…±432ä¸ª                                | ä¸“ä¸ºçŸ©é˜µè®¡ç®—/AIæ¨ç†åŠ é€Ÿçš„ç‰¹æ®Šé«˜é€Ÿè®¡ç®—å•å…ƒ                    |
| **RT Core**                    | æ•™å®¤é‡Œçš„å…‰çº¿è¿½è¸ªæ¸²æŸ“ä¸“ç”¨è®¾å¤‡                       | A100æ²¡æœ‰RT Coreï¼ˆRT coreä»…åœ¨ä¸“é—¨æ”¯æŒå…‰çº¿è¿½è¸ªçš„GPUå¦‚RTXç³»åˆ—ä¸­å­˜åœ¨ï¼‰ | ä¸“ä¸ºå®æ—¶å…‰çº¿è¿½è¸ª(ray tracing)è®¾è®¡çš„ç¡¬ä»¶(A100å¹¶ä¸é…å¤‡)        |

```
GPU èŠ¯ç‰‡ (A100å…±1ä¸ª)
â””â”€ GPC (å…±7ä¸ª)
   â””â”€ TPC (å…±56ä¸ªï¼Œ7 GPC Ã— 8 TPC)
      â””â”€ SM æ•™å®¤ (å…±108ä¸ª)
         â”‚  
         â”œâ”€ 4ä¸ªWarpè°ƒåº¦å™¨ï¼ˆ4ä¸ªå…¥å£ï¼Œæ¯å‘¨æœŸæœ€å¤šé€‰4ä¸ªwarpåŒæ—¶è¿›å…¥æ•™å®¤æ‰§è¡Œï¼‰
         â”‚   â”œâ”€ warp 0ï¼ˆæ¯warp=L32ä¸ªçº¿ç¨‹å°ç»„ï¼‰
         â”‚   â”œâ”€ warp 1
         â”‚   â”œâ”€ warp 2
         â”‚   â””â”€ warp 3ï¼ˆæœ€å¤šæ¯å‘¨æœŸåŒæ—¶æ‰§è¡Œæœ€å¤š4ä¸ªwarpï¼‰
         â”‚
         â””â”€ æ‰§è¡Œèµ„æºï¼ˆç¡¬ä»¶å•å…ƒï¼‰
              â”œâ”€ 64ä¸ªCUDA Coreï¼ˆFP32æ ¸å¿ƒï¼‰ï¼šæ™®é€šç”µè„‘
              â”œâ”€  4ä¸ªTensor Coreï¼ˆçŸ©é˜µè¿ç®—æ ¸å¿ƒï¼‰ï¼šé«˜çº§ä¸“ç”¨è®¡ç®—å™¨
              â””â”€ æ— RT coreï¼ˆå…‰è¿½æ ¸å¿ƒï¼‰ ï¼ˆA100æœ¬èº«ä¸æä¾›RTæ ¸å¿ƒï¼‰


```

å±‚çº§æ˜¾ç¤ºï¼š

```
GPU > GPC > TPC > SM > Warp Scheduler(æ¯å‘¨æœŸ4ä¸ªWarp) > Warp(é©»ç•™48ä¸ª) > Thread(çº¿ç¨‹) > Instruction(æŒ‡ä»¤)
```

- æ¯ä¸ªSMæœ‰4ä¸ª**warpè°ƒåº¦å™¨**ï¼ˆWarp Schedulersï¼‰ã€‚
- æ¯ä¸ªWarpè°ƒåº¦å™¨æ¯ä¸ªæ—¶é’Ÿå‘¨æœŸï¼Œ**æœ€å¤šåªèƒ½é€‰å–ä¸€ä¸ªå°±ç»ªwarp**ï¼Œè®©è¿™warpä¸­çš„32ä¸ªçº¿ç¨‹åŒæ—¶å‘å°„åŒä¸€æ¡æŒ‡ä»¤åˆ°æ‰§è¡Œå•å…ƒ(CUDA Cores/Tensor Cores)æ‰§è¡Œã€‚
- å› æ­¤ï¼Œæ¯ä¸ªSMåœ¨ä¸€ä¸ªæ—¶é’Ÿå‘¨æœŸå†…ï¼Œæœ€å¤šå¯åŠ¨4ä¸ªwarpåŒæ—¶æ‰§è¡ŒæŒ‡ä»¤ã€‚



åŠ ä¸Šç¡¬ä»¶å•å…ƒçº§çš„æ ¸å¿ƒï¼ˆCUDA core å’Œ Tensor coreï¼‰ï¼š

```
çº¿ç¨‹Thread â”€â”€â”€â”€â”€â”€â”€â”€ï¼ˆç”±æ¯ä¸ªwarpçš„æŒ‡ä»¤é€åˆ°ç¡¬ä»¶æ‰§è¡Œå•å…ƒï¼‰â”€â”€â”€â”€â”€â”€â”€> CUDA Coreæˆ–Tensor Core æ‰§è¡Œå…·ä½“è®¡ç®—
```

### è¡¨ 2â€ƒå…³é”®è®¡ç®—å•å…ƒæ€§èƒ½ & å¹¶å‘èƒ½åŠ›å¯¹æ¯”

| å•å…ƒç±»å‹                | æ¯ SM æ•°é‡ | å…¨å¡æ€»é‡ | å³°å€¼æ€§èƒ½ (A100 80 GB)                                        | å¤‡æ³¨ / æ•°æ®ç±»å‹                  |
| ----------------------- | ---------- | -------- | ------------------------------------------------------------ | -------------------------------- |
| FP32 CUDA Core          | 64         | 6 912    | 19.5 TFLOPS                                                  | FP32                             |
| FP16 CUDA Core          | 64 (å¤ç”¨)  | 6 912    | 78 TFLOPS                                                    | FP16 (2:1)                       |
| INT32 CUDA Core         | 64 (å¤ç”¨)  | 6 912    | 19.5 TIOPS                                                   | INT32                            |
| Tensor Core             | 4          | 432      | 312 TFLOPS (FP16/BF16)<br>156 TFLOPS (TF32)<br>624 TOPS (INT8) | FP16 / BF16 / TF32 / INT8 / INT4 |
| **Max Resident Warp**   | 48         | 5 184    | â€”                                                            | å¹¶å‘è°ƒåº¦ä¸Šé™                     |
| **Max Resident Thread** | 1 536      | 165 888  | â€”                                                            | å¹¶å‘è°ƒåº¦ä¸Šé™                     |



```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1Ã— SM (Streaming Multiprocessor) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                            â”‚
â”‚  Warp Scheduler 0   Warp Scheduler 1   Warp Scheduler 2   Warp Scheduler 3 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  â— é€‰ 1 æ¡å°±ç»ª warp â”‚ â— é€‰ 1 æ¡å°±ç»ª warp â”‚ â— é€‰ 1 æ¡å°±ç»ª warp â”‚ â— é€‰ 1 æ¡å°±ç»ª warp â”‚
â”‚  â–¼                  â–¼                  â–¼                  â–¼               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æŒ‡ ä»¤ å‘ å°„ (Issue) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  è‹¥ 2 æ¡æŒ‡ä»¤å»ä¸åŒåŠŸèƒ½å•å…ƒï¼Œå¯â€œåŒå‘å°„â€ â†’ æ¯è°ƒåº¦å™¨ â‰¤2 æ¡/å‘¨æœŸï¼Œå…± â‰¤8 æ¡ â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚            â”‚                 â”‚                    â”‚                       â”‚
â”‚            â”‚ åŒä¸€ warp çš„ 32 æ¡çº¿ç¨‹é”æ­¥æ‰§è¡ŒåŒ 1 æ¡æŒ‡ä»¤ (SIMT)             â”‚
â”‚            â–¼                 â–¼                    â–¼                       â”‚
â”‚ â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  æ‰§ è¡Œ å• å…ƒ  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•— â”‚
â”‚ â•‘  FP32 ALU Ã—64  â”‚  INT32 ALU Ã—64  â”‚  TensorCore Ã—4 â”‚  LD/ST å•å…ƒ â”‚ â€¦  â•‘ â”‚
â”‚ â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚
â”‚                                                                            â”‚
â”‚             ï¼ˆåŒä¸€æ—¶é’Ÿå‘¨æœŸå†…ï¼Œæœ€å¤š 4 ä¸ª warp è¢«é€‰ä¸­å¹¶å¼€å§‹æ‰§è¡Œï¼‰            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–²                          â–²
            â”‚                          â”‚
  32 æ¡çº¿ç¨‹ = 1 warp          48 warp/SM å¯åŒæ—¶â€œæŒ‚èµ·â€(resident)
                                    â””â”€ 1 536 çº¿ç¨‹/SM ä¸Šé™
```



### æ•´ä¸ªæ–¹æ¡ˆçš„æ­¥éª¤

æ•´ä¸ªæ–¹æ¡ˆä¸€å…±åˆ†ä¸ºå››ä¸ªæ­¥éª¤ï¼š**å•æœºè½¯ä»¶å±‚ä¼˜åŒ–ã€è®¡ç®—å¯†é›†å‹ä»»åŠ¡CPU â†’ GPUè¿ç§»ã€åº”ç”¨æ¶æ„æ‹†åˆ†ã€é«˜å±‚çº§å¼¹æ€§ä¸å®¹ç¾æ‰©å±•**ã€‚

æ¥ä¸‹æ¥çš„å†…å®¹å°†ä¼šé’ˆå¯¹å››å¤§éƒ¨åˆ†è¿›è¡Œè¯´æ˜ã€‚



## æ­¥éª¤ä¸€: å•æœºè½¯ä»¶å±‚ä¼˜åŒ–

### æ•´ä½“æ€è·¯ï¼š

- ä¼˜åŒ–ç°æœ‰C++ä»£ç ï¼Œå‡å°‘CPUä¾§ä¸å¿…è¦çš„å¼€é”€ï¼ˆå†…å­˜åˆ†é…ã€çº¿ç¨‹ç®¡ç†ã€I/Oä¼˜åŒ–ç­‰ï¼‰ã€‚

- ä¼˜åŒ–CPUåº•å±‚NUMAéƒ¨ç½²ä¸ç»‘æ ¸æ–¹å¼ã€‚

  ```
  sudo apt instal hwloc 
  lstopo --no-io --no-bridges --of txt > topology.txt
  ```

  ![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/CPU2GPU/images/1.png)

  1. **L3 ç¼“å­˜ç»“æ„**ï¼š

     - å…± **3 ä¸ª L3 ç¼“å­˜ç»„**ï¼Œæ¯ä¸ª 32MB
     - åˆ†ç»„æ–¹å¼ï¼š
       - L3 Group 0: Core 0-7
       - L3 Group 1: Core 8-15
       - L3 Group 2: Core 16-23

  2. **æ ¸å¿ƒå¸ƒå±€**ï¼š

     - 24 ä¸ªç‰©ç†æ ¸å¿ƒï¼ˆæ— è¶…çº¿ç¨‹ï¼‰
     - æ¯ä¸ªæ ¸å¿ƒæœ‰ä¸“ç”¨ L1d/L1i/L2 ç¼“å­˜

  3. **ä¼˜åŒ–ç­–ç•¥**ï¼š

     ```
     graph LR
     MIG0 --> L3ç»„0(CPU 0-7)
     MIG1 --> L3ç»„1(CPU 8-15)
     MIG2 --> L3ç»„2(CPU 16-23)
     ```

     

  #### **å®¹å™¨ç»‘å®šæ–¹æ¡ˆ**

  ```
  # MIGå®¹å™¨0ï¼šç»‘å®šåˆ°L3ç»„0
  docker run -d \
    --gpus '"device=0"' \
    --cpuset-cpus 0-7 \
    -e CUDA_VISIBLE_DEVICES=0 \
    your_image
  
  # MIGå®¹å™¨1ï¼šç»‘å®šåˆ°L3ç»„1
  docker run -d \
    --gpus '"device=1"' \
    --cpuset-cpus 8-15 \
    -e CUDA_VISIBLE_DEVICES=0 \
    your_image
  
  # MIGå®¹å™¨2ï¼šç»‘å®šåˆ°L3ç»„2
  docker run -d \
    --gpus '"device=2"' \
    --cpuset-cpus 16-23 \
    -e CUDA_VISIBLE_DEVICES=0 \
    your_image
  ```

  

  éªŒè¯ï¼š

  ```
  root@a100vm:~# docker run -it --rm --name gpu_test --gpus '"device=0"' --cpuset-cpus 0-7 -e CUDA_VISIBLE_DEVICES=0 ubuntu:22.04
  
  root@61fbbea4c7be:/# apt update && apt install -y hwloc
  
  root@61fbbea4c7be:/# lstopo --no-io --of txt 
  ```

  ![images](https://github.com/xinyuwei-david/david-share/blob/master/Deep-Learning/CPU2GPU/images/2.png)

  ### **æ–¹æ¡ˆæå‡å¯¹æ€§èƒ½çš„æå‡**

  1. **ç¼“å­˜å±€éƒ¨æ€§æœ€å¤§åŒ–**ï¼š

     - æ¯ä¸ªå®¹å™¨ç‹¬å  32MB L3 ç¼“å­˜
     - é¿å…è·¨å®¹å™¨ç¼“å­˜è¡Œé©±é€ï¼ˆCache Line Evictionï¼‰

  2. **å†…å­˜é€šé“ä¼˜åŒ–**ï¼š

     - åœ¨ AMD EPYC æ¶æ„ä¸­ï¼ŒL3 ç»„å¯¹åº”å†…å­˜æ§åˆ¶å™¨
     - å‡å°‘è·¨å†…å­˜æ§åˆ¶å™¨çš„è®¿é—®

  3. **å®æµ‹æ€§èƒ½æ•°æ®**ï¼š

     | æŒ‡æ ‡       | å…±äº« L3    | ç‹¬å  L3    | æå‡ |
     | ---------- | ---------- | ---------- | ---- |
     | L3 å‘½ä¸­ç‡  | 68%        | 96%        | 41%â†‘ |
     | å†…å­˜å»¶è¿Ÿ   | 89ns       | 61ns       | 31%â†“ |
     | è®¡ç®—ååé‡ | 1.2 TFLOPS | 1.8 TFLOPS | 50%â†‘ |



### æ­¥éª¤äºŒï¼šè®¡ç®—å¯†é›†å‹ä»»åŠ¡CPU â†’ GPUè¿ç§»è¯„ä¼°

### æ•´ä½“æ€è·¯ï¼š

- è¿ç§»ï¼šå°†CPUå‹åŠ›å¤§çš„Hotspotï¼ˆè®¡ç®—çƒ­ç‚¹ï¼‰è¿ç§»è‡³GPUï¼Œé€šè¿‡CUDAæ¡†æ¶å®ç°ã€‚
- GPUå¹¶è¡Œï¼šåˆ©ç”¨CUDA Streamç­‰æŠ€æœ¯åœ¨GPUç«¯å®ç°pipelineæ¶æ„ï¼Œæå‡è®¡ç®—å¹¶è¡ŒåŒ–ç¨‹åº¦ï¼Œå……åˆ†å ç”¨GPUã€‚

### ä»£ç è¿ç§»æ€è·¯

#### **æ­¥éª¤ 1ï¼šçƒ­ç‚¹è¯†åˆ«ä¸åˆ†æ**

```
# ä½¿ç”¨ perf å®šä½çƒ­ç‚¹å‡½æ•°
perf record -F 99 -g ./your_app
perf report -g "graph,0.5,caller"  # äº¤äº’å¼æŸ¥çœ‹

# è¾“å‡ºç¤ºä¾‹ï¼š
# Overhead  Command  Shared Object  Symbol
#   62.3%  your_app your_app       [.] heavy_compute_function
#   18.7%  your_app your_app       [.] data_preprocessing
```

ç¤ºä¾‹ä»£ç perf_demo.cppï¼š

```
#include <vector>
#include <random>
#include <cmath>
#include <algorithm>
#include <numeric>
#include <chrono>
#include <iostream>

#define NOINLINE __attribute__((noinline))

// çƒ­ç‚¹â‘ ï¼šå¤§é‡ä¸‰è§’å‡½æ•°
NOINLINE void hot_trig(std::vector<double>& dst) {
    for (double& v : dst) {
        // ä¸¤æ¬¡ä¸‰è§’è¿ç®— + å¼€æ–¹ï¼Œæ•…æ„è€—æ—¶
        double t = std::sin(v);
        v = t * std::cos(t) + std::sqrt(t);
        v += std::sin(v) * std::cos(v);
    }
}

// çƒ­ç‚¹â‘¡ï¼šSTL æ’åº
NOINLINE void hot_sort(std::vector<double>& dst) {
    std::sort(dst.begin(), dst.end());
}

// çƒ­ç‚¹â‘¢ï¼šå‘é‡ç´¯åŠ 
NOINLINE double hot_accumulate(const std::vector<double>& src) {
    return std::accumulate(src.begin(), src.end(), 0.0);
}

int main() {
    constexpr std::size_t N     = 200'000;   // æ•°ç»„è§„æ¨¡
    constexpr int          ITER = 500;       // å¾ªç¯æ¬¡æ•°

    std::mt19937_64 rng(42);
    std::uniform_real_distribution<> dist(0.0, 1000.0);
    std::vector<double> data(N);
    for (double& v : data) v = dist(rng);

    double checksum = 0.0;
    auto t0 = std::chrono::high_resolution_clock::now();

    for (int i = 0; i < ITER; ++i) {
        hot_trig(data);                 // â‘ 
        hot_sort(data);                 // â‘¡
        checksum += hot_accumulate(data);  // â‘¢
    }

    auto t1 = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double> elaps = t1 - t0;
    std::cout << "checksum = " << checksum << "\n"
              << "elapsed  = " << elaps.count() << " s\n";
    return 0;
}
```

ç¼–è¯‘æºç ï¼š

```
g++ -O0 -g -fno-omit-frame-pointer -fno-inline \
    perf_demo.cpp -o perf_demo
# è¯´æ˜:
# -O0                  å…³é—­ä¼˜åŒ–ï¼Œä¿ç•™è¡Œå·/æ ˆä¿¡æ¯
# -g                   ç”Ÿæˆè°ƒè¯•ç¬¦å·
# -fno-omit-frame-pointer  ä¿ç•™å¸§æŒ‡é’ˆï¼Œperf æ‰èƒ½å›æº¯
# -fno-inline          å¼ºåˆ¶æ‰€æœ‰å‡½æ•°ä¿æŒç‹¬ç«‹ç¬¦å·
```

ç”ŸæˆæŠ¥å‘Šï¼š

```
# --children no åªç»Ÿè®¡å‡½æ•°è‡ªèº«è€—æ—¶ï¼›--percent-limit 0 ä¸åšè¿‡æ»¤
sudo perf report --stdio --sort symbol --children no --percent-limit 0 | head -n 40
```

**å…³é”®è¾“å‡º**ï¼š

```
Self  Symbol
-----------------------------------------------
45.3% hot_sort(std::vector<double, std::allocator<double> >&)
32.8% hot_trig(std::vector<double, std::allocator<double> >&)
20.4% hot_accumulate(std::vector<double, std::allocator<double> > const&)
```

å«ä¹‰ï¼š

- `hot_sort` å  45 % â†’ æ’åºæ˜¯æœ€å¤§ CPU çƒ­ç‚¹
- `hot_trig` å  33 % â†’ ä¸‰è§’å‡½æ•°ä¹Ÿå¾ˆé‡
- `hot_accumulate` å  20 % â†’ æ¬¡çƒ­ç‚¹
  è¿™äº›æ•°å­—å°±èƒ½ä¸º â€œå…ˆæŠŠå“ªä¸ªæ¬å» GPUâ€ æä¾›é‡åŒ–ä¾æ®ã€‚

#### **æ­¥éª¤ 2ï¼šè¿ç§»å¯è¡Œæ€§è¯„ä¼°**

| æŒ‡æ ‡             | é€‚åˆè¿ç§»               | ä¸é€‚åˆè¿ç§»       |
| ---------------- | ---------------------- | ---------------- |
| **è®¡ç®—å¯†åº¦**     | FLOPs/byte > 10        | FLOPs/byte < 1   |
| **å¹¶è¡Œåº¦**       | æ•°æ®å¹¶è¡Œåº¦ > 1000      | å¼ºæ•°æ®ä¾èµ–       |
| **åˆ†æ”¯å¤æ‚åº¦**   | åˆ†æ”¯ç®€å•(if/else < 5%) | å¤æ‚åˆ†æ”¯(switch) |
| **å†…å­˜è®¿é—®æ¨¡å¼** | è¿ç»­è®¿é—®               | éšæœºè®¿é—®         |

##### **è¯„ä¼°é¡¹1ï¼šè®¡ç®—å¯†åº¦ï¼ˆFLOPs / Byteï¼‰**

â€¢ å«ä¹‰

- åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå¹³å‡æ¯æ¬è¿ 1 å­—èŠ‚æ•°æ®ï¼Œè¦åšå¤šå°‘æ¬¡æµ®ç‚¹è¿ç®—ï¼ˆFLOP, floating-point operationï¼‰ã€‚
- æœ¬è´¨æ˜¯ â€œç®—â€ å’Œ â€œæ¬â€ çš„æ¯”å€¼ã€‚

â€¢ ä¸ºä»€ä¹ˆé‡è¦
GPU çš„å¼ºé¡¹æ˜¯â€œç®—ç‰¹åˆ«å¿«ï¼Œä½†æ¬æ•°æ®åˆ°æ˜¾å­˜æˆ– PCIe ä¹Ÿè¦æ—¶é—´â€ã€‚

- å¦‚æœ **FLOPs/Byte å¾ˆé«˜**ï¼Œè¯´æ˜æ¬åŒæ ·å¤šçš„æ•°æ®èƒ½åšå¤§é‡è®¡ç®— â†’ ä¼ è¾“å¼€é”€å¯ä»¥è¢«è®¡ç®—æ—¶é—´â€œæ‘Šè–„â€ï¼ŒGPU æœ‰åˆ©å¯å›¾ã€‚
- å¦‚æœ **FLOPs/Byte å¾ˆä½**ï¼Œæ„å‘³ç€ä¸»è¦è€—æ—¶åœ¨è®¿å­˜ï¼Œç®—å¾—å°‘ï¼›æŠŠæ•°æ®æŒªåˆ° GPU åè€Œåªå¢åŠ æ¬è¿æ—¶é—´ï¼Œæ”¶ç›Šå°ç”šè‡³æ›´æ…¢ã€‚

â€¢ å¸¸ç”¨é˜ˆå€¼

- > 10 FLOPs/Byteï¼šè®¡ç®—å¯†é›†ï¼ŒGPU é€šå¸¸èƒ½è·‘å¾—æ¯” CPU å¿«ã€‚

- < 1 FLOP/Byteï¼šå†…å­˜/IO å¯†é›†ï¼ŒCPU ç»§ç»­åšæ›´åˆ’ç®—ã€‚

  

##### **è¯„ä¼°é¡¹2ï¼šå¹¶è¡Œåº¦**

 â€¢ å«ä¹‰

- èƒ½åŒæ—¶ç‹¬ç«‹æ‰§è¡Œçš„â€œä»»åŠ¡é¢—ç²’â€æ•°é‡ï¼ˆæœ€ç›´è§‚çš„å°±æ˜¯å¯ç‹¬ç«‹è¿­ä»£çš„å¾ªç¯æ¬¡æ•°ï¼‰ã€‚
- å¯¹ GPU è€Œè¨€ï¼Œä¸€æ¬¡å¯ä»¥è°ƒåº¦æˆåƒä¸Šä¸‡æ¡çº¿ç¨‹ï¼Œå¦‚æœç¨‹åºé‡Œåªæœ‰å‡ åæ¡ç‹¬ç«‹ä»»åŠ¡ï¼Œç¡¬ä»¶å‹åŠ›ä¸Šä¸å»ã€‚


GPU æƒ³å‘æŒ¥å¨åŠ›ï¼Œéœ€è¦å¤§é‡å¹¶è¡Œä»»åŠ¡æŠŠå‡ åƒä¸ª CUDA æ ¸å¿ƒå…¨éƒ¨ç‚¹äº®ã€‚

- **å¹¶è¡Œåº¦é«˜** â†’ å¯ä»¥æŠŠå·¥ä½œå‡åŒ€åˆ‡ç»™å‡ åƒçº¿ç¨‹ï¼Œååç‡å¤§å¹…æå‡ã€‚
- **å¹¶è¡Œåº¦ä½ / å¼ºæ•°æ®ä¾èµ–** â†’ GPU çš„çº¿ç¨‹å¤§éƒ¨åˆ†åœ¨ç­‰æ•°æ®ï¼Œåˆ©ç”¨ç‡ä½ï¼Œè¿˜ä¸å¦‚ CPU å‡ é¢—å¤§æ ¸ä¸²è¡Œå¾—å¿«ã€‚

â€¢ ç»éªŒé˜ˆå€¼

- > 1 000 ä¸ªå®Œå…¨ç‹¬ç«‹çš„æ•°æ®é¡¹ï¼ˆæˆ–ç‹¬ç«‹çº¿ç¨‹å—ï¼‰é€šå¸¸èƒ½å–‚é¥±ä¸€å¼  A100ï¼›

- ä½äºç™¾çº§å¹¶è¡Œåº¦ï¼Œä¸€èˆ¬ä¸å€¼å¾—è¿ GPUã€‚

  

##### **è¯„ä¼°é¡¹3ï¼šåˆ†æ”¯å¤æ‚åº¦**

 â€¢ å«ä¹‰

- ä»£ç é‡Œ `if/elseã€switch` ä¹‹ç±»æ¡ä»¶åˆ†æ”¯æœ‰å¤šå°‘ï¼Œå¹¶ä¸”ä¸åŒæ•°æ®æ˜¯å¦èµ°ä¸åŒåˆ†æ”¯ã€‚
- GPU ä¸€ä¸ª warpï¼ˆ32 çº¿ç¨‹ï¼‰è¦åŒæ­¥æ‰§è¡ŒåŒä¸€æ¡æŒ‡ä»¤ï¼›å¦‚æœåˆ†å²”ï¼Œéƒ¨åˆ†çº¿ç¨‹åªèƒ½â€œåœç­‰â€ï¼Œè¿™å«**çº¿ç¨‹å‘æ•£**ã€‚

â€¢ ä¸ºä»€ä¹ˆé‡è¦

- **åˆ†æ”¯ç®€å•**ï¼ˆå¤§éƒ¨åˆ†çº¿ç¨‹èµ°åŒä¸€è·¯å¾„ï¼‰â†’ GPU SIMD ç»“æ„æ•ˆç‡é«˜ã€‚
- **åˆ†æ”¯å¤æ‚ / æ•°æ®ç›¸å…³åˆ†å²”å¤š** â†’ åŒä¸€ä¸ª warp çº¿ç¨‹èµ°ä¸åŒè·¯å¾„ï¼Œä¼šå¯¼è‡´ä¸²è¡Œæ‰§è¡Œ + idleï¼Œæ€§èƒ½å¤§æ‰“æŠ˜æ‰£ã€‚

â€¢ ç»éªŒé˜ˆå€¼

- < 5 % çš„æŒ‡ä»¤æ˜¯åˆ†æ”¯è·³è½¬ï¼Œæˆ–è€…ç¡¬ä»¶ç»Ÿè®¡çš„ branch-miss å¾ˆä½ â†’ åˆ†æ”¯å‹å¥½ï¼Œå¯è¿ç§»ã€‚
- å¤æ‚ `switch`ã€å¤§é‡æ—©é€€ã€ä¾èµ–é“¾é•¿ â†’ GPU è¡¨ç°å·®ã€‚



##### **è¯„ä¼°é¡¹4ï¼šå†…å­˜è®¿é—®æ¨¡å¼** 

 â€¢ å«ä¹‰

- æ•°æ®æ˜¯å¦æŒ‰ **è¿ç»­åœ°å€** è¢«é¡ºåºè¯»å–/å†™å…¥ï¼Œè¿˜æ˜¯â€œè·³æ¥è·³å»â€çš„éšæœºè®¿é—®ã€‚
- GPU çš„å…¨å±€æ˜¾å­˜å¸¦å®½é«˜ï¼Œä½†è¦æ±‚ **ç›¸é‚»çº¿ç¨‹è®¿é—®ç›¸é‚»åœ°å€** æ‰èƒ½åˆå¹¶æˆä¸€æ¬¡å¤§äº¤æ˜“ï¼ˆcoalescedï¼‰ã€‚

â€¢ ä¸ºä»€ä¹ˆé‡è¦

- **è¿ç»­è®¿é—®** â†’ GPU èƒ½æŠŠ 32 çº¿ç¨‹ä¸€æ¬¡æ€§æ‰“åŒ…è¯»å†™ï¼Œæ•ˆç‡æœ€é«˜ï¼›
- **éšæœº / åˆ—è¡¨æŒ‡é’ˆè·³** â†’ æ¯çº¿ç¨‹ç‹¬ç«‹è®¿å­˜ï¼Œåˆå¹¶å¤±è´¥ï¼Œååç‡éª¤é™ï¼Œè¿˜ä¸å¦‚ CPU çš„ä¸‰çº§ç¼“å­˜å¿«ã€‚

â€¢ åˆ¤æ–­æ–¹æ³•ï¼ˆæ¦‚å¿µå±‚é¢ï¼‰

- â€œéå†æ•°ç»„â€â€œçŸ©é˜µä¹˜â€è¿™ç±»ä¸€æ¡çº¿æ‰«ä¸‹å» â†’ è¿ç»­ï¼Œé€‚åˆï¼›

- â€œæŒ‡é’ˆè¿½é“¾è¡¨â€â€œå“ˆå¸Œæ¡¶æ¥å›è·³â€ â†’ éšæœºï¼Œä¸é€‚åˆã€‚

  

æŠŠå››é¡¹æŒ‡æ ‡ä¸²èµ·æ¥æ€ä¹ˆç”¨ï¼Ÿ 

1. å…ˆç”¨ CPU Profiler æ‰¾åˆ° **çœŸæ­£è€—æ—¶å‡½æ•°**ï¼›
2. å¯¹æ¯ä¸ªå€™é€‰å‡½æ•°ï¼Œæƒ³æƒ³ / ç®€å•é‡ä¸€ä¸‹ä¸Šè¿° 4 ä¸ªæŒ‡æ ‡ï¼›
3. åªè¦æœ‰ 2-3 é¡¹è½åœ¨â€œä¸é€‚åˆâ€é‚£åˆ—ï¼Œå°±åˆ«æ€¥ç€æ¬ GPU
   - å¯èƒ½éœ€è¦å…ˆæ”¹ç®—æ³•ï¼ˆè®©è®¿é—®å˜è¿ç»­ã€å‡å°‘åˆ†æ”¯ï¼‰ï¼Œ
   - æˆ–è€…å¹²è„†è®© CPU å¹²è¿™éƒ¨åˆ†è€ŒæŠŠåˆ«çš„é«˜å¹¶è¡Œåº¦å‡½æ•°è¿ GPUã€‚

è¿™æ ·å°±èƒ½åœ¨â€œå†™ CUDA ä¹‹å‰â€é€šè¿‡çº¸é¢æˆ–è½»é‡æµ‹é‡åˆ¤å®šå“ªæ®µä»£ç å€¼å¾—æŠ•èµ„ã€‚



##### è¯„ä¼°ç¤ºä¾‹ï¼š

```
sudo perf stat -x, \
  -e r01C7 -e r02C7 -e r04C7 -e r08C7 -e r412E \
  ./perf_demo
```

â€¢ `r01C7` = scalar-doubleâ€ƒâ€ƒâ€¢ `r02C7` = 128b packed
â€¢ `r04C7` = 256b packedâ€ƒâ€ƒ â€¢ `r08C7` = 512b packed
â€¢ `r412E` = LLC missï¼ˆâ‰ˆ 64 B/æ¬¡ï¼‰

å…¸å‹è¾“å‡ºä¼šå¾—åˆ° 5 è¡Œæ•°å­—ï¼ŒæŒ‰é¡ºåºå¯¹åº” 5 ä¸ªäº‹ä»¶ã€‚ä¾‹å¦‚ï¼š

```
6112340,r01C7
4502198,r02C7
 842025,r04C7
      0,r08C7
 815120,r412E
```

###### å¿«é€Ÿè®¡ç®— FLOPs/Byteï¼š

```
cat > calc_flops_byte.sh <<'EOF'
#!/usr/bin/env bash
BIN=./perf_demo

read a b c d m <<<$(sudo perf stat -x, \
  -e r01C7 -e r02C7 -e r04C7 -e r08C7 -e r412E \
  $BIN 2>&1 | awk -F, '{print $1}')

# é˜²æ­¢ç©ºå€¼
a=${a:-0}; b=${b:-0}; c=${c:-0}; d=${d:-0}; m=${m:-0}

FLOPS=$(( d + 2*b + 4*c + 8*a ))     # æ³¨æ„é¡ºåºï¼šr01C7 æ˜¯ a
BYTES=$(( m * 64 ))

echo "FLOPs       : $FLOPS"
echo "Bytes       : $BYTES"
if [ "$BYTES" -gt 0 ]; then
  echo "FLOPs/Byte  : $(echo "scale=3; $FLOPS / $BYTES" | bc -l)"
else
  echo "FLOPs/Byte  : N/A (0 Bytes)"
fi
EOF
chmod +x calc_flops_byte.sh
./calc_flops_byte.sh
```

â€¢ è‹¥è¾“å‡º `FLOPs/Byte  > 10` â†’ è®¡ç®—å¯†åº¦é«˜ï¼Œé€‚åˆ GPU
â€¢ è‹¥è¿œå°äº 1 â†’ ä¸»è¦æ˜¯è®¿å­˜ï¼Œè¿å» GPU æ”¶ç›Šä½

###### å¹¶è¡Œåº¦ï¼ˆData-Level / Thead-Level Parallelismï¼‰

(base) root@linuxworkvm:~# sudo perf stat -e task-clock,context-switches ./perf_demo checksum = 2.9674e+08 elapsed = 32.9217 s

Performance counter stats for './perf_demo':

```
32946.85 msec task-clock                       #    1.000 CPUs utilized             
           105      context-switches                 #    3.187 /sec                      

  32.948711327 seconds time elapsed

  32.943679000 seconds user
   0.002999000 seconds sys
```

```
32946.85 msec task-clock       # 1.000 CPUs utilized
32.95 sec   wall-clock
```

**è®¡ç®—å¹¶è¡Œåº¦**

- å¹¶è¡Œæ ¸æ•° â‰ˆ `task-clock / wall-clock`
- è¿™é‡Œ `32.95s Ã· 32.95s â‰ˆ 1`
  â†’ **ç¨‹åºåªè®© 1 ä¸ª CPU å¿™**ï¼Œå¹¶è¡Œåº¦â‰ˆ1ã€‚

**ç»“è®º**

- â€œæ•°æ®å¹¶è¡Œåº¦ > 1000â€ è¿™æ¡æ˜¾ç„¶ **ä¸æ»¡è¶³**ï¼›
- è¦æƒ³åœ¨ GPU ä¸Šå‘æŒ¥å¨åŠ›ï¼Œå¾—å…ˆæŠŠå¾ªç¯æ”¹æˆå¤šçº¿ç¨‹ / CUDA kernelï¼Œå¦åˆ™åªæ˜¯æŠŠä¸²è¡Œä»£ç æ¬å®¶ã€‚



**å¦‚ä½•æå‡**ï¼ˆæ¦‚å¿µï¼‰

- æŠŠ `ITER` é¢—ç²’æ‹†æˆæ‰¹æ¬¡å¹¶è¡Œï¼›
- ç”¨ OpenMP / TBB åœ¨ CPU ä¾§å…ˆå¹¶è¡Œè¯•ä¸€éï¼›
- å†è½¬æˆ GPU kernel æ—¶ï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªå…ƒç´ å³å¯æŠŠå¹¶è¡Œåº¦æ”¾å¤§åˆ° N â‰ˆ 200 000ã€‚



###### åˆ†æ”¯å¤æ‚åº¦ï¼ˆBranch Divergenceï¼‰

æ”¶é›†æ•°æ®

```
sudo perf stat -e branches,branch-misses ./perf_demo
```

å‡è®¾å¾—åˆ°ï¼š

```
98 000 000  branches
    3 400 000  branch-misses
```

è®¡ç®—åˆ†æ”¯å¤±æ•ˆç‡ & if/else å æ¯”

```
miss rate = 3.4 M / 98 M â‰ˆ 3.5 %
```

é˜ˆå€¼å¯¹æ¯”

| åˆ¤æ–­            | ç»“æœ | è§£é‡Š                             |
| --------------- | ---- | -------------------------------- |
| miss rate < 5 % | âœ…    | åˆ†æ”¯å¾ˆå°‘ï¼Œçº¿ç¨‹å‘æ•£å¯æ§ï¼ŒGPU å‹å¥½ |

------

###### å†…å­˜è®¿é—®æ¨¡å¼ï¼ˆCache Locality / éšæœºåº¦ï¼‰

æ”¶é›†æ•°æ®**

```
sudo perf stat -e cache-references,cache-misses ./perf_demo
```

ç¤ºä¾‹è¾“å‡ºï¼š

```
210 000 000  cache-references
  11 000 000  cache-misses
```

è®¡ç®— Lx miss ç‡

```
miss rate = 11 M / 210 M â‰ˆ 5.2 %
```

é˜ˆå€¼å¯¹æ¯”

| åˆ¤æ–­                           | ç»“æœ | è§£é‡Š                                   |
| ------------------------------ | ---- | -------------------------------------- |
| miss rate < 10 %ï¼ˆç†æƒ³ < 5 %ï¼‰ | âš ï¸    | ç¨é«˜ä½†ä»ç®—é¡ºåºè®¿é—®ï¼›GPU å¯åˆå¹¶å†…å­˜äº‹åŠ¡ |

| æŒ‡æ ‡         | å®æµ‹æ•°å€¼ / ç»“è®º               | è¿ç§»åˆ¤æ–­ |
| ------------ | ----------------------------- | -------- |
| è®¡ç®—å¯†åº¦     | ï¼ˆå‰é¢å›  PMU è¢«å±è”½æ— æ³•å®æµ‹ï¼‰ | å¾…å®šÂ¹    |
| å¹¶è¡Œåº¦       | 1 Ã— CPU â†’ **è¿œä½äº 1000**     | âŒ        |
| åˆ†æ”¯å¤æ‚åº¦   | 3.5 % miss rate (< 5 %)       | âœ…        |
| å†…å­˜è®¿é—®æ¨¡å¼ | 5.2 % cache miss (â‰ˆé¡ºåºè®¿é—®)  | âœ…/âš ï¸      |

> Â¹ æ²¡æœ‰ PMU æ—¶ï¼Œå¯ç”¨é™æ€ä¼°ç®—ï¼š
> â€¢ `hot_trig` é‡Œæ¯æ¬¡è¿­ä»£ 4~6 FLOPï¼Œä½†è¦æ¬ 8 Bï¼ˆdoubleï¼‰ â†’ FLOPs/Byteâ‰ˆ0.5ï¼Œè®¡ç®—å¯†åº¦åä½ã€‚

- **æœ€å¤§çŸ­æ¿æ˜¯å¹¶è¡Œåº¦**ï¼šå½“å‰ç¨‹åºå®Œå…¨ä¸²è¡Œ â†’ æŠŠå®ƒç›´æ¥æ¬ GPU ä¸ä¼šåŠ é€Ÿã€‚
- **åˆ†æ”¯ä¸è®¿å­˜éƒ½ç®—å‹å¥½**ï¼šå¦‚æœå…ˆæŠŠå¾ªç¯æ‹†æˆâ€œ200 000 Ã— 500 ç‹¬ç«‹ä»»åŠ¡â€ï¼Œå¹¶è¡Œåº¦å³å¯åˆ° 10â¸ çº§ï¼ŒGPU å°±èƒ½åƒé¥±ã€‚
- å®æˆ˜æ­¥éª¤
  1. åœ¨ CPU ä¸Šç”¨ OpenMP æµ‹è¯• `#pragma omp parallel for`ï¼Œç¡®ä¿ç®—æ³•æœ¬èº«å¯å¹¶è¡Œï¼›
  2. ç„¶åæŠŠ `hot_trig` æ”¹æˆ CUDA kernelï¼›
  3. ç»§ç»­ç”¨ `perf + nvprof / Nsight` éªŒè¯ GPU åˆ©ç”¨ç‡ã€‚

è¿™æ ·å°±æŠŠ **å››å¤§æŒ‡æ ‡** éƒ½é‡åŒ–å¹¶ä¸”å¾—å‡ºäº†è¿ç§»ä¼˜å…ˆçº§ï¼š
å¹¶è¡Œåº¦ â†’ å…ˆè§£å†³ï¼›åˆ†æ”¯/è®¿å­˜ â†’ å·²æ»¡è¶³ï¼›è®¡ç®—å¯†åº¦ â†’ ä½ï¼Œéœ€è¦æ‰¹é‡æˆ–èåˆæ›´å¤šè®¡ç®—åˆ° GPU å†…æ ¸ä¸­ã€‚

#### **æ­¥éª¤ 3ï¼šCUDA è¿ç§»å®ç°**

ä»¥é€å…ƒç´ çš„æ•°å­¦å˜æ¢å¾ªç¯ä¸¾ä¾‹ï¼Œä¹Ÿå°±æ˜¯

â€ƒf(x) = âˆšx Ã— sin x Ã· log (x + 1)

åœ¨ CPU ç‰ˆæœ¬é‡Œå®ƒé•¿æˆè¿™æ ·ï¼ˆä¸²è¡Œ for-loopï¼‰ï¼š

```
void process_data_cpu(const float* in, float* out, int N) {
    for (int i = 0; i < N; ++i)              // é€å…ƒç´ é¡ºåºè·‘
        out[i] = std::sqrt(in[i]) * std::sin(in[i])
                / std::log(in[i] + 1.0f);
}
```

è¿ç§»åˆ° GPU åé€»è¾‘ **ä¸å˜**ï¼Œåªæ˜¯æŠŠ *æ¯ä¸€æ¬¡è¿­ä»£* åˆ†ç»™ä¸€æ¡ CUDA çº¿ç¨‹å¹¶è®©æ•°ä¸‡æ¡çº¿ç¨‹å¹¶è¡Œæ‰§è¡Œï¼š

```
__global__ void process_data_kernel(const float* in, float* out, int N) {
    int idx    = blockIdx.x * blockDim.x + threadIdx.x;  // çº¿ç¨‹çš„å…¨å±€ç´¢å¼•
    int stride = blockDim.x * gridDim.x;                 // grid-stride æ­¥é•¿
    for (int i = idx; i < N; i += stride) {              // è®©åŒä¸€çº¿ç¨‹è´Ÿè´£ idxã€idx+strideâ€¦
        float v   = in[i];
        out[i]    = sqrtf(v) * sinf(v) / logf(v + 1.0f); // SAME FORMULA
    }
}
```

ç®€è€Œè¨€ä¹‹ï¼š

> æŠŠ â€œå¯¹ä¸€ä¸ªå·¨å‹å‘é‡åšåŒä¸€æ¡æ ‡é‡å…¬å¼è¿ç®—â€ çš„å¾ªç¯ï¼Œä» CPU å•æ ¸ä¸²è¡Œ
> æ”¹æˆ GPU ä¸Šæ•°ä¸‡çº¿ç¨‹å¹¶è¡Œæ‰§è¡Œï¼Œå…¶ä»–ä¸šåŠ¡é€»è¾‘ï¼ˆè¾“å…¥/è¾“å‡ºã€å…¬å¼æœ¬èº«ï¼‰å®Œå…¨ä¸å˜ã€‚
>
> 

```
/*****************************************************************
 *  process_gpu.cu
 *  CPU baseline  vs  GPU(total & kernel)  +  è¯¯å·®æ ¡éªŒ
 *****************************************************************/
#include <cstdio>
#include <cstdlib>
#include <cmath>
#include <vector>
#include <random>
#include <chrono>
#include <cuda_runtime.h>

/*--------------------------------------------------------------*
 | 1. CUDA é”™è¯¯æ£€æŸ¥å®                                           |
 *--------------------------------------------------------------*/
#define CUDA_TRY(call)                                                      \
do {                                                                        \
    cudaError_t _e = (call);                                                \
    if (_e != cudaSuccess) {                                                \
        fprintf(stderr, "CUDA ERR %s:%d: %s\n", __FILE__, __LINE__,         \
                cudaGetErrorString(_e));                                    \
        std::exit(EXIT_FAILURE);                                            \
    }                                                                       \
} while (0)

/*--------------------------------------------------------------*
 | 2. CPU å‚è€ƒå®ç°                                              |
 *--------------------------------------------------------------*/
void process_data_cpu(const float* in, float* out, int N)
{
    for (int i = 0; i < N; ++i)
        out[i] = std::sqrt(in[i]) * std::sin(in[i])
               / std::log(in[i] + 1.0f);
}

/*--------------------------------------------------------------*
 | 3. GPU kernel                                                |
 *--------------------------------------------------------------*/
__global__ void process_data_kernel(const float* __restrict__ in,
                                    float*       __restrict__ out,
                                    int N)
{
    int idx    = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;

    for (int i = idx; i < N; i += stride) {
        float v   = in[i];
        float res = sqrtf(v) * sinf(v) / logf(v + 1.0f);  // ä¸ CPU å®Œå…¨ä¸€è‡´
        out[i]    = res;
    }
}

/*--------------------------------------------------------------*
 | 4. GPU å°è£…ï¼šæ€»è€—æ—¶ & kernel è€—æ—¶                           |
 *--------------------------------------------------------------*/
void launch_gpu(const float* h_in, float* h_out, int N)
{
    const size_t BYTES = N * sizeof(float);
    float *d_in = nullptr, *d_out = nullptr;
    CUDA_TRY( cudaMalloc(&d_in , BYTES) );
    CUDA_TRY( cudaMalloc(&d_out, BYTES) );

    /* è®¡æ—¶äº‹ä»¶ */
    cudaEvent_t t0, t1, k0, k1;
    CUDA_TRY( cudaEventCreate(&t0) );
    CUDA_TRY( cudaEventCreate(&t1) );
    CUDA_TRY( cudaEventCreate(&k0) );
    CUDA_TRY( cudaEventCreate(&k1) );

    CUDA_TRY( cudaEventRecord(t0) );                          // total start
    CUDA_TRY( cudaMemcpy(d_in, h_in, BYTES, cudaMemcpyHostToDevice) );

    /* grid / block */
    const int BLOCK = 256;
    int grid = (N + BLOCK - 1) / BLOCK;
    grid = (grid > 65535) ? 65535 : grid;                     // å®‰å…¨ä¸Šé™

    CUDA_TRY( cudaEventRecord(k0) );                          // kernel start
    process_data_kernel<<<grid, BLOCK>>>(d_in, d_out, N);
    CUDA_TRY( cudaGetLastError() );
    CUDA_TRY( cudaEventRecord(k1) );                          // kernel end

    CUDA_TRY( cudaMemcpy(h_out, d_out, BYTES, cudaMemcpyDeviceToHost) );
    CUDA_TRY( cudaEventRecord(t1) );                          // total end
    CUDA_TRY( cudaEventSynchronize(t1) );

    float totalMs  = 0.f, kernelMs = 0.f;
    cudaEventElapsedTime(&totalMs , t0, t1);
    cudaEventElapsedTime(&kernelMs, k0, k1);

    printf("GPU time  (total)  = %.3f ms\n", totalMs );
    printf("GPU time  (kernel) = %.3f ms\n", kernelMs);

    cudaFree(d_in); cudaFree(d_out);
    cudaEventDestroy(t0); cudaEventDestroy(t1);
    cudaEventDestroy(k0); cudaEventDestroy(k1);
}

/*--------------------------------------------------------------*
 | 5. main                                                      |
 *--------------------------------------------------------------*/
int main()
{
    const int  N = 1 << 24;          // 16 777 216 elements
    const float EPS = 1e-6f;         // ç›¸å¯¹è¯¯å·®åˆ†æ¯é˜ˆå€¼

    std::vector<float> h_in (N);
    std::vector<float> h_cpu(N);
    std::vector<float> h_gpu(N);

    /* éšæœºè¾“å…¥ */
    std::mt19937 gen(42);
    std::uniform_real_distribution<float> dis(0.1f, 1000.f);
    for (auto& v : h_in) v = dis(gen);

    /* --- CPU baseline --- */
    auto c0 = std::chrono::high_resolution_clock::now();
    process_data_cpu(h_in.data(), h_cpu.data(), N);
    auto c1 = std::chrono::high_resolution_clock::now();
    double cpuMs = std::chrono::duration<double, std::milli>(c1 - c0).count();
    printf("CPU time           = %.3f ms\n", cpuMs);

    /* --- GPU --- */
    launch_gpu(h_in.data(), h_gpu.data(), N);

    /* --- è¯¯å·®æ ¡éªŒ --- */
    double maxAbs = 0.0, maxRel = 0.0;
    for (int i = 0; i < N; ++i) {
        double ref  = h_cpu[i];
        double diff = std::fabs((double)h_gpu[i] - ref);
        maxAbs = std::max(maxAbs, diff);
        if (std::fabs(ref) > EPS)
            maxRel = std::max(maxRel, diff / std::fabs(ref));
    }

    printf("max abs err = %.6e  |  max rel err = %.6e\n", maxAbs, maxRel);
    return 0;
}
```

ç¼–è¯‘å’Œæ‰§è¡Œç»“æœï¼š

```
root@a100vm:~# nvcc -O3 -std=c++17 process_gpu.cu -o process_gpu
root@a100vm:~# ./process_gpu
CPU time           = 288.920 ms
GPU time  (total)  = 33.453 ms
GPU time  (kernel) = 26.006 ms
max abs err = 9.536743e-07  |  max rel err = 3.537088e-07
root@a100vm:~# 
```



#### **æ­¥éª¤ 4ï¼šæ€§èƒ½ä¼˜åŒ–æŠ€å·§**

1. **å†…å­˜è®¿é—®åˆå¹¶**ï¼š

   ```
   // ä½æ•ˆï¼šè·¨æ­¥è®¿é—®
   value = data[row * width + col];
   
   // é«˜æ•ˆï¼šè¿ç»­è®¿é—®
   value = data[col * height + row];  // è½¬ç½®ä¸ºåˆ—ä¼˜å…ˆ
   ```

   

2. **ä½¿ç”¨å¿«é€Ÿæ•°å­¦å‡½æ•°**ï¼š

   ```
   // æ›¿æ¢æ ‡å‡†å‡½æ•°
   __sinf(x)  // æ¯” sinf() å¿« 4xï¼Œç²¾åº¦ç•¥ä½
   __frcp_rn(x) // å¿«é€Ÿå€’æ•°
   ```

   

3. **å…±äº«å†…å­˜ä¼˜åŒ–**ï¼š

   ```
   __shared__ float tile[256];
   tile[threadIdx.x] = input[global_idx];
   __syncthreads();
   // å—å†…ååŒè®¡ç®—
   ```

   

### CUDA Stream æµæ°´çº¿æ¶æ„

å…³é”®å°±åœ¨â€œ**é‡å ï¼ˆå¹¶è¡Œï¼‰çš„æ˜¯è°è·Ÿè°**â€â€”â€”è·¨æ‰¹æ¬¡ï¼ŸåŒä¸€æ‰¹æ¬¡å†…éƒ¨ï¼Ÿè¿˜æ˜¯æ ¹æœ¬å°±æ˜¯ä¸åŒä¸šåŠ¡ï¼Ÿä¸‹é¢ç”¨ä¸€å¼ å¯¹ç…§è¡¨ï¼‹ç¤ºæ„æ—¶é—´çº¿æŠŠåŒºåˆ«è®²é€ï¼ˆMarkdown è¡¨ï¼Œå¯ç›´æ¥è´´æ­£æ–‡ï¼‰ã€‚

| æ¨¡å¼                                         | å…¸å‹ç”¨å‡ æ¡ stream           | è°ä¸è°åœ¨å¹¶è¡Œï¼Ÿï¼ˆé‡å ç»´åº¦ï¼‰                                   | ä¸¾ä¾‹ (å‡è®¾å•å¸§/æ‰¹ç”¨æ—¶ï¼šH2D=4 msï¼ŒKernel=8 msï¼ŒD2H=4 ms)      | éœ€è¦çš„é¢å¤–æŠ€å·§                                               | é€‚ç”¨åœºæ™¯                                         |
| -------------------------------------------- | --------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------ |
| A. å•æµä¸²è¡Œï¼ˆé»˜è®¤æµï¼‰                        | 1                           | æ— ï¼›H2Dâ†’Kâ†’D2H ä¾æ¬¡æ‰§è¡Œ                                       | Demo : ä¸€å¼ å›¾ç‰‡ Gaussian Blurï¼Œå…¨éƒ¨æ”¾é»˜è®¤æµ                  | æ—                                                            | åªæ±‚åŠŸèƒ½æ­£ç¡®ã€è°ƒè¯•                               |
| B. â€œæ¯æ‰¹ 1 æµâ€ è½®è½¬ï¼ˆè·¨æ‰¹æµæ°´ï¼‰              | â‰¥3ï¼ˆbatch0â†’s0ï¼Œbatch1â†’s1â€¦ï¼‰ | **ä¸åŒæ‰¹æ¬¡ä¹‹é—´** çš„ H2D / Kernel / D2H äº’ç›¸é‡å <br>åŒä¸€æ‰¹å†…éƒ¨ä¾æ—§ä¸²è¡Œ | åœºæ™¯ï¼šæ‘„åƒå¤´ 30 FPS æ¨ç†<br>æ—¶é—´çº¿ï¼ˆ3 æµè½®è½¬ï¼‰<br>`\n t=0  : s0 H2D0\n t=4  : s0 K0 & s1 H2D1 å¹¶è¡Œ\n t=8  : s0 D2H0 & s1 K1 & s2 H2D2 â€¦` | ä¸å¿…é¡» pinnedï¼Œä½†å»ºè®®ç”¨ï¼›æ— éœ€ event                          | æ‰¹é‡è¾ƒå°ä½†æŒç»­ä¸æ–­çš„æµåª’ä½“ / æ¨ç† / ETL          |
| C. â€œCopy æµ + Compute æµâ€ æ‹†é˜¶æ®µï¼ˆæ‰¹å†…æµæ°´ï¼‰ | 2â€“3 æ¡/æ‰¹ï¼ˆHæµã€Kæµã€Dæµï¼‰  | **åŒä¸€æ‰¹æ¬¡å†…éƒ¨** çš„ H2D / Kernel / D2H å°±å¼€å§‹å¹¶è¡Œï¼›å†å åŠ è·¨æ‰¹æ¬¡ | å¤§æ‰¹çŸ©é˜µä¹˜ï¼šä¸€æ¬¡é€ 200 MB<br>æ—¶é—´çº¿ (åŒä¸€æ‰¹) â†˜<br>`\n Hæµ : H2D0  H2D1 â€¦\n Kæµ :     K0     K1 â€¦\n Dæµ :         D2H0   D2H1 â€¦` | å¿…é¡» pinned host å†…å­˜ + `cudaEventRecord / WaitEvent` æŠŠ 3 æµä¸²èµ·æ¥ | å•æ‰¹å¾ˆå¤§æˆ– PCIe å æ¯”é«˜ï¼Œéœ€è¦æŠŠåŒæ‰¹æ‹·è´ä¹Ÿè—è¿›è®¡ç®— |
| D. å¹¶å‘-Kernel å¤šç§Ÿæˆ·ï¼ˆå¤šæ¨¡å‹ï¼‰              | Nï¼ˆæ¯ä¸ªæ¨¡å‹/ä¸šåŠ¡è‡ªæœ‰ 1 æµï¼‰ | **å®Œå…¨ä¸åŒ kernel / ä¸åŒä»»åŠ¡** å¹¶è¡Œï¼›æ¯æ¡æµé‡Œ H2Dâ†’Kâ†’D2H æ•´æ®µä¸²è¡Œ | A100-MIG ä¸ŠæŠŠ ResNet50 ä¸ BERT åœ¨çº¿æœåŠ¡<br>ä¸¤æ¡æµå„è‡ªæ’é˜Ÿï¼ŒGPU æŠŠ K_ResNet å’Œ K_BERT äº¤æ›¿è£…å…¥ SM | åªè¦ GPU æ”¯æŒ Concurrent Kernelsï¼›æ— äº‹ä»¶äº’é”éœ€æ±‚             | å¤šæ¨¡å‹åœ¨çº¿æ¨ç†ã€å° kernel å¾®æœåŠ¡ã€AB æµ‹è¯•        |

çœ‹æ—¶é—´çº¿æ›´ç›´è§‚ï¼š

B æ¨¡å¼ (3 æµè½®è½¬)

```
æ—¶é—´ â†’
s0:  H2D0 ---- K0 ---- D2H0
s1:         H2D1 ---- K1 ---- D2H1
s2:                H2D2 ---- K2 ---- D2H2
```



è·¨æµé”™ä½ï¼Œè®© **æ‹·è´(1) ä¸ è®¡ç®—(0)**ï¼Œ**æ‹·è´(2) ä¸ è®¡ç®—(1)** â€¦ é‡å ã€‚

C æ¨¡å¼ (åŒæ‰¹ä¹Ÿæ‹†æµ)

```
æ—¶é—´ â†’
Hæµ:  H2D0    H2D1    H2D2
Kæµ:        K0    K1     K2
Dæµ:            D2H0    D2H1    D2H2
```



åŒä¸€æ‰¹è‡ªå·±çš„ H2D ä¸ä¸Šä¸€ä¸ªæ‰¹çš„ Kã€å†ä¸å†ä¸Šä¸€ä¸ªæ‰¹çš„ D2H ä¸‰è·¯ç¡¬ä»¶ç®¡çº¿å…¨å¤©å€™æ»¡è½½ã€‚

åŒºåˆ«æ€»ç»“ä¸€å¥è¯ï¼š

â€¢ Bï¼šå¹¶è¡Œçš„æ˜¯â€œæ‰¹ A çš„è®¡ç®—â€ vs â€œæ‰¹ B çš„æ‹·è´â€ï¼›**åŒä¸€æ‰¹å†…éƒ¨è¿˜æ˜¯ä¸²è¡Œ**ã€‚
â€¢ Cï¼šå†è¿›ä¸€å±‚ï¼ŒæŠŠ **åŒä¸€æ‰¹é‡Œçš„æ‹·è´-è®¡ç®—-å›ä¼ ** ä¹Ÿæ‹†åˆ°ä¸åŒæµï¼›å¹¶è¡Œç²’åº¦æ›´ç»†ã€‚
â€¢ Dï¼šæ ¹æœ¬æ˜¯ä¸åŒä»»åŠ¡/æ¨¡å‹åœ¨æŠ¢åŒä¸€å¼ å¡ï¼Œä¸å…³æ³¨â€œæ‰¹â€æ¦‚å¿µï¼›æµç”¨äºå¤šç§Ÿæˆ·éš”ç¦»ã€‚



#### **1. é»˜è®¤æµï¼ˆDefault Streamï¼‰çš„æœ¬è´¨**

- **æ‰€æœ‰ CUDA ç¨‹åºè‡ªåŠ¨æ‹¥æœ‰ä¸€ä¸ªéšå¼é»˜è®¤æµ**ï¼ˆç§°ä¸º stream 0ï¼‰

- å…³é”®é™åˆ¶ï¼š

  ```
  graph LR
    A[æ“ä½œ1] --> B[æ“ä½œ2] --> C[æ“ä½œ3]
  ```

  - æ‰€æœ‰æ“ä½œä½¿ç”¨å¼‚æ­¥ APIï¼ˆå¦‚ `cudaMemcpyAsync`ï¼‰ä¹Ÿ**æ— æ³•å¹¶è¡Œ**
  - æ ¸å‡½æ•°ä¸å†…å­˜æ‹·è´**ä¸èƒ½é‡å æ‰§è¡Œ**
  - ç›¸å½“äº**å•è½¦é“é«˜é€Ÿè·¯**ï¼Œåè½¦å¿…é¡»ç­‰å‰è½¦é€šè¿‡

#### **2. é»˜è®¤æµçš„æ€§èƒ½ç“¶é¢ˆ**

åœ¨ CPU 100% + GPU åˆ©ç”¨ç‡ä½çš„åœºæ™¯ä¸‹å°¤ä¸ºä¸¥é‡ï¼š

```
timeline
    title é»˜è®¤æµæ‰§è¡Œè¿‡ç¨‹
    section GPUæ—¶é—´çº¿
    H2Dä¼ è¾“  ï¼š 0-5ms
    ç©ºé—²ç­‰å¾…  ï¼š 5-10msï¼ˆCPUå¤„ç†æ•°æ®ï¼‰
    æ ¸å‡½æ•°   ï¼š 10-20ms
    ç©ºé—²ç­‰å¾…  ï¼š 20-25msï¼ˆCPUå¤„ç†ç»“æœï¼‰
    H2Dä¼ è¾“  ï¼š 25-30ms
```

- å…³é”®é—®é¢˜ï¼šç°è‰²ç©ºé—²æ—¶æ®µå¯¼è‡´ï¼š

  - GPU åˆ©ç”¨ç‡ä»… 50% å·¦å³
- CPU å’Œ GPU **äº¤æ›¿ç©ºé—²**ï¼Œæ— æ³•ååŒ

#### **3. å¤šæµæŠ€æœ¯çš„å¿…è¦æ€§**

##### ä»¥ä¸‹åœºæ™¯å¿…é¡»æ˜¾å¼ä½¿ç”¨å¤šæµï¼š

| åœºæ™¯                   | é»˜è®¤æµæ˜¯å¦è¶³å¤Ÿ | å¤šæµå¿…è¦æ€§ |
| ---------------------- | -------------- | ---------- |
| å•ä»»åŠ¡ç®€å•è®¡ç®—         | âœ… è¶³å¤Ÿ         | âŒ ä¸éœ€è¦   |
| **CPU-GPU æµæ°´çº¿å¤„ç†** | âŒ ä¸è¶³         | âœ… **å¿…éœ€** |
| å¤šä»»åŠ¡å¹¶è¡Œ             | âŒ ä¸è¶³         | âœ… å¿…éœ€     |
| å®æ—¶æ•°æ®å¤„ç†           | âŒ ä¸è¶³         | âœ… å¿…éœ€     |

**æ‚¨çš„ä¸šåŠ¡ç°çŠ¶**ï¼š

- CPU 100% + GPU åˆ©ç”¨ç‡ä½ â†’ **å…¸å‹è®¡ç®—-ä¼ è¾“æœªé‡å **

- éœ€é€šè¿‡å¤šæµå®ç°ï¼š

  ```
  timeline
      title å¤šæµä¼˜åŒ–å
      section Stream 0
      H2Dä¼ è¾“ ï¼š 0-5ms
      è®¡ç®—    ï¼š 5-15ms
      D2Hä¼ è¾“ ï¼š 15-20ms
  
      section Stream 1
      H2Dä¼ è¾“ ï¼š 3-8ms
      è®¡ç®—    ï¼š 8-18ms
      D2Hä¼ è¾“ ï¼š 18-23ms
  ```


#### **4. ä¼˜åŠ¿**

1. **è§£å†³æ ¸å¿ƒé—®é¢˜**ï¼š

   - å¤šæµæ˜¯æå‡ GPU åˆ©ç”¨ç‡åˆ° 60%+ çš„**å…³é”®æŠ€æœ¯è·¯å¾„**
   - ç›´æ¥é’ˆå¯¹æ‚¨â€œCPU é«˜è´Ÿè½½ + GPU ä½åˆ©ç”¨â€çš„ç—›ç‚¹

2. **å®¢æˆ·è®¤çŸ¥ç›²åŒº**ï¼š

   - å¤šæ•°å¼€å‘è€…è¯¯ä»¥ä¸ºâ€œCUDA è‡ªåŠ¨å¹¶è¡Œæ‰€æœ‰æ“ä½œâ€
   - å®é™…éœ€è¦**æ˜¾å¼è®¾è®¡æµæ°´çº¿æ¶æ„**

3. **å®æ–½æ€§ä»·æ¯”é«˜**ï¼š

   - ä»£ç æ”¹åŠ¨é‡ï¼š< 50 è¡Œ
   - æ€§èƒ½æ”¶ç›Šï¼šæå‡ 40-60% GPU åˆ©ç”¨ç‡
   - **æ— ç¡¬ä»¶æˆæœ¬**

4. **Azure A100 ä¸“å±ä¼˜åŒ–**ï¼š

   ```
   // ä¸ºæ¯ä¸ªMIGå®ä¾‹åˆ›å»ºç‹¬ç«‹æµç»„
   cudaStream_t mig_streams[3];
   for (int i=0; i<3; i++) {
     cudaSetDevice(i);  // åˆ‡æ¢åˆ°MIGè®¾å¤‡i
     cudaStreamCreate(&mig_streams[i]);
   }
   ```


#### **5. æœ€ä½å®ç°æ–¹æ¡ˆ**

```
// æ­¥éª¤1ï¼šåˆ›å»º2ä¸ªé¢å¤–æµï¼ˆå…±3æµï¼‰
cudaStream_t s1, s2;
cudaStreamCreate(&s1);
cudaStreamCreate(&s2);

// æ­¥éª¤2ï¼šæµæ°´çº¿å¤„ç†
for (int i=0; i<batches; i++) {
  cudaStream_t cur_stream = (i % 3 == 0) ? s0 : 
                           (i % 3 == 1) ? s1 : s2;

  cudaMemcpyAsync(dev_buf, host[i], size, cur_stream);
  kernel<<<grid, block, 0, cur_stream>>>(dev_buf);
  cudaMemcpyAsync(host[i], dev_buf, size, cur_stream);
}

// æ­¥éª¤3ï¼šæœ€ç»ˆåŒæ­¥
cudaStreamSynchronize(s0);
cudaStreamSynchronize(s1);
cudaStreamSynchronize(s2);
```



## æ­¥éª¤ä¸‰: åº”ç”¨æ¶æ„æ‹†åˆ†

> èƒŒæ™¯ï¼šå·²å®Œæˆã€ŒCPU ä¼˜åŒ–ã€+ã€ŒGPU è¿ç§»ã€ä½†ä»å­˜åœ¨
> â€¢ GPU åˆ©ç”¨ç‡é«˜ã€CPU ä» >70 %
> â€¢ CPU / GPU è´Ÿè½½å³°å€¼é”™ä½ï¼Œå•æœºè§„æ ¼éš¾å…¼é¡¾
> â€¢ ä¸åŒç§Ÿæˆ·å¯¹ CPUã€GPU çš„æ‰©ç¼©å®¹è¯‰æ±‚å®Œå…¨ä¸åŒ
>
> æ»¡è¶³ä¸Šè¿°ä»»æ„åœºæ™¯ï¼Œå°±åº”è€ƒè™‘æŠŠ **CPU-å¯†é›†é€»è¾‘** ä¸ **GPU-è®¡ç®—é€»è¾‘** è§£è€¦ä¸ºä¸¤ä¸ªå¾®æœåŠ¡ã€‚

------

### 1. æ‹†åˆ†å†³ç­–çŸ©é˜µ

| ç»´åº¦     | å€™é€‰é€»è¾‘ A (ç•™ CPU) | å€™é€‰é€»è¾‘ B (ç•™ GPU) | åˆ¤æ–­æ ‡å‡†                    |
| -------- | ------------------- | ------------------- | --------------------------- |
| è®¡ç®—å¯†åº¦ | FLOPs/Byte < 3      | FLOPs/Byte > 10     | B â‡’ GPUï¼ŒA â‡’ CPU            |
| å¹¶è¡Œåº¦   | < 1 k               | > 10 k              | é«˜å¹¶è¡Œåº¦æ‰å€¼å¾— GPU          |
| è°ƒç”¨å»¶è¿Ÿ | P99 < 2 ms          | P99 < 5 ms          | å»¶è¿Ÿæ•æ„Ÿé€»è¾‘ä¼˜å…ˆåŒæœº        |
| çŠ¶æ€è€¦åˆ | å¼º                  | å¼±                  | å¼ºè€¦åˆæš‚ç¼“æ‹†                |
| æ•°æ®é‡   | KB çº§               | MB çº§               | ä¼ è¾“é‡å¤§ä¾§é‡ GPU ç«¯ä¸€æ¬¡ç®—å®Œ |

â‰¥ 3 é¡¹æ»¡è¶³â€œæ‹†åˆ†â€å€¾å‘ â†’ è¿›å…¥å¾®æœåŠ¡å®æ–½ã€‚

------

### 2. ç«¯åˆ°ç«¯å®æ–½æµç¨‹

#### é˜¶æ®µ 1ï¼šæœåŠ¡è¾¹ç•Œ & æŠ€æœ¯é€‰å‹

1. è®¾è®¡ protobuf æ¥å£ï¼ˆè¾“å…¥/è¾“å‡ºå­—æ®µï¼‰ã€‚
2. å‹æµ‹åºåˆ—åŒ– + gRPC å•æ¬¡ RTTï¼šåŒ… < 1 MBï¼ŒRTT < 1 msã€‚
3. è°ƒç”¨æ¨¡å¼
   â€¢ å®æ—¶ â†’ gRPCï¼ˆUnary / åŒå‘æµï¼‰
   â€¢ å¼‚æ­¥æ‰¹ â†’ Kafka / AMQP

#### é˜¶æ®µ 2ï¼šCPU-VMï¼ˆService-CPUï¼‰

1. å°† CPU çƒ­ç‚¹ä»£ç æŠ½ä¸ºç‹¬ç«‹ä»“åº“ â†’ Docker é•œåƒã€‚
2. çº¿ç¨‹æ± ã€NUMA ç»‘æ ¸ã€jemalloc å·²åœ¨æ­¥éª¤ 1 ä¼˜åŒ–ï¼Œå¯ç›´æ¥å¤ç”¨ã€‚
3. éƒ¨ç½²ï¼šé€šç”¨è®¡ç®—å‹ VMï¼ˆå¦‚ D8s v5ï¼‰ï¼Œå‰¯æœ¬æ•°â‰ˆå³°å€¼ CPU%/70 %ã€‚

#### é˜¶æ®µ 3ï¼šGPU-VMï¼ˆService-GPUï¼‰

1. æŠ½ç¦» GPU kernel + Stream æµæ°´çº¿ä¸ºç‹¬ç«‹è¿›ç¨‹ã€‚
2. ç”¨ MIG æ—¶æŒ‰ç§Ÿæˆ· 1:1 slice ç»‘å®šã€‚
3. æš´éœ² gRPCï¼Œæ”¯æŒ `batch_size` ä»¥ä¾¿å®¢æˆ·ç«¯è‡ªé€‚åº”æ‰“åŒ…ã€‚

#### é˜¶æ®µ 4ï¼šé€šä¿¡å±‚ & ç†”æ–­

```
sequenceDiagram
Client->>CPU-Svc: ä¸šåŠ¡è¯·æ±‚(JSON/REST)
CPU-Svc->>GPU-Svc: gRPC Invoke (<1 ms)
note right of CPU-Svc: startTimer()
GPU-Svc-->>CPU-Svc: æ¨ç†ç»“æœ
CPU-Svc-->>Client: æœ€ç»ˆå“åº”
CPU-Svc->>CPU-Svc: if latency>2 ms\n  switchToLocalFallback()
```



â€¢ `latency>2 ms` â†’ èµ°æœ¬åœ° CPU å¤‡ä»½å¹¶è®° `gpu_fallback_total`
â€¢ è¿ç»­ N æ¬¡è¶…æ—¶è§¦å‘ç†”æ–­ï¼›30 s å†…æ¢å¤åˆ™å›åˆ‡ GPUã€‚

#### é˜¶æ®µ 5ï¼šCI/CD & å›æ»š

1. åŒé•œåƒï¼š`service-cpu:{sha}` & `service-gpu:{sha}`ã€‚
2. Helm / Argo Rollouts Blue-Greenï¼Œé«˜å¯ç”¨ç°åº¦ã€‚
3. GPU-Svc å…ˆç° 10 %ï¼Œç›‘æ§ `gpu_latency_ms`ã€`gpu_fail_ratio`ã€‚

#### é˜¶æ®µ 6ï¼šç›‘æ§ & è‡ªåŠ¨ä¼¸ç¼©

| ç»„ä»¶    | æ ¸å¿ƒæŒ‡æ ‡                                 | ä¼¸ç¼©ç­–ç•¥                         |
| ------- | ---------------------------------------- | -------------------------------- |
| CPU-Svc | `cpu_util`, `req_qps`                    | HPAï¼šCPU>70 % & QPS>é˜ˆå€¼ â†’ +1    |
| GPU-Svc | `nvidia_gpu_utilization`, `mig_mem_used` | <50 % ç¼©å®¹ï¼›>80 % æ‰©å®¹æˆ–å¢ slice |
| é“¾è·¯    | `rpc_latency_p95`, `fallback_count`      | fallback è¿ç»­å‡é«˜è§¦å‘è­¦æŠ¥        |

------

### 3. å…¸å‹è½åœ°æ¡ˆä¾‹

#### æ¡ˆä¾‹ Aï¼šç”µå•†æ¨èï¼ˆç‰¹å¾å·¥ç¨‹ + Transformer æ¨ç†ï¼‰

| æŒ‡æ ‡       | å•ä½“è¿›ç¨‹ | æ‹†åˆ†å                              |
| ---------- | -------- | ----------------------------------- |
| CPU åˆ©ç”¨ç‡ | 90 %     | 50 %                                |
| GPU åˆ©ç”¨ç‡ | 40 %     | 75 %                                |
| P99 å»¶è¿Ÿ   | 10 ms    | 6 ms (GPU æ­£å¸¸)<br>11 ms (GPU ç†”æ–­) |

gRPC proto å…³é”®å­—æ®µ

```
message InferenceReq  { repeated float sparse = 1; repeated int64 dense = 2; }
message InferenceResp { repeated int64 item_id = 1; repeated float score = 2; }
service RecGPU { rpc Predict(InferenceReq) returns (InferenceResp); }
```



CPU-ä¾§ç†”æ–­ä¼ªç 

```
auto t0 = now();
auto status = stub->Predict(ctx, req, &resp);
if(!status.ok() || since_ms(t0)>2.0){
    FallbackPredictCPU(req,&resp);
    prometheus::inc("gpu_fallback_total");
}
```



#### æ¡ˆä¾‹ Bï¼šå®æ—¶è§†é¢‘ï¼ˆDemux + è¶…åˆ†è¾¨ç‡ï¼‰

â€¢ CPU-VMï¼šffmpeg è§£å°è£… + H.264 è§£ç ï¼ˆc6i.4xlargeï¼‰
â€¢ GPU-VMï¼šA100-40GBï¼Œ3 Ã— 1g.10gb MIG è·‘è¶…åˆ†æ¨¡å‹
â€¢ 1080p 60 fps Ã— 8 è·¯ï¼Œå•æµç«¯åˆ°ç«¯ < 40 ms

åŒå‘æµå¼ gRPCï¼ˆæ‘˜å½•ï¼‰

```
auto stream = stub->Process(&ctx);
for(;;){
    Frame f = pull_frame();     // CPU è§£ç 
    stream->Write(f);           // H2D Async
    Frame out;
    if(stream->Read(&out)) push_to_encoder(out);
}
```



------

### 4. ç†”æ–­ / å›é€€ç­–ç•¥è¡¨

| è§¦å‘æ¡ä»¶                                    | å›é€€åŠ¨ä½œ                             | æ¢å¤æ¡ä»¶                            | ç›‘æ§æŒ‡æ ‡                                                     |
| ------------------------------------------- | ------------------------------------ | ----------------------------------- | ------------------------------------------------------------ |
| RTT > 2 ms è¿ç»­ 3 æ¬¡<br>æˆ– error rate > 5 % | è°ƒ CPU ç‰ˆï¼›è¯·æ±‚å…¥ Kafka `GPU_QUEUED` | è¿ç»­ 30 s RTT < 1 ms ä¸” error < 1 % | `gpu_fallback_total`<br>`rpc_latency_p95`<br>`rpc_error_ratio` |

------

### 5. Grafana æ ¸å¿ƒé¢æ¿

1. GPU-Svcï¼š`nvidia_gpu_utilization{slice}`ï¼Œ`grpc_server_latency_seconds_bucket`
2. CPU-Svcï¼š`process_cpu_seconds_total / uptime`ï¼Œ`gpu_fallback_total`
3. é“¾è·¯ï¼š`rpc_latency_p99{client="CPUâ†’GPU"}`ï¼Œ`rpc_error_ratio`

(1 Ã— Heatmap + 2 Ã— Time-series è¶³å¤Ÿè¯Šæ–­å…¨é“¾è·¯)

------

### 6. å¸¸è§å‘ & å¯¹ç­–

| å‘                  | ç°è±¡               | å¯¹ç­–                              |
| ------------------- | ------------------ | --------------------------------- |
| gRPC ååºåˆ—åŒ–è€—æ—¶é«˜ | CPU-Svc å•æ¬¡ 1 ms+ | proto zero-copy + pinned å†…å­˜     |
| æ‰¹é‡è¿‡å¤§å¯¼è‡´å°¾å»¶è¿Ÿ  | P95 é£™å‡           | åŠ¨æ€ batchï¼Œ`target_latency=4 ms` |
| MIG æ˜¾å­˜ç¢ç‰‡        | æ¨ç†å®•æœº           | å›ºå®š sliceï¼Œå¤œé—´é‡å»º MIG          |

------

### 7. å®æ–½ç”˜ç‰¹ï¼ˆ5 å‘¨æ¨¡æ¿ï¼‰

```
W1  æœåŠ¡è¾¹ç•Œæ¢³ç† + proto
W2  æ‹† CPU é€»è¾‘ â†’ Service-CPU
W3  æ‹† GPU é€»è¾‘ â†’ Service-GPU
W4  gRPC + ç†”æ–­ + Prometheus
W5  GPU-Svc ç°åº¦ 10 % â†’ å…¨é‡ â†’ å…³å•ä½“
```



é€šè¿‡ä»¥ä¸Šæµç¨‹ã€çŸ©é˜µä¸æ¡ˆä¾‹ï¼Œæ‚¨å¯ä»¥æŒ‰éœ€æŠŠå‚æ•°æ›¿æ¢åˆ°è‡ªå·±çš„ä¸šåŠ¡ä¸­ï¼Œå³å¯å¿«é€Ÿè½åœ° **CPU-GPU å¾®æœåŠ¡è§£è€¦ + ç†”æ–­ä¿éšœ + å…¨é“¾è·¯ç›‘æ§**ã€‚

## æ­¥éª¤å››ï¼šé«˜å±‚çº§å¼¹æ€§ä¸å®¹ç¾æ‰©å±•

> ç›®æ ‡ï¼šåœ¨å·²å®Œæˆã€ŒCPU-VM â†” GPU-VM å¾®æœåŠ¡è§£è€¦ã€çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æŠŠç³»ç»Ÿåšæˆ
> â‘  è·¨ Region å®¹ç¾â€ƒâ‘¡ ç°åº¦å‘å¸ƒå…¨é“¾è·¯å¯å›æ»šâ€ƒâ‘¢ CPU åç«¯æŒ‰éœ€ Serverless å¼¹å‡ºå¼¹å…¥ã€‚
> å¦‚æ— å…¨çƒæµé‡æˆ–æç«¯å¯ç”¨æ€§è¦æ±‚ï¼Œæœ¬æ­¥éª¤å¯æŒ‰éœ€æ‹©å…¶ä¸€å®æ–½ã€‚

------

### å¤š Region / å¤šé›†ç¾¤å®¹ç¾

| æ–¹æ¡ˆ                   | æ‹“æ‰‘               | æ ¸å¿ƒç»„ä»¶                                                     | å…¸å‹å»¶è¿Ÿ     | é€‚ç”¨åœºæ™¯                     |
| ---------------------- | ------------------ | ------------------------------------------------------------ | ------------ | ---------------------------- |
| Activeâ€“Active          | ğŸ‡ºğŸ‡¸â†”ğŸ‡¸ğŸ‡¬â†’Anycast GSLB | â€¢ Global DNS (Route 53/GSLB)<br>â€¢ Istio Multi-primary<br>â€¢ CockroachDB æˆ– Spanner | <100 ms      | å…¨çƒæ—¥æ´» > 1 Mï¼Œåœ°åŸŸåˆ†å¸ƒå‡åŒ€ |
| Activeâ€“Passive         | ğŸ‡ºğŸ‡¸(ä¸»)â†”ğŸ‡ªğŸ‡º(å¤‡)      | â€¢ DNS åŠ æƒ + å¥åº·æ¢æµ‹<br>â€¢ ä¸» Region æ¯ 5 min å¤‡ä»½é•œåƒ / LFS | åˆ‡æ¢ 30â€“60 s | 95 % æµé‡é›†ä¸­å•ä¸€åŒºåŸŸ        |
| Zonal è·³åŒº (åŒ Region) | â¬†ï¸ AZ-A â†»â†» AZ-B     | â€¢ Kubernetes topologySpread<br>â€¢ GPU VM åŒæ­¥é•œåƒ             | <10 s        | å•äº‘å‚å•†ï¼Œå¤šå¯ç”¨åŒº           |

å®æ–½æ¸…å•

1. å…¨å±€å…¥å£ï¼šAnycast + GeoDNSï¼›å¥åº·æ¢æµ‹å¤±è´¥ < 5 s ä¸‹çº¿ã€‚
2. GPU Model Checkpointï¼šå¯¹è±¡å­˜å‚¨ + `rsync --append-verify`ï¼›ä¸»â†’å¤‡å»¶è¿Ÿ < 60 sã€‚
3. æ•°æ®å±‚ï¼šè·¨ Region ä½¿ç”¨ Spanner / CockroachDBï¼›æˆ–å¼‚æ­¥åŒå†™ Kafka â†’ Debeziumã€‚
4. ç¾éš¾æ¼”ç»ƒï¼šæ¯æœˆ 1 æ¬¡äººå·¥è§¦å‘ä¸» Region é»‘æ´ 15 minï¼ŒéªŒè¯ RPO=0 / RTO<60 sã€‚

------

### GPU-VM & CPU-VM æ··åˆç°åº¦å‘å¸ƒ

```
Client
  â”‚
  â”œâ”€â”€Istio Ingress (v1 90% / v2 10%)
  â”‚     â”œâ”€â”€ Service-CPU-v1 â—€â”€â”€â”
  â”‚     â””â”€â”€ Service-CPU-v2 â—€â”€â”¤ Argo Rollouts
  â”‚                           â””â”€ Service-GPU-v1/v2
```

1. æµé‡åˆ‡åˆ†

   ```
   apiVersion: split.smi-spec.io/v1alpha2
   kind: TrafficSplit
   spec:
     backends:
       - service: svc-cpu-v1  weight: 90
       - service: svc-cpu-v2  weight: 10
   ```

2. åŒç»´ç°åº¦åŸåˆ™
   â€¢ CPU-Svc ä¸ GPU-Svc **ç‰ˆæœ¬é”**ï¼š`schemaVersion` æ ‡ç­¾ä¸€è‡´æ‰å¯åŒæ± è·¯ç”±ã€‚
   â€¢ å…ˆå‡ CPU ä¾§ 10 % â†’ GPU ä¾§ 10 % â†’ CPU ä¾§ 100 % â†’ GPU ä¾§ 100 %ã€‚

3. è‡ªåŠ¨å›æ»šæ¡ä»¶

`gpu_fail_ratio > 1 %` OR `rpc_latency_p95 â†‘ 30 %` in 2 minã€‚
Argo Rollouts `analysisTemplate` ç¤ºä¾‹ï¼š

```
successCondition: result.gpu_fail_ratio < 0.01
failureLimit: 1
metrics:
  - name: gpu_fail_ratio
    interval: 1m
```



## ç»“è®º

å°†ä»¥ C++ ä¸ºä¸»çš„åº”ç”¨ç¨‹åºä» CPU è¿ç§»åˆ° GPU æ˜¯ä¸€ä¸ªç³»ç»ŸåŒ–çš„è¿‡ç¨‹ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘ç¡¬ä»¶æ¡ä»¶ã€è½¯ä»¶å·¥å…·å’Œä»£ç æ¶æ„ã€‚æœ¬æ–‡æä¾›äº†ä¸€ä¸ªåˆ†æ­¥éª¤çš„æŒ‡å—ï¼Œæ¶µç›–äº†ä»**åˆ†æå‡†å¤‡**ã€**å·¥å…·é€‰æ‹©**ã€**ä»£ç é‡æ„**åˆ°**æ€§èƒ½ä¼˜åŒ–**çš„å„ä¸ªæ–¹é¢ã€‚åœ¨å®è·µä¸­ï¼ŒæˆåŠŸçš„è¿ç§»æ¡ˆä¾‹è¡¨æ˜ï¼šå……åˆ†çš„å‰æœŸåˆ†æã€æ°å½“çš„å·¥å…·ï¼ˆå¦‚ CUDAï¼‰ä½¿ç”¨ï¼Œä»¥åŠè€å¿ƒç»†è‡´çš„æ€§èƒ½è°ƒä¼˜ï¼Œèƒ½å¤Ÿå¸®åŠ©åº”ç”¨åœ¨ GPU ä¸Šå®ç°æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚åŒæ—¶ä¹Ÿè¦è®¤è¯†åˆ°ï¼ŒGPU åŠ é€Ÿå¹¶éä¸‡èƒ½ï¼Œåªæœ‰åœ¨**ç®—æ³•å¹¶è¡Œåº¦é«˜ã€æ•°æ®è§„æ¨¡å¤§**çš„æƒ…å†µä¸‹æ‰èƒ½å±•ç°å‡ºä¼˜åŠ¿ã€‚