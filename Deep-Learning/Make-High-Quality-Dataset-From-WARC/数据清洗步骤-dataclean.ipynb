{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 14:30:12.719464: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 14:30:16.396905: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 12.3665\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.5180\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 10.6987\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.9333\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.1499\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.3664\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.5305\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 6.6356\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 5.6925\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4.6734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f438ff53eb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Generate Sample Data\n",
    "# For demonstration, we create synthetic data for a regression task\n",
    "X = np.random.rand(100, 3)  # 100 samples, 3 features\n",
    "y = X @ np.array([2.0, -1.0, 3.0]) + 1.5  # Linear relationship with some coefficients and bias\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),  # Input layer with 3 inputs\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Another dense layer\n",
    "    tf.keras.layers.Dense(1)  # Output layer with 1 unit (for regression)\n",
    "])\n",
    "# Compile the Model with the Adam Optimizer\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='mean_squared_error')  # MSE is typical for regression\n",
    "model.fit(X, y, epochs=10)  # Train for 10 epochs as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk  \n",
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords')  \n",
    "nltk.download('wordnet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'coding', 'pythonprogramming', 'fun', 'let', 'clean', 'text']\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "import nltk  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "  \n",
    "# å¸¦æœ‰è¡¨æƒ…ç¬¦å·ã€å“ˆå¸Œæ ‡ç­¾å’Œå…¶ä»–å­—ç¬¦çš„ç¤ºä¾‹æ–‡æœ¬  \n",
    "text = \"I love coding! ğŸ˜Š #PythonProgramming is fun! ğŸâœ¨ Letâ€™s clean some text ğŸ§¹\"  \n",
    "  \n",
    "# å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦  \n",
    "text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "  \n",
    "# åˆ†è¯  \n",
    "tokens = word_tokenize(text)  \n",
    "  \n",
    "# æ ‡å‡†åŒ–ï¼ˆè½¬æ¢ä¸ºå°å†™ï¼‰  \n",
    "cleaned_tokens = [token.lower() for token in tokens]  \n",
    "  \n",
    "# å»é™¤åœç”¨è¯  \n",
    "stop_words = set(stopwords.words('english'))  \n",
    "cleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]  \n",
    "  \n",
    "# è¯å½¢è¿˜åŸ  \n",
    "lemmatizer = WordNetLemmatizer()  \n",
    "cleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]  \n",
    "  \n",
    "print(cleaned_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'coding', 'pythonprogramming', 'fun', 'let', 'clean', 'text']\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "import nltk  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "  \n",
    "# å¸¦æœ‰è¡¨æƒ…ç¬¦å·ã€å“ˆå¸Œæ ‡ç­¾å’Œå…¶ä»–å­—ç¬¦çš„ç¤ºä¾‹æ–‡æœ¬  \n",
    "text = \"I love coding! ğŸ˜Š #PythonProgramming is fun! ğŸâœ¨ Letâ€™s clean some text ğŸ§¹\"  \n",
    "  \n",
    "# å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦  \n",
    "text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "  \n",
    "# åˆ†è¯  \n",
    "tokens = word_tokenize(text)  \n",
    "  \n",
    "# æ ‡å‡†åŒ–ï¼ˆè½¬æ¢ä¸ºå°å†™ï¼‰  \n",
    "cleaned_tokens = [token.lower() for token in tokens]  \n",
    "\n",
    "  \n",
    "# å»é™¤åœç”¨è¯  \n",
    "stop_words = set(stopwords.words('english'))  \n",
    "stop_words.remove('i')  # ä»åœç”¨è¯åˆ—è¡¨ä¸­ç§»é™¤â€œIâ€ï¼Œæ³¨æ„åœç”¨è¯åˆ—è¡¨ä¸­çš„â€œIâ€æ˜¯å°å†™  \n",
    "cleaned_tokens = [token for token in cleaned_tokens if token not in stop_words]  \n",
    "  \n",
    "# è¯å½¢è¿˜åŸ  \n",
    "lemmatizer = WordNetLemmatizer()  \n",
    "cleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]  \n",
    "  \n",
    "print(cleaned_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But it's not ccenhancese about more language ccenhancese. Other important aspect is ensuring accurate retrieval by ccenhancese product name spellings. Additionally, refining descriptions enhances the ccenhancese of the content.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re  \n",
    "  \n",
    "# å¸¦æœ‰æ‹¼å†™é”™è¯¯çš„ç¤ºä¾‹æ–‡æœ¬  \n",
    "text_with_errors = \"\"\"But it's not coherence about more language coherence. Other important aspect is ensuring accurate retrieval by coherence product name spellings. Additionally, refining descriptions enhances the coherence of the content.\"\"\"  \n",
    "  \n",
    "# çº æ­£æ‹¼å†™é”™è¯¯çš„å‡½æ•°  \n",
    "def correct_spelling_errors(text):  \n",
    "    # å®šä¹‰å¸¸è§æ‹¼å†™é”™è¯¯åŠå…¶çº æ­£çš„å­—å…¸  \n",
    "    spelling_corrections = {  \n",
    "        \"oherence\": \"coherence\",  # æ³¨æ„è¿™é‡Œä¿®æ­£äº†å¤šä¸ª \"oherence\" çš„æ˜ å°„ï¼Œåªä¿ç•™äº†ä¸€ä¸ªæ­£ç¡®çš„æ˜ å°„  \n",
    "        \"accurte\": \"accurate\",  \n",
    "        \"retrievel\": \"retrieval\",  \n",
    "        \"refning\": \"refining\",  \n",
    "        \"oherenc\": \"enhances\",  \n",
    "        \"contnt\": \"content\",  \n",
    "    }  \n",
    "  \n",
    "    # éå†å­—å…¸ä¸­çš„æ¯ä¸€å¯¹é”®å€¼å¯¹ï¼Œç”¨æ­£ç¡®çš„ç‰ˆæœ¬æ›¿æ¢æ‹¼å†™é”™è¯¯çš„å•è¯  \n",
    "    for mistake, correction in spelling_corrections.items():  \n",
    "        text = re.sub(mistake, correction, text)  \n",
    "  \n",
    "    return text  \n",
    "  \n",
    "# åœ¨ç¤ºä¾‹æ–‡æœ¬ä¸­çº æ­£æ‹¼å†™é”™è¯¯  \n",
    "cleaned_text = correct_spelling_errors(text_with_errors)  \n",
    "  \n",
    "print(cleaned_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"text\": \"'The Top 10 Tech Trends\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"2024\", \"label\": \"DATE\"}, {\"text\": \"John\", \"label\": \"PERSON\"}, {\"text\": \"Google\", \"label\": \"ORG\"}, {\"text\": \"Microsoft\", \"label\": \"ORG\"}, {\"text\": \"AI\", \"label\": \"ORG\"}]\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_sm  \n",
    "import spacy  \n",
    "import json  \n",
    "  \n",
    "# åŠ è½½è‹±è¯­è¯­è¨€æ¨¡å‹  \n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "  \n",
    "# å¸¦æœ‰å…ƒæ•°æ®å€™é€‰è€…çš„ç¤ºä¾‹æ–‡æœ¬  \n",
    "text = \"\"\"In a blog post titled 'The Top 10 Tech Trends of 2024,'  \n",
    "John Doe discusses the rise of artificial intelligence and machine learning  \n",
    "in various industries. The article mentions companies like Google and Microsoft  \n",
    "as pioneers in AI research. Additionally, it highlights emerging technologies  \n",
    "such as natural language processing and computer vision.\"\"\"  \n",
    "  \n",
    "# ä½¿ç”¨spaCyå¤„ç†æ–‡æœ¬  \n",
    "doc = nlp(text)  \n",
    "  \n",
    "# æå–å‘½åå®ä½“åŠå…¶æ ‡ç­¾  \n",
    "meta_data = [{\"text\": ent.text, \"label\": ent.label_} for ent in doc.ents]  \n",
    "  \n",
    "# å°†å…ƒæ•°æ®è½¬æ¢ä¸ºJSONæ ¼å¼  \n",
    "meta_data_json = json.dumps(meta_data, ensure_ascii=False)  \n",
    "  \n",
    "print(meta_data_json)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"text\": \"'The Top 10 Tech Trends\",\n",
      "        \"label\": \"WORK_OF_ART\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"2024\",\n",
      "        \"label\": \"DATE\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"John\",\n",
      "        \"label\": \"PERSON\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"Google\",\n",
      "        \"label\": \"ORG\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"Microsoft\",\n",
      "        \"label\": \"ORG\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"AI\",\n",
      "        \"label\": \"ORG\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json  \n",
    "  \n",
    "# å‡è®¾meta_data_jsonæ˜¯æ‚¨ä»spaCyå¤„ç†åå¾—åˆ°çš„JSONå­—ç¬¦ä¸²  \n",
    "meta_data_json = '[{\"text\": \"\\'The Top 10 Tech Trends\", \"label\": \"WORK_OF_ART\"}, {\"text\": \"2024\", \"label\": \"DATE\"}, {\"text\": \"John\", \"label\": \"PERSON\"}, {\"text\": \"Google\", \"label\": \"ORG\"}, {\"text\": \"Microsoft\", \"label\": \"ORG\"}, {\"text\": \"AI\", \"label\": \"ORG\"}]'  \n",
    "  \n",
    "# ä½¿ç”¨json.loadså°†å­—ç¬¦ä¸²è½¬æ¢æˆPythonå¯¹è±¡  \n",
    "meta_data = json.loads(meta_data_json)  \n",
    "  \n",
    "# ä½¿ç”¨json.dumpså°†Pythonå¯¹è±¡è½¬æ¢å›æ ¼å¼åŒ–çš„JSONå­—ç¬¦ä¸²  \n",
    "formatted_json = json.dumps(meta_data, indent=4, ensure_ascii=False)  \n",
    "  \n",
    "print(formatted_json)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello, how are you?\n",
      "Translated Text: Hola, Â¿cÃ³mo estÃ¡s?\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "\n",
    "# åŸå§‹æ–‡æœ¬  \n",
    "text = \"Hello, how are you?\"  \n",
    "\n",
    "# ç¿»è¯‘æ–‡æœ¬  \n",
    "translator = Translator(to_lang=\"es\")\n",
    "translated_text = translator.translate(text)\n",
    "\n",
    "print(\"Original Text:\", text)  \n",
    "print(\"Translated Text:\", translated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello, how are you?\n",
      "Translated Text: ä½ å¥½ä½ æ€ä¹ˆæ ·ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "\n",
    "# åŸå§‹æ–‡æœ¬  \n",
    "text = \"Hello, how are you?\"  \n",
    "\n",
    "# ç¿»è¯‘æ–‡æœ¬  \n",
    "translator = Translator(to_lang=\"zh\")\n",
    "translated_text = translator.translate(text)\n",
    "\n",
    "print(\"Original Text:\", text)  \n",
    "print(\"Translated Text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "aims tone text analysis sentiment\n",
      "Topic 2:\n",
      "human learning function algorithms structure\n"
     ]
    }
   ],
   "source": [
    "#ï¼conda install scikit-learn  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.decomposition import LatentDirichletAllocation  \n",
    "  \n",
    "# ç¤ºä¾‹æ–‡æ¡£  \n",
    "documents = [  \n",
    "    \"Machine learning is a subset of artificial intelligence.\",  \n",
    "    \"Natural language processing involves analyzing and understanding human languages.\",  \n",
    "    \"Deep learning algorithms mimic the structure and function of the human brain.\",  \n",
    "    \"Sentiment analysis aims to determine the emotional tone of a text.\"  \n",
    "]  \n",
    "  \n",
    "# å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾å‘é‡  \n",
    "vectorizer = CountVectorizer(stop_words='english')  \n",
    "X = vectorizer.fit_transform(documents)  \n",
    "  \n",
    "# åº”ç”¨Latent Dirichlet Allocation (LDA)è¿›è¡Œä¸»é¢˜å»ºæ¨¡  \n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)  \n",
    "lda.fit(X)  \n",
    "  \n",
    "# æ˜¾ç¤ºä¸»é¢˜  \n",
    "for topic_idx, topic in enumerate(lda.components_):  \n",
    "    print(\"Topic %d:\" % (topic_idx + 1))  \n",
    "    # ä½¿ç”¨.get_feature_names_out()ä»£æ›¿.get_feature_names()  \n",
    "    print(\" \".join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-5 - 1:-1]]))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_text = \"\"\"\n",
    "Sarah (S): Technology Enthusiast\n",
    "Mark (M): AI Expert\n",
    "S: Hey Mark! How's it going? Heard about the latest advancements in Generative AI (GA)?\n",
    "M: Hey Sarah! Yes, I've been diving deep into the realm of GA lately. It's fascinating how it's shaping the future of technology!\n",
    "S: Absolutely! I mean, GA has been making waves across various industries. What do you think is driving its significance?\n",
    "M: Well, GA, especially Retrieval Augmented Generative (RAG), is revolutionizing content generation. It's not just about regurgitating information anymore; it's about creating contextually relevant and engaging content.\n",
    "S: Right! And with Machine Learning (ML) becoming more sophisticated, the possibilities seem endless.\n",
    "M: Exactly! With advancements in ML algorithms like GPT (Generative Pre-trained Transformer), we're seeing unprecedented levels of creativity in AI-generated content.\n",
    "S: But what about concerns regarding bias and ethics in GA?\n",
    "M: Ah, the age-old question! While it'\n",
    "s \n",
    "true\n",
    " that GA can inadvertently perpetuate biases \n",
    "present\n",
    "in\n",
    " the training \n",
    "data\n",
    ", there \n",
    "are\n",
    " techniques \n",
    "like\n",
    " Adversarial Training (\n",
    "AT\n",
    ") that aim \n",
    "to\n",
    " mitigate such issues.\n",
    "S: Interesting! So, where do you see GA headed in the next few years?\n",
    "M: Well, I believe we'll witness a surge in applications leveraging GA for personalized experiences. From virtual assistants to content creation tools, GA will become ubiquitous in our daily lives.\n",
    "S: That'\n",
    "s exciting! Imagine AI-powered \n",
    "virtual\n",
    " companions tailored \n",
    "to\n",
    " our preferences.\n",
    "M: Indeed! And with advancements in Natural Language Processing (NLP) and computer vision, these virtual companions will be more intuitive and lifelike than ever before.\n",
    "S: I can't wait to see what the future holds!\n",
    "M: Agreed! It'\n",
    "s an exciting \n",
    "time\n",
    "to\n",
    " be \n",
    "in\n",
    " the \n",
    "field\n",
    "of\n",
    " AI.\n",
    "S: Absolutely! Thanks for sharing your insights, Mark.\n",
    "M: Anytime, Sarah. Let's keep pushing the boundaries of Generative AI together!\n",
    "S: Definitely! Catch you later, Mark!\n",
    "M: Take care, Sarah!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "import nltk  \n",
    "  \n",
    "# Download necessary NLTK data  \n",
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords')  \n",
    "nltk.download('wordnet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sarah', 'technology', 'enthusiast', 'mark', 'ai', 'expert', 'hey', 'mark', 'going', 'heard', 'latest', 'advancement', 'generative', 'ai', 'ga', 'hey', 'sarah', 'yes', 'diving', 'deep', 'realm', 'ga', 'lately', 'fascinating', 'shaping', 'future', 'technology', 'absolutely', 'mean', 'ga', 'making', 'wave', 'across', 'various', 'industry', 'think', 'driving', 'significance', 'well', 'ga', 'especially', 'retrieval', 'augmented', 'generative', 'rag', 'revolutionizing', 'content', 'generation', 'regurgitating', 'information', 'anymore', 'creating', 'contextually', 'relevant', 'engaging', 'content', 'right', 'machine', 'learning', 'ml', 'becoming', 'sophisticated', 'possibility', 'seem', 'endless', 'exactly', 'advancement', 'ml', 'algorithm', 'like', 'gpt', 'generative', 'pretrained', 'transformer', 'seeing', 'unprecedented', 'level', 'creativity', 'aigenerated', 'content', 'concern', 'regarding', 'bias', 'ethic', 'ga', 'ah', 'ageold', 'question', 'true', 'ga', 'inadvertently', 'perpetuate', 'bias', 'present', 'training', 'data', 'technique', 'like', 'adversarial', 'training', 'aim', 'mitigate', 'issue', 'interesting', 'see', 'ga', 'headed', 'next', 'year', 'well', 'believe', 'witness', 'surge', 'application', 'leveraging', 'ga', 'personalized', 'experience', 'virtual', 'assistant', 'content', 'creation', 'tool', 'ga', 'become', 'ubiquitous', 'daily', 'life', 'exciting', 'imagine', 'aipowered', 'virtual', 'companion', 'tailored', 'preference', 'indeed', 'advancement', 'natural', 'language', 'processing', 'nlp', 'computer', 'vision', 'virtual', 'companion', 'intuitive', 'lifelike', 'ever', 'ca', 'nt', 'wait', 'see', 'future', 'hold', 'agreed', 'exciting', 'time', 'field', 'ai', 'absolutely', 'thanks', 'sharing', 'insight', 'mark', 'anytime', 'sarah', 'let', 'keep', 'pushing', 'boundary', 'generative', 'ai', 'together', 'definitely', 'catch', 'later', 'mark', 'take', 'care', 'sarah']\n",
      "sarah technology enthusiast mark ai expert hey mark going heard latest advancement generative ai ga hey sarah yes diving deep realm ga lately fascinating shaping future technology absolutely mean ga making wave across various industry think driving significance well ga especially retrieval augmented generative rag revolutionizing content generation regurgitating information anymore creating contextually relevant engaging content right machine learning ml becoming sophisticated possibility seem endless exactly advancement ml algorithm like gpt generative pretrained transformer seeing unprecedented level creativity aigenerated content concern regarding bias ethic ga ah ageold question true ga inadvertently perpetuate bias present training data technique like adversarial training aim mitigate issue interesting see ga headed next year well believe witness surge application leveraging ga personalized experience virtual assistant content creation tool ga become ubiquitous daily life exciting imagine aipowered virtual companion tailored preference indeed advancement natural language processing nlp computer vision virtual companion intuitive lifelike ever ca nt wait see future hold agreed exciting time field ai absolutely thanks sharing insight mark anytime sarah let keep pushing boundary generative ai together definitely catch later mark take care sarah\n"
     ]
    }
   ],
   "source": [
    "# Sample text with emojis, hashtags, and unicode characters\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(synthetic_text)\n",
    "\n",
    "# Remove Noise and filter out empty tokens  \n",
    "cleaned_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens if re.sub(r'[^\\w\\s]', '', token)]  \n",
    "  \n",
    "# Normalization (convert to lowercase)  \n",
    "cleaned_tokens = [token.lower() for token in cleaned_tokens]  \n",
    "  \n",
    "# Remove Stopwords  \n",
    "stop_words = set(stopwords.words('english'))  \n",
    "cleaned_tokens = [token for token in cleaned_tokens if token not in stop_words and token.strip()]  \n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
    "\n",
    "print(cleaned_tokens)\n",
    "new_content_string = ' '.join([lemmatizer.lemmatize(token) for token in cleaned_tokens])\n",
    "print(new_content_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "MESSAGE_SYSTEM_CONTENT = \"\"\"You are a customer service agent that helps   \n",
    "a customer with answering questions. Please answer the question based on the  \n",
    "provided context below.   \n",
    "Make sure not to make any changes to the context if possible,  \n",
    "when prepare answers so as to provide accurate responses. If the answer   \n",
    "cannot be found in context, just politely say that you do not know,   \n",
    "do not try to make up an answer.\"\"\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "from openai import AzureOpenAI  \n",
    "  \n",
    "# å®šä¹‰ Azure OpenAI å®¢æˆ·ç«¯  \n",
    "client = AzureOpenAI(  \n",
    "  azure_endpoint=\"https://gpt4-0125.openai.azure.com/\",  \n",
    "  api_key=\"7b1d32be7008483086cc936a046ab848\",  \n",
    "  api_version=\"2024-02-15-preview\"  \n",
    ")  \n",
    "  \n",
    "# æ›´æ–°åçš„ response_test å‡½æ•°  \n",
    "def response_test(question: str, context: str, model: str = \"gpt4-0125\"):    \n",
    "    message_text = [    \n",
    "        {\"role\": \"system\", \"content\": MESSAGE_SYSTEM_CONTENT},    \n",
    "        {\"role\": \"user\", \"content\": question},    \n",
    "        {\"role\": \"assistant\", \"content\": context},    \n",
    "    ]    \n",
    "        \n",
    "    completion = client.chat.completions.create(    \n",
    "        model=model,    \n",
    "        messages=message_text,    \n",
    "        temperature=0.7,    \n",
    "        max_tokens=4040,    \n",
    "        top_p=0.95,    \n",
    "        frequency_penalty=0,    \n",
    "        presence_penalty=0,    \n",
    "        stop=None    \n",
    "    )    \n",
    "        \n",
    "    return completion.choices[0].message.content  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the provided conversation between Sarah and Mark does not include specific techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models. They mention the importance and the potential of Adversarial Training in addressing bias, but they do not delve into specific techniques or methodologies. If you have any other questions or need further clarification on a different topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# ç¤ºä¾‹è°ƒç”¨  \n",
    "question = \"What are some specific techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models?\"  \n",
    "response = response_test(question, synthetic_text)    \n",
    "print(response)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the provided context does not specifically mention techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models. Adversarial Training is indeed a method used to improve the robustness of AI models, including generative ones, by exposing them to adversarial examples during training. However, the conversation primarily focuses on the general advancements and potentials of Generative AI, without delving into specific techniques for bias mitigation. If you have any other questions or need further information on a different topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "question = \"What are some specific techniques in Adversarial Training (AT) that can help mitigate biases in Generative AI models?\" \n",
    "response = response_test(question, new_content_string)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
